[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome SCHOOL Module 2: Air Quality",
    "section": "",
    "text": "Welcome to the second module of the SCHOOL curriculum!\nThe Science Core Heuristics for Open Science Outcomes in Learning (SCHOOL) is part of the NASA Transform to Open Science (TOPS) Training (TOPST) initiative, designed to teach the data science lifecycle using data from the NASA Earth Sciences division and to foster an inclusive culture of open science. You can learn more about the SCHOOL Project and other modules on the SCHOOL Project home page.\nThis TOPS SCHOOL module on the theme of “Air Quality and Health” provides a foundational understanding of the complex interplay between air quality, public health, and environmental justice. We will explore how we can use spatial data for the study of air pollution, the sources and types of pollutants, and their impacts on vulnerable communities. You will gain critical insights into how environmental factors shape health outcomes. As you engage with the lessons, you will develop a comprehensive understanding of key concepts, including the composition of air, the significance of social vulnerability, and the tools available for analyzing environmental data. Ultimately, these lessons will empower you to contribute meaningfully to discussions and actions pertaining to air quality and environmental justice.\nThe following lessons examine how poor air quality impacts human health. Each lesson uses one or more datasets to walk users through accessing and analyzing data, and further adapting the code to perform their analyses including: data cleaning, processing to subset to an area of interest, and creating visualizations to share what they have learned with their communities, explore the relationships with variables such as extreme temperatures, demographic and socioeconomic factors, environmental injustice, and environmental disasters."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html",
    "href": "m201-student-led-monitoring-nyc.html",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "",
    "text": "In this lesson, you will use…."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#learning-objectives",
    "href": "m201-student-led-monitoring-nyc.html#learning-objectives",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this lesson, you should be able to:\n\nDetermine…"
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#introduction",
    "href": "m201-student-led-monitoring-nyc.html#introduction",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Introduction",
    "text": "Introduction\nPollution represents a critical threat to environmental health (Manisalidis et al. 2020; World Health Organization 2021; Mananga et al. 2023), characterized by the introduction of harmful substances into the environment in quantities that disrupt natural processes and negatively affect living organisms and ecosystems. Among the various forms of pollution, air pollution stands out due to its widespread impact on both health and the environment. Air, primarily composed of nitrogen and oxygen, can become a vehicle for a variety of pollutants—including gases like carbon monoxide and sulfur dioxide, particulate matter, and even hazardous liquids and solids like heavy metals and plastic debris (Nathanson 2024). These pollutants can have immediate or long-term consequences, compromising air quality, water resources, soil health, and biodiversity, which ultimately jeopardizes the well-being of both ecosystems and human populations.\n\nWhat is Air?\nAir is essential to our existence, yet we often underestimate its potential to harm us through the pollutants it carries, which can negatively affect health and environmental quality. Composed primarily of about 78% nitrogen and 21% oxygen, air also contains trace amounts of other gases like carbon dioxide, neon, and hydrogen. Beyond gases, air is filled with tiny particles known as aerosols. These include non-living substances such as dust and soot, as well as living organisms called bioaerosols, which can travel long distances through wind, rain, or even a sneeze. Some of these particles fall under the category of particulate matter (PM), PM0.1 PM2.5 and PM10, which are small enough to be inhaled and can significantly impact health. While some aerosols occur naturally, others—like smoke from vehicles and power plants—contribute to air pollution (NASA, 2016).\n\nParticulate Matter Size Table\n\n\n\n\n\n\n\n\nName\nAerodynamic Diameter\nAbbreviation\n\n\n\n\nCoarse Particles\n2.5μm <= x < 10μm\nPM10\n\n\nFine Particles\n0.01μm <= x < 2.5μm\nPM2.5\n\n\nUltra Fine Particles (UFP)\nx < 0.01μm\nPM0.1\n\n\n\nSince the onset of industrialization and urbanization, human activities have significantly contributed to rising levels of air pollution. Multiple studies (Gallagher and Holloway 2022; Mananga et al. 2023) have established a link between anthropogenic activities and particulate matter concentration, which is associated with adverse health effects related to local air pollution. In fact, New York City, one of the most urbanized metropolitan areas, has stated that by 2050, the city aims to reduce GreenHouse Gases (GHG) by 80% compared to 2005 levels. Like many other metropolitan areas, New York City is experiencing ongoing consequences of climate change driven by fossil fuels and GHG. This change has led to increased average temperatures, resulting in more intense heat waves, severe storms, and disturbances to ecological systems. These events contribute to the degradation of air quality, the environment, and human health.\n1\n\n\n\nLocal scale\nHowever, the effects of air pollution and climate change are not felt equally in New York City; marginalized communities, often subjected to historical injustices like redlining, face heightened vulnerabilities. Redlining originated in the 1930s as a discriminatory practice where neighborhoods, primarily inhabited by Black families and other racial minorities, were marked in red on maps by government agencies and banks to indicate high-risk areas for mortgage lending (Hwa Jung et al. 2022).\nThe Home Owners’ Loan Corporation (HOLC) played a pivotal role in this process, assessing neighborhoods based on their perceived creditworthiness. Those deemed “undesirable” were systematically denied access to loans and investment, leading to disinvestment in these communities. The legacy of redlining has resulted in persistent socioeconomic inequalities, including limited access to quality housing, education, and healthcare, which continue to exacerbate vulnerabilities to environmental hazards, such as air pollution (Hwa Jung et al. 2022). Understanding this historical context is essential for addressing the ongoing impacts of these injustices in cities like New York.\nIn ‘The Effects of the Historical Practice of Residential Redlining in the United States on Recent Temporal Trends of Air Pollution Near New York City Schools,’ the authors delve into the consequences of this discriminatory housing practice (Hwa Jung et al. 2022). The researchers’ findings reveal that, while there has been a notable decrease in annual average air pollution across New York City, this reduction is not evenly distributed among neighborhoods. Schools located in historically redlined areas have experienced smaller decreases in air pollution compared to those in other neighborhoods. This disparity highlights the enduring impact of past discriminatory practices and underscores the importance of addressing historical inequities to improve air quality outcomes for all communities. Recognizing these differences is crucial in promoting social justice and ensuring equitable access to clean air for all residents, particularly vulnerable populations such as children attending schools near polluted areas.\nMoreover, historically redlined neighborhoods and high-poverty areas often have a higher density of transportation routes (Kheirbek et al. 2016; Hwa Jung et al. 2022). These routes are primarily used by trucks powered by fossil fuels, which emit greenhouse gasses, fine particulate matter, and other pollutants. These pollutants can infiltrate the human respiratory system through inhalation. Short-term exposure to air pollution, particularly particulate matter (PM), has been shown to cause respiratory issues such as coughing, wheezing, and shortness of breath, along with increased hospitalization rates (Mananga et al. 2023) . In contrast, long-term exposure is linked to more severe health problems, including cardiovascular diseases, chronic asthma, and higher mortality rates from cardiovascular conditions [Bont et al. (2022)]. These health impacts disproportionately affect ethnic minorities living in these areas, highlighting the urgent need for targeted interventions to address these environmental injustices and protect vulnerable populations.\n\n\nRegional scale\nWhile the challenges of air pollution and historical injustices in New York City are striking, they are reflective of broader trends seen across the United States. Many locations across the U.S. are grappling with devastating hurricanes, wildfires, unprecedented flooding, and droughts. The frequency and intensity of these events are increasing due to climate change (Rahman et al. 2022). As these occurrences become more common, the concentration of particulate matter in the air rises, creating a lasting harmful environment. Researchers and practitioners utilize critical datasets such as the EPA’s Toxic Release Inventory (TRI): EPA TRI Program, the Integrated Compliance Information System for Air (ICIS-AIR): ICIS-AIR, and the CDC’s PLACES health outcomes dataset (CDC PLACES). By querying and retrieving data from these sources, users can process the information into usable formats, create visualizations, and conduct analyses, which you will do in subsequent lessons. Additionally, NASA’s Social Vulnerability Index (SVI): NASA SVI can be employed to examine socioeconomic patterns across the United States and explore their correlations with public health outcomes.\nAcross various states, the impacts of climate change manifest in different but equally troubling ways. In California, wildfires and prolonged droughts highlight the intersection of environmental degradation and socioeconomic disparities, particularly affecting low-income agricultural communities in regions like the San Joaquin Valley (Yadav et al. 2023). In Florida, coastal communities face the relentless threat of hurricanes, where storm surges and flooding disproportionately impact marginalized neighborhoods with inadequate infrastructure (Waddell et al. 2021). Meanwhile, in Michigan, industrial pollution and aging infrastructure have led to significant air quality issues, particularly in cities like Flint, where the combined effects of environmental neglect and socioeconomic challenges exacerbate public health risks. These examples illustrate a nationwide pattern: while macro-level data may indicate overall trends, they often obscure the specific struggles of small, vulnerable communities. Elevated asthma rates among children living near industrial zones or the heightened risks faced by families in substandard housing reveal the urgent need for hyperlocal data that captures the unique conditions and needs of these populations. By amplifying these voices, we can better address environmental injustices and foster meaningful change in the ongoing conversation about climate change and public health."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#data-collection-and-preparation",
    "href": "m201-student-led-monitoring-nyc.html#data-collection-and-preparation",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Data Collection and Preparation",
    "text": "Data Collection and Preparation\n\nMobile and Static Monitoring\nEquip students with mobile sensors to collect air quality data. Compare this with data from static sensors placed in schools and remote sensing sources.\n\n\nSatellite Data Integration\nUtilize satellite data such as MODIS, OMI, MERRA-2, GOES, CHIRTS-daily, and SEDAC’s Air Quality Data to complement ground data.\n\n\nData Upload and Management\nStudents upload collected data to an online platform, ensuring proper documentation and metadata inclusion to maintain data quality and integrity."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#data-cleaning-and-preprocessing",
    "href": "m201-student-led-monitoring-nyc.html#data-cleaning-and-preprocessing",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Data Cleaning and Preprocessing",
    "text": "Data Cleaning and Preprocessing\n\nData Validation\nCheck for inconsistencies or anomalies in the collected data. This includes cross-referencing mobile sensor data with static and satellite data to ensure accuracy.\n\n\nHandling Missing Data\nApply techniques such as interpolation or imputation to address missing data points, ensuring a complete dataset for analysis."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#data-analysis-and-visualization",
    "href": "m201-student-led-monitoring-nyc.html#data-analysis-and-visualization",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\n\nDescriptive Statistics\nCalculate basic statistics (mean, median, mode, standard deviation) to understand the distribution and central tendencies of the data.\n\n\nCorrelation Analysis\nExamine relationships between air quality, temperature, and socioeconomic characteristics of the schools.\n\n\nVisualization\nUse tools like ArcGIS Online for participatory mapping and visualizations to represent data spatially and temporally. Modeling and Interpretation\n\n\nPredictive Modeling\nDevelop models to predict air quality and temperature variations based on historical data and current trends. Impact Analysis: Assess the health impacts of hazardous air quality and extreme temperatures on school populations, with a focus on vulnerable groups. Reporting and Communication\n\n\nStory Development\nEngage with the Solutions Journalism Network to develop stories that highlight the project’s findings and advocate for environmental justice.\n\n\nPublications and Presentations\nCompile findings into reports, articles, and presentations for dissemination through academic channels and public forums."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#open-science-principles-and-data-sharing",
    "href": "m201-student-led-monitoring-nyc.html#open-science-principles-and-data-sharing",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Open Science Principles and Data Sharing",
    "text": "Open Science Principles and Data Sharing\n\nTransparency\nEnsure all data processing steps, methodologies, and code are well-documented and accessible.\n\n\nReproducibility\nProvide clear instructions and datasets for others to replicate the study.\n\n\nData Sharing\nPublish datasets and findings in open-access repositories to promote further research and collaboration.\nCongratulations! …. Now you should be able to:\n\nTest test."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#lesson-2",
    "href": "m201-student-led-monitoring-nyc.html#lesson-2",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Lesson 2",
    "text": "Lesson 2\nExplore how the SOciel Vulnerability Index (SVI) highlights areas at greater risk for adverse health outcomes due to environmental hazards. This lesson will examine the connections between socioeconomic status, health disparities, and environmental injustice.\nLesson 2: Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit"
  },
  {
    "objectID": "m202-ejscreen.html",
    "href": "m202-ejscreen.html",
    "title": "EJSCREEN tool",
    "section": "",
    "text": "In this lesson, you will use…."
  },
  {
    "objectID": "m202-ejscreen.html#learning-objectives",
    "href": "m202-ejscreen.html#learning-objectives",
    "title": "EJSCREEN tool",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this lesson, you should be able to:\n\nDetermine…"
  },
  {
    "objectID": "m202-ejscreen.html#introduction",
    "href": "m202-ejscreen.html#introduction",
    "title": "EJSCREEN tool",
    "section": "Introduction",
    "text": "Introduction\nIn recent years, researchers have explored the intersection between environmental factors and public health, particularly concerning diseases like COVID-19 and non-small cell lung cancer (NSCLC). Studies have highlighted the impact of air pollution on disease susceptibility and outcomes, emphasizing the need for rigorous analysis and understanding of these relationships. This overview focuses on several key studies that utilize data science principles to investigate how environmental justice, air pollution, and demographic factors intersect, providing insights that contribute to open science principles and the data science life cycle."
  },
  {
    "objectID": "m202-ejscreen.html#data-collection-and-integration",
    "href": "m202-ejscreen.html#data-collection-and-integration",
    "title": "EJSCREEN tool",
    "section": "Data Collection and Integration",
    "text": "Data Collection and Integration\nGather comprehensive datasets from reliable sources such as the US EPA’s EJSCREEN tool and other public health databases like John Hopkins and County Health Rankings. Integrate relevant environmental data (e.g., air pollutant concentrations, pollution source proximity) with health outcomes data (e.g., COVID-19 prevalence, NSCLC incidence). Ensure data compatibility and quality through data cleaning and validation procedures.\n\nExploratory Data Analysis (EDA)\nConduct initial exploratory analyses to understand the distribution and relationships within the data. Visualize data using plots and charts to identify patterns and correlations between environmental factors, demographic variables, and health outcomes. Perform statistical tests to assess associations and identify potential confounding factors.\n\n\nModel Development and Analysis\nApply statistical models (e.g., regression analyses, machine learning algorithms) to quantify the relationships between environmental exposures and health outcomes. Adjust for confounders such as demographic characteristics (e.g., age, race/ethnicity) and socioeconomic factors (e.g., income, education). Validate models using cross-validation techniques to ensure robustness and generalizability of findings.\n\n\nInterpretation and Communication of Results\nInterpret findings in the context of environmental justice principles, highlighting disparities and vulnerabilities observed in different populations. Discuss implications for public health policy and environmental regulations based on study outcomes. Communicate results transparently using accessible language and visual aids to engage stakeholders and the broader community."
  },
  {
    "objectID": "m202-ejscreen.html#the-data-science-life-cycle",
    "href": "m202-ejscreen.html#the-data-science-life-cycle",
    "title": "EJSCREEN tool",
    "section": "The Data Science Life Cycle",
    "text": "The Data Science Life Cycle\nThe data science life cycle guides the systematic approach to handling data from collection to interpretation:\n\nData Acquisition\nObtain relevant datasets from sources like EJSCREEN, County Health Rankings, and specific studies’ databases. ### Data Preparation Clean and preprocess data to ensure accuracy and consistency, handling missing values and outliers appropriately.\n\n\nExploratory Data Analysis\nExplore data distributions, correlations, and initial insights to guide further analysis.\n\n\nModeling\nDevelop statistical models to test hypotheses and predict outcomes, considering factors like pollution exposure and demographic variables.\n\n\nEvaluation\nAssess model performance and validity through metrics and cross-validation techniques.\n\n\nDeployment\nCommunicate findings through reports, presentatio\nCongratulations! …. Now you should be able to:\n\nTest test…"
  },
  {
    "objectID": "m202-ejscreen.html#lesson-2",
    "href": "m202-ejscreen.html#lesson-2",
    "title": "EJSCREEN tool",
    "section": "Lesson 2",
    "text": "Lesson 2\nIn this lesson, we explored ….\nLesson 2: EJSCREEN"
  },
  {
    "objectID": "m202-ejscreen.html#lesson-3",
    "href": "m202-ejscreen.html#lesson-3",
    "title": "EJSCREEN tool",
    "section": "Lesson 3",
    "text": "Lesson 3\nIn this lesson, we explored ….\nLesson 3"
  },
  {
    "objectID": "m202-svi-ejscreen.html",
    "href": "m202-svi-ejscreen.html",
    "title": "EJSCREEN tool",
    "section": "",
    "text": "In this lesson, you will use…."
  },
  {
    "objectID": "m202-svi-ejscreen.html#learning-objectives",
    "href": "m202-svi-ejscreen.html#learning-objectives",
    "title": "EJSCREEN tool",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this lesson, you should be able to:\n\nDetermine…"
  },
  {
    "objectID": "m202-svi-ejscreen.html#introduction",
    "href": "m202-svi-ejscreen.html#introduction",
    "title": "EJSCREEN tool",
    "section": "Introduction",
    "text": "Introduction\nIn recent years, researchers have explored the intersection between environmental factors and public health, particularly concerning diseases like COVID-19 and non-small cell lung cancer (NSCLC). Studies have highlighted the impact of air pollution on disease susceptibility and outcomes, emphasizing the need for rigorous analysis and understanding of these relationships. This overview focuses on several key studies that utilize data science principles to investigate how environmental justice, air pollution, and demographic factors intersect, providing insights that contribute to open science principles and the data science life cycle."
  },
  {
    "objectID": "m202-svi-ejscreen.html#data-collection-and-integration",
    "href": "m202-svi-ejscreen.html#data-collection-and-integration",
    "title": "EJSCREEN tool",
    "section": "Data Collection and Integration",
    "text": "Data Collection and Integration\nGather comprehensive datasets from reliable sources such as the US EPA’s EJSCREEN tool and other public health databases like John Hopkins and County Health Rankings. Integrate relevant environmental data (e.g., air pollutant concentrations, pollution source proximity) with health outcomes data (e.g., COVID-19 prevalence, NSCLC incidence). Ensure data compatibility and quality through data cleaning and validation procedures.\n\nExploratory Data Analysis (EDA)\nConduct initial exploratory analyses to understand the distribution and relationships within the data. Visualize data using plots and charts to identify patterns and correlations between environmental factors, demographic variables, and health outcomes. Perform statistical tests to assess associations and identify potential confounding factors.\n\n\nModel Development and Analysis\nApply statistical models (e.g., regression analyses, machine learning algorithms) to quantify the relationships between environmental exposures and health outcomes. Adjust for confounders such as demographic characteristics (e.g., age, race/ethnicity) and socioeconomic factors (e.g., income, education). Validate models using cross-validation techniques to ensure robustness and generalizability of findings.\n\n\nInterpretation and Communication of Results\nInterpret findings in the context of environmental justice principles, highlighting disparities and vulnerabilities observed in different populations. Discuss implications for public health policy and environmental regulations based on study outcomes. Communicate results transparently using accessible language and visual aids to engage stakeholders and the broader community."
  },
  {
    "objectID": "m202-svi-ejscreen.html#the-data-science-life-cycle",
    "href": "m202-svi-ejscreen.html#the-data-science-life-cycle",
    "title": "EJSCREEN tool",
    "section": "The Data Science Life Cycle",
    "text": "The Data Science Life Cycle\nThe data science life cycle guides the systematic approach to handling data from collection to interpretation:\n\nData Acquisition\nObtain relevant datasets from sources like EJSCREEN, County Health Rankings, and specific studies’ databases. ### Data Preparation Clean and preprocess data to ensure accuracy and consistency, handling missing values and outliers appropriately.\n\n\nExploratory Data Analysis\nExplore data distributions, correlations, and initial insights to guide further analysis.\n\n\nModeling\nDevelop statistical models to test hypotheses and predict outcomes, considering factors like pollution exposure and demographic variables.\n\n\nEvaluation\nAssess model performance and validity through metrics and cross-validation techniques.\n\n\nDeployment\nCommunicate findings through reports, presentatio\nCongratulations! …. Now you should be able to:\n\nTest test…"
  },
  {
    "objectID": "m202-svi-ejscreen.html#lesson-3",
    "href": "m202-svi-ejscreen.html#lesson-3",
    "title": "EJSCREEN tool",
    "section": "Lesson 3",
    "text": "Lesson 3\nIn this lesson, we explored ….\nLesson 3"
  },
  {
    "objectID": "m203-grdiv1-pm25.html",
    "href": "m203-grdiv1-pm25.html",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "",
    "text": "In this lesson, you will use NASA socioeconomic and environmental Earthdata available at NASA SEDAC to compare relationships between levels of socioeconomic deprivation agaisnts air quality data of particulate matter (PM) in different international administrative areas.\nThis lesson walks through the process of calculating and visualizing zonal statistics for a set of countries using raster data, focusing on GRDI country quintiles and PM2.5 concentration levels within these quintile areas. It begins by subsetting data by country and iterating over each country to extract relevant zonal statistics like mean, median, and various percentiles for each quintile. These statistics are stored in a GeoDataFrame, which is later used to create a choropleth map that visualizes specific GRDI metrics across countries. The lesson includes a detailed analysis of PM2.5 concentrations within different GRDI quartiles for selected countries. This involves clipping the raster data to each country’s geometry, filtering the data based on the GRDI quartiles, and calculating the mean PM2.5 levels for each quartile. The results are then visualized using customized plots to highlight the relationship between air quality and GRDI metrics across the selected countries."
  },
  {
    "objectID": "m203-grdiv1-pm25.html#learning-objectives",
    "href": "m203-grdiv1-pm25.html#learning-objectives",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this lesson, you should be able to:\n\nGain a general understanding of what is particulate matter (PM) in the air and how it impacts human health.\nLearn about global socioeconomic dimensions of deprivation and how they are spatially represented.\nFind statistical thresholds in socioeconomic data.\nPerform zonal statistics to summarize spatial data\nResample spatial data to harmoniza and compare socioeconomic data against environmental data.\nDisplay data on a maps to get a general understanding of the spatial distribution of data.\nSummarize spatial data into table plots to compare how air quality differs in different socioeconomic conditions of international administrative areas."
  },
  {
    "objectID": "m203-grdiv1-pm25.html#introduction",
    "href": "m203-grdiv1-pm25.html#introduction",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#data-collection-and-integration",
    "href": "m203-grdiv1-pm25.html#data-collection-and-integration",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Data Collection and Integration",
    "text": "Data Collection and Integration\nThe Global (GL) Annual PM2.5 Grids from MODIS, MISR and SeaWiFS Aerosol Optical Depth (AOD), v4.03 (1998 – 2019) can can be downloaded from the Socioeconomic Data and Applications Center ([SEDAC](https://sedac.ciesin.columbia.edu/)) (Center For International Earth Science Information Network-CIESIN-Columbia University 2022a).\nThe Global Gridded Relative Deprivation Index (GRDI), v1 (2010 – 2020) dataset can be downloaded from SEDAC as well (Center For International Earth Science Information Network-CIESIN-Columbia University 2022b).\nGather comprehensive datasets from reliable sources such as the US EPA’s EJSCREEN tool and other public health databases like John Hopkins and County Health Rankings. Integrate relevant environmental data (e.g., air pollutant concentrations, pollution source proximity) with health outcomes data (e.g., COVID-19 prevalence, NSCLC incidence). Ensure data compatibility and quality through data cleaning and validation procedures.\n\nPreparing Environment and Variables\nImporting python packages required:\n\nimport xarray as xr\nimport rioxarray\nimport rasterstats\nfrom rasterio.enums import Resampling\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport geopandas as gpd\nimport pygadm\n\nimport plotly.graph_objects as go \n\nLoad the GRDIv1 and PM2.5 data from local sources:\n\n# Load rasters\ngrdi_path = r\"Z:\\Sedac\\GRDI\\data\\povmap-grdi-v1-geotiff\\final data\\povmap-grdi-v1.tif\"\npm25_path = r\"F:\\TOPSSCHOOL\\git\\TOPSTSCHOOL-air-quality\\data\\sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-v4-gl-03-2019-geotiff\\sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-v4-gl-03-2019.tif\"\n\nUsing the package rasterio to load the data into memory. This allows us to read the data and use it for processing.\n\n# Open the input and reference rasters\ngrdi_raster = rioxarray.open_rasterio(grdi_path, mask_and_scale=True)\npm25_raster = rioxarray.open_rasterio(pm25_path, mask_and_scale=True)\n\n\n\nMatching Data Points using Bilinear Resample\nThe GRDI raster and PM2.5 rasters are incompatible in resolution. One method of harmonizing data is by using the Resampling bethod with a bilinear method. In this case, we reduce, or coarsen, the resolution of the GRDI raster to match the PM2.5 raster.\n\n# Resample the input raster to match the reference raster\ngrdi_raster = grdi_raster.rio.reproject_match(pm25_raster,resampling=Resampling.bilinear)"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#the-data-science-life-cycle",
    "href": "m203-grdiv1-pm25.html#the-data-science-life-cycle",
    "title": "EJSCREEN tool",
    "section": "The Data Science Life Cycle",
    "text": "The Data Science Life Cycle\nThe data science life cycle guides the systematic approach to handling data from collection to interpretation:\n\nData Acquisition\nObtain relevant datasets from sources like EJSCREEN, County Health Rankings, and specific studies’ databases. ### Data Preparation Clean and preprocess data to ensure accuracy and consistency, handling missing values and outliers appropriately.\n\n\nExploratory Data Analysis\nExplore data distributions, correlations, and initial insights to guide further analysis.\n\n\nModeling\nDevelop statistical models to test hypotheses and predict outcomes, considering factors like pollution exposure and demographic variables.\n\n\nEvaluation\nAssess model performance and validity through metrics and cross-validation techniques.\n\n\nDeployment\nCommunicate findings through reports, presentatio\nCongratulations! …. Now you should be able to:\n\nTest test…"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#module-2-air-quality-home",
    "href": "m203-grdiv1-pm25.html#module-2-air-quality-home",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Module 2: Air Quality Home",
    "text": "Module 2: Air Quality Home\nIn this lesson, we explored ….\nModule 2: Air Quality"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#previewing-spatial-data-in-a-plot",
    "href": "m203-grdiv1-pm25.html#previewing-spatial-data-in-a-plot",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Previewing Spatial Data in a Plot",
    "text": "Previewing Spatial Data in a Plot\n\n# Plotting the rasters\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 20))\n\n# Plot the original GRDI raster in the first subplot\nim1 = ax1.imshow(grdi_raster.values[0], cmap='viridis', interpolation='nearest')\nax1.set_title('Original GRDI Raster')\nfig.colorbar(im1, ax=ax1, orientation='horizontal', label='GRDI Values')\n\n# Plot the PM2.5 raster in the second subplot\nim2 = ax2.imshow(pm25_raster.values[0], cmap='hot', interpolation='nearest')\nax2.set_title('PM2.5 Raster')\nfig.colorbar(im2, ax=ax2, orientation='horizontal', label='PM2.5 Values')\n\n\n# Show the plots\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#working-with-administrative-data",
    "href": "m203-grdiv1-pm25.html#working-with-administrative-data",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Working with administrative Data",
    "text": "Working with administrative Data\npygadm is a package that has international administrative units from levels 0 to 2. We can search the available countries by listing the Names.\n\ncountry_table = gpd.GeoDataFrame(pygadm.Names())\nlen(country_table)\n\n263\n\n\nSome available areas with a unique GID_0 code share Names; therefore we drop the rows that contain digits.\n\ncountry_table = country_table[~country_table['GID_0'].str.contains('\\d', na=False)]\nlen(country_table)\n\n254\n\n\n\nSubset Data From a Table\nDoing Zonal statistics for more than 200 countries may take a while, therefore, we can subset the data randomly with the .sample() method.\n\ncountry_sample = country_table.sample(n=15)\ncountry_sample\n\n\n\n\n\n  \n    \n      \n      NAME_0\n      GID_0\n    \n  \n  \n    \n      251\n      Samoa\n      WSM\n    \n    \n      76\n      Fiji\n      FJI\n    \n    \n      72\n      Spain\n      ESP\n    \n    \n      200\n      Senegal\n      SEN\n    \n    \n      183\n      Philippines\n      PHL\n    \n    \n      231\n      Trinidad and Tobago\n      TTO\n    \n    \n      7\n      United Arab Emirates\n      ARE\n    \n    \n      226\n      Tajikistan\n      TJK\n    \n    \n      166\n      New Caledonia\n      NCL\n    \n    \n      260\n      Zambia\n      ZMB\n    \n    \n      112\n      British Indian Ocean Territory\n      IOT\n    \n    \n      77\n      Falkland Islands\n      FLK\n    \n    \n      259\n      South Africa\n      ZAF\n    \n    \n      139\n      Lithuania\n      LTU\n    \n    \n      212\n      South Sudan\n      SSD"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#zonal-statistics-for-each-administrative-area",
    "href": "m203-grdiv1-pm25.html#zonal-statistics-for-each-administrative-area",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Zonal Statistics for Each Administrative Area",
    "text": "Zonal Statistics for Each Administrative Area\nrasterstats has a funcion zonal_stats() which allows you to use vectors to summarize raster data. We summarize GRDIv1 data to calculate the following statistics: count, minimum, mean, max, median, standard deviation, range, and percentiles 20, 40, 60, and 80.\n\nstats_results = gpd.GeoDataFrame()\n\nfor index, row in country_sample.iloc[:].iterrows():\n    country = row['NAME_0']\n    country_GID = row['GID_0']\n    try:\n        country_poly =  pygadm.Items(admin=country_GID, content_level=0)\n    except:\n        print(country, \" skipped.\")\n        continue\n\n    # Create a mask for the polygons\n    grdi_country_zs= rasterstats.zonal_stats(country_poly, grdi_raster.values[0], affine=grdi_raster.rio.transform(), stats=\"count min mean max median std median range percentile_20 percentile_40 percentile_60 percentile_80\")\n    # # pm25_country_zs= rasterstats.zonal_stats(country_poly, pm25_arr, affine=pm25_transform, stats=\"count min mean max median std median range percentile_20 percentile_40 percentile_60 percentile_80\", nodata=pm25_raster.nodata)\n    # # Extract statistics into a dictionary\n    country_stats = {\n        'Country_Name': country,\n        'Country_GID' : country_GID,\n        'GRDI_Count': grdi_country_zs[0]['count'],\n        'GRDI_Min': grdi_country_zs[0]['min'],\n        'GRDI_Mean': grdi_country_zs[0]['mean'],\n        'GRDI_Max': grdi_country_zs[0]['max'],\n        'GRDI_Median': grdi_country_zs[0]['median'],\n        'GRDI_Std': grdi_country_zs[0]['std'],\n        'GRDI_Range': grdi_country_zs[0]['range'],\n        'GRDI_P20': grdi_country_zs[0]['percentile_20'],\n        'GRDI_P40': grdi_country_zs[0]['percentile_40'],\n        'GRDI_P60': grdi_country_zs[0]['percentile_60'],\n        'GRDI_P80': grdi_country_zs[0]['percentile_80'],\n        #     # 'PM25_Count': pm25_country_zs[0]['count'],\n        #     # 'PM25_Min': pm25_country_zs[0]['min'],\n        # 'PM25_Mean': pm25_country_zs[0]['mean'],\n        #     # 'PM25_Max': pm25_country_zs[0]['max'],\n        #     # 'PM25_Median': pm25_country_zs[0]['median'],\n        #     # 'PM25_Std': pm25_country_zs[0]['std'],\n        #     # 'PM25_Range': pm25_country_zs[0]['range'],\n        #     # 'PM25_P20': pm25_country_zs[0]['percentile_20'],\n        #     # 'PM25_P40': pm25_country_zs[0]['percentile_40'],\n        #     # 'PM25_P60': pm25_country_zs[0]['percentile_60'],\n        #     # 'PM25_P80': pm25_country_zs[0]['percentile_80'],\n        'geometry' : country_poly['geometry'].iloc[0]\n    }\n    country_stats_gdf = gpd.GeoDataFrame([country_stats], geometry='geometry')\n    # stats_results.append(country_stats_gdf)\n    stats_results = pd.concat([stats_results, country_stats_gdf], ignore_index=True)\n\nLet’s use the .head() method from Pandas to check the top of our table\n\nstats_results.head()\n\n\n\n\n\n  \n    \n      \n      Country_Name\n      Country_GID\n      GRDI_Count\n      GRDI_Min\n      GRDI_Mean\n      GRDI_Max\n      GRDI_Median\n      GRDI_Std\n      GRDI_Range\n      GRDI_P20\n      GRDI_P40\n      GRDI_P60\n      GRDI_P80\n      geometry\n    \n  \n  \n    \n      0\n      Samoa\n      WSM\n      893\n      16.779119\n      72.627572\n      82.130798\n      75.354385\n      9.305466\n      65.351679\n      69.140343\n      73.877106\n      76.735214\n      79.172623\n      MULTIPOLYGON (((-171.4079 -14.0746, -171.4055 ...\n    \n    \n      1\n      Fiji\n      FJI\n      4899\n      11.950370\n      72.009945\n      86.728622\n      74.518845\n      9.951100\n      74.778253\n      69.718742\n      73.071259\n      75.752144\n      77.892212\n      MULTIPOLYGON (((178.1063 -19.1592, 178.1064 -1...\n    \n    \n      2\n      Spain\n      ESP\n      302383\n      2.526078\n      53.366115\n      72.849083\n      58.745010\n      12.811808\n      70.323005\n      47.256508\n      56.190113\n      60.127953\n      62.182636\n      MULTIPOLYGON (((-17.9632 27.6904, -17.9657 27....\n    \n    \n      3\n      Senegal\n      SEN\n      32706\n      26.157717\n      85.125428\n      99.563820\n      88.179062\n      9.393603\n      73.406103\n      83.247147\n      86.728622\n      88.949966\n      89.592674\n      MULTIPOLYGON (((-15.9534 12.4426, -16.0378 12....\n    \n    \n      4\n      Philippines\n      PHL\n      134581\n      6.626353\n      72.399834\n      91.909012\n      75.517365\n      11.636341\n      85.282659\n      68.165047\n      73.975517\n      76.879448\n      79.703705\n      MULTIPOLYGON (((119.4706 4.5911, 119.4689 4.58..."
  },
  {
    "objectID": "m203-grdiv1-pm25.html#defining-a-funtion",
    "href": "m203-grdiv1-pm25.html#defining-a-funtion",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Defining a Funtion",
    "text": "Defining a Funtion\nWe can create a custom function that can allow us to use the zonal statistics process multiple times. A custom function can be created using the def FUNCTION_NAME(PARAMETER1, PARAMETER2): fuction to define what the fucntion will do.\n\ndef calculate_country_stats(country_sample, grdi_raster, pm25_raster=None):\n    \"\"\"\n    Calculate statistics for each country in the sample.\n\n    Parameters:\n    - country_sample: A pandas DataFrame containing country information with 'NAME_0' and 'GID_0' columns, in this case the country_table.\n    - grdi_raster: A raster object with which to perform the zonal statistics.\n    - pm25_raster: (Optional) A raster object for PM2.5 data. If provided, statistics will also be calculated for this raster.\n\n    Returns:\n    - stats_results: A GeoDataFrame containing the statistics for each country.\n    \"\"\"\n    stats_results = gpd.GeoDataFrame()\n\n    for index, row in country_sample.iloc[:].iterrows():\n        country = row['NAME_0']\n        country_GID = row['GID_0']\n        try:\n            country_poly = pygadm.Items(admin=country_GID, content_level=0)\n        except Exception as e:\n            print(country, \"skipped due to error:\", e)\n            continue\n\n        # Create a mask for the polygons and perform zonal statistics on GRDI raster\n        grdi_country_zs = rasterstats.zonal_stats(\n            country_poly, grdi_raster.values[0], \n            affine=grdi_raster.rio.transform(), \n            stats=\"count min mean max median std range percentile_20 percentile_40 percentile_60 percentile_80\"\n        )\n\n        # Uncomment and update the following lines if you want to include PM2.5 statistics\n        # if pm25_raster is not None:\n        #     pm25_country_zs = rasterstats.zonal_stats(\n        #         country_poly, pm25_raster.values[0], \n        #         affine=pm25_raster.rio.transform(), \n        #         stats=\"count min mean max median std range percentile_20 percentile_40 percentile_60 percentile_80\", \n        #         nodata=pm25_raster.nodata\n        #     )\n\n        # Extract statistics into a dictionary\n        country_stats = {\n            'Country_Name': country,\n            'Country_GID' : country_GID,\n            'GRDI_Count': grdi_country_zs[0]['count'],\n            'GRDI_Min': grdi_country_zs[0]['min'],\n            'GRDI_Mean': grdi_country_zs[0]['mean'],\n            'GRDI_Max': grdi_country_zs[0]['max'],\n            'GRDI_Median': grdi_country_zs[0]['median'],\n            'GRDI_Std': grdi_country_zs[0]['std'],\n            'GRDI_Range': grdi_country_zs[0]['range'],\n            'GRDI_P20': grdi_country_zs[0]['percentile_20'],\n            'GRDI_P40': grdi_country_zs[0]['percentile_40'],\n            'GRDI_P60': grdi_country_zs[0]['percentile_60'],\n            'GRDI_P80': grdi_country_zs[0]['percentile_80'],\n            'geometry' : country_poly['geometry'].iloc[0]\n        }\n\n        # If PM2.5 statistics are calculated, add them to the dictionary\n        # if pm25_raster is not None:\n        #     country_stats.update({\n        #         'PM25_Count': pm25_country_zs[0]['count'],\n        #         'PM25_Min': pm25_country_zs[0]['min'],\n        #         'PM25_Mean': pm25_country_zs[0]['mean'],\n        #         'PM25_Max': pm25_country_zs[0]['max'],\n        #         'PM25_Median': pm25_country_zs[0]['median'],\n        #         'PM25_Std': pm25_country_zs[0]['std'],\n        #         'PM25_Range': pm25_country_zs[0]['range'],\n        #         'PM25_P20': pm25_country_zs[0]['percentile_20'],\n        #         'PM25_P40': pm25_country_zs[0]['percentile_40'],\n        #         'PM25_P60': pm25_country_zs[0]['percentile_60'],\n        #         'PM25_P80': pm25_country_zs[0]['percentile_80'],\n        #     })\n\n        country_stats_gdf = gpd.GeoDataFrame([country_stats], geometry='geometry')\n        stats_results = pd.concat([stats_results, country_stats_gdf], ignore_index=True)\n\n    return stats_results\n\nFrom the table above, we can choose an attribute, or column, to display it in a map plot. In this case, I’m choosing the GRDI Max\n\ncolumn_chosen = 'GRDI_Max' #GRDI_Max, GRDI_Min, GRDI_Median\n# Plotting\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nstats_results.plot(column=column_chosen, ax=ax, legend=True,\n    legend_kwds={'label': f\"{column_chosen} per country.\",\n                      'orientation': \"horizontal\"})\nax.set_title('Choropleth Map Showing GRDI Mean per country')\nax.set_axis_off()  # Turn off the axis numbers and ticks\nplt.show()\n\n\n\n\n\nSelecting Data by Column\nStart my creating a list of countries that you are interested in to Subset data from the DataFrame that match the values in the NAME_0 column. The .isin() mehthod checks each element in the DataFrame’s column for the item present in the list and returns matching rows.\n\n# selected_countries = [\"Algeria\", \"Somalia\", \"Colombia\", \"Timor Leste\", \"Finland\", \"Nicaragua\", \"United Kingdom\", \"Mali\"]\n# selected_countries = [\"Anguilla\", \"Armenia\", \"Angola\", \"Argentina\", \"Albania\", \"United Arab Emirates\", \"American Samoa\", \"Australia\" ]\nselected_countries = [\"Algeria\", \"Somalia\", \"Colombia\", \"Timor Leste\", \"Finland\", \"Nicaragua\", \"United Kingdom\", \"Mali\", \"Armenia\", \"Argentina\",  \"Albania\", \"United Arab Emirates\", \"Indonesia\", \"Qatar\"]\n\n#use the list above to subset the country_table DataFrame by the column NAME_0 \nselected_countries = country_table[country_table['NAME_0'].isin(selected_countries)]\n\n\n\nUsing a Defined Custom Function\nRecalling the defined fucntion calculate_country_stats, we can use our selected_countries list, and the GRDI and PM2.5 rasters, to create a new table of zonal statistics.\n\nstats_results = calculate_country_stats(selected_countries, grdi_raster)\n\nShow the head of the table again:\n\nstats_results.head()\n\n\n\n\n\n  \n    \n      \n      Country_Name\n      Country_GID\n      GRDI_Count\n      GRDI_Min\n      GRDI_Mean\n      GRDI_Max\n      GRDI_Median\n      GRDI_Std\n      GRDI_Range\n      GRDI_P20\n      GRDI_P40\n      GRDI_P60\n      GRDI_P80\n      geometry\n    \n  \n  \n    \n      0\n      Albania\n      ALB\n      19076\n      8.272310\n      61.513866\n      75.395561\n      66.220139\n      11.644754\n      67.123251\n      55.861691\n      64.445587\n      67.104332\n      68.589424\n      MULTIPOLYGON (((20.0541 39.6917, 20.0389 39.69...\n    \n    \n      1\n      United Arab Emirates\n      ARE\n      18229\n      5.732072\n      42.347647\n      67.470955\n      45.034630\n      17.949307\n      61.738883\n      24.688160\n      38.623692\n      50.710217\n      61.909931\n      MULTIPOLYGON (((54.1541 22.7548, 53.3313 22.85...\n    \n    \n      2\n      Argentina\n      ARG\n      474297\n      7.572341\n      66.341158\n      81.701645\n      68.789696\n      10.416561\n      74.129304\n      66.774643\n      68.278297\n      69.335510\n      71.098465\n      MULTIPOLYGON (((-66.5458 -55.061, -66.5486 -55...\n    \n    \n      3\n      Armenia\n      ARM\n      9108\n      6.944193\n      59.871617\n      73.707901\n      64.272858\n      12.315193\n      66.763708\n      52.692970\n      61.288189\n      66.743027\n      69.004845\n      MULTIPOLYGON (((45.8319 39.8311, 45.8448 39.82...\n    \n    \n      4\n      Colombia\n      COL\n      223557\n      11.956444\n      71.523674\n      84.922409\n      73.350586\n      8.762370\n      72.965965\n      69.966934\n      72.190056\n      74.405403\n      76.502792\n      MULTIPOLYGON (((-77.491 4.1451, -77.4985 4.140...\n    \n  \n\n\n\n\nPlot the map again choosing a column to plot:\n\ncolumn_chosen = 'GRDI_Max' #GRDI_Max, GRDI_Min, GRDI_Median\nstats_results.plot(column=column_chosen, legend=True)\nplt.show()"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#creating-a-table-with-results",
    "href": "m203-grdiv1-pm25.html#creating-a-table-with-results",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Creating a Table with Results",
    "text": "Creating a Table with Results\nWe can create a list of tuples that we can use to refer to the GRDI statistical values, and the name, color, and symbol we want to assign. In this case, we are using the GRDI zonal statistics of each country we selected that include the Mean, Minimum, Maximum, and interquartiles.\n\n# List of GRDI values and their corresponding properties\n#column, value name, color, symbol\ngrdi_data = [\n    ('GRDI_Mean', 'Mean', 'orange', 'diamond'),\n    ('GRDI_Min', 'Min', 'gray', '152'),\n    ('GRDI_Max', 'Max', 'gray', '151'),\n    ('GRDI_P20', 'Q20', 'blue', '142'),\n    ('GRDI_P40', 'Q40', 'purple', '142'),\n    ('GRDI_P60', 'Q60', 'green', '142'),\n    ('GRDI_P80', 'Q80', 'red', '142')\n]\n\nWe can create a figure to display the data based on the names colors and symbols we selected.\n\n# Create a figure\nfig = go.Figure()\n\n# Add traces to the figure based on the data\nfor col, name, color, symbol in grdi_data:\n    fig.add_trace(go.Scatter(\n        x=stats_results[col],\n        y=stats_results['Country_Name'],\n        mode='markers',\n        name=name,\n        marker=dict(color=color, size=10, symbol=symbol)\n    ))\n\n# Customize layout\nfig.update_layout(\n    title='GRDI Statistics by Country',\n    xaxis_title='GRDI Values',\n    yaxis_title='Country Name',\n    yaxis=dict(tickmode='linear'),\n    legend_title='Statistics',\n    yaxis_type='category',\n    xaxis=dict(tickvals=[0, 20, 40, 60, 80, 100])\n)\n\n# Show plot\nfig.show()"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#summarizing-pm2.5-values-by-socioeconomic-deprivation",
    "href": "m203-grdiv1-pm25.html#summarizing-pm2.5-values-by-socioeconomic-deprivation",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Summarizing PM2.5 Values by Socioeconomic Deprivation",
    "text": "Summarizing PM2.5 Values by Socioeconomic Deprivation\nConsidering the GRDI quartile values as a level of socieoeconomic deprivation within each country, we can use the stats_results GeoDataFrame, the GRDI raster, and the PM2.5 raster to calculate the Mean PM.25 value within each of those areas in each country. This can describe how the air quality for different socioeconomic strata compare within the country, as well as against other countries.\nThe results will be added to the stats_results with the corresponting columns.\n\n# iterate through the stats_results table rows\nfor index, row in stats_results.iloc[:].iterrows():\n    #isolate each country's respective row\n    row_df = gpd.GeoDataFrame([row], geometry='geometry').reset_index(drop=True)\n    print(row_df.loc[0,'Country_GID'])\n    try:\n        #use rioxarray to clip the GRDI and PM2.5 rasters by the geometry of the respective country.\n        grdi_country = grdi_raster.rio.clip(row_df.geometry, grdi_raster.rio.crs)\n        pm25_country = pm25_raster.rio.clip(row_df.geometry, grdi_raster.rio.crs)\n    except:\n        print('Error in clip')\n        continue\n\n    #Applying squeeze() to this array removes the singleton dimension, reducing it to a 2D array with dimensions (rows, columns)\n    grdi_country= grdi_country.squeeze()\n    pm25_country= pm25_country.squeeze()\n\n\n    # Subset the GRDI raster where values fall between each GRDI quintiles\n    grdi_countryQ1 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_Min']) & (grdi_country <= row_df.loc[0, 'GRDI_P20']))\n    grdi_countryQ2 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P20']) & (grdi_country <= row_df.loc[0, 'GRDI_P40']))\n    grdi_countryQ3 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P40']) & (grdi_country <= row_df.loc[0, 'GRDI_P60']))\n    grdi_countryQ4 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P60']) & (grdi_country <= row_df.loc[0, 'GRDI_P80']))\n    grdi_countryQ5 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P80']) & (grdi_country <= row_df.loc[0, 'GRDI_Max']))\n\n\n    # Mask the PM2.5 raster using the above GRDI quartile rasters, keeping only the cells that intersect\n    pm25_countryQ1 = pm25_country.where(grdi_countryQ1.notnull())\n    pm25_countryQ2 = pm25_country.where(grdi_countryQ2.notnull())\n    pm25_countryQ3 = pm25_country.where(grdi_countryQ3.notnull())\n    pm25_countryQ4 = pm25_country.where(grdi_countryQ4.notnull())\n    pm25_countryQ5 = pm25_country.where(grdi_countryQ5.notnull())\n\n    #Find the mean value of of the intersected PM2.5 rasters in each quartile\n    pm25_countryQ1v = pm25_countryQ1.mean().item()\n    pm25_countryQ2v = pm25_countryQ2.mean().item()\n    pm25_countryQ3v = pm25_countryQ3.mean().item()\n    pm25_countryQ4v = pm25_countryQ4.mean().item()\n    pm25_countryQ5v = pm25_countryQ5.mean().item()\n\n    #add the resuts to the stats_results table in the respective column\n    stats_results.at[index, 'PM25_Q1'] = pm25_countryQ1v\n    stats_results.at[index, 'PM25_Q2'] = pm25_countryQ2v\n    stats_results.at[index, 'PM25_Q3'] = pm25_countryQ3v\n    stats_results.at[index, 'PM25_Q4'] = pm25_countryQ4v\n    stats_results.at[index, 'PM25_Q5'] = pm25_countryQ5v\n\nALB\n\n\nARE\n\n\nARG\n\n\nARM\n\n\nCOL\n\n\nDZA\n\n\nFIN\n\n\nGBR\n\n\nIDN\n\n\nMLI\n\n\nNIC\n\n\nQAT\n\n\nSOM\n\n\n\nstats_results.head()\n\n\n\n\n\n  \n    \n      \n      Country_Name\n      Country_GID\n      GRDI_Count\n      GRDI_Min\n      GRDI_Mean\n      GRDI_Max\n      GRDI_Median\n      GRDI_Std\n      GRDI_Range\n      GRDI_P20\n      GRDI_P40\n      GRDI_P60\n      GRDI_P80\n      geometry\n      PM25_Q1\n      PM25_Q2\n      PM25_Q3\n      PM25_Q4\n      PM25_Q5\n    \n  \n  \n    \n      0\n      Albania\n      ALB\n      19076\n      8.272310\n      61.513866\n      75.395561\n      66.220139\n      11.644754\n      67.123251\n      55.861691\n      64.445587\n      67.104332\n      68.589424\n      MULTIPOLYGON (((20.0541 39.6917, 20.0389 39.69...\n      15.438293\n      15.031855\n      14.699892\n      14.612475\n      15.709009\n    \n    \n      1\n      United Arab Emirates\n      ARE\n      18229\n      5.732072\n      42.347647\n      67.470955\n      45.034630\n      17.949307\n      61.738883\n      24.688160\n      38.623692\n      50.710217\n      61.909931\n      MULTIPOLYGON (((54.1541 22.7548, 53.3313 22.85...\n      47.710175\n      47.893559\n      47.822292\n      48.220722\n      49.729328\n    \n    \n      2\n      Argentina\n      ARG\n      474297\n      7.572341\n      66.341158\n      81.701645\n      68.789696\n      10.416561\n      74.129304\n      66.774643\n      68.278297\n      69.335510\n      71.098465\n      MULTIPOLYGON (((-66.5458 -55.061, -66.5486 -55...\n      7.519970\n      5.924110\n      6.083218\n      7.562082\n      8.672854\n    \n    \n      3\n      Armenia\n      ARM\n      9108\n      6.944193\n      59.871617\n      73.707901\n      64.272858\n      12.315193\n      66.763708\n      52.692970\n      61.288189\n      66.743027\n      69.004845\n      MULTIPOLYGON (((45.8319 39.8311, 45.8448 39.82...\n      19.292490\n      16.728804\n      16.334721\n      16.337336\n      15.342737\n    \n    \n      4\n      Colombia\n      COL\n      223557\n      11.956444\n      71.523674\n      84.922409\n      73.350586\n      8.762370\n      72.965965\n      69.966934\n      72.190056\n      74.405403\n      76.502792\n      MULTIPOLYGON (((-77.491 4.1451, -77.4985 4.140...\n      27.425064\n      29.047859\n      24.132767\n      22.309097\n      20.367249"
  },
  {
    "objectID": "m203-grdiv1-pm25.html#plot-results-of-mean-pm2.5-in-socieceonomic-deprivation-quartiles-per-country",
    "href": "m203-grdiv1-pm25.html#plot-results-of-mean-pm2.5-in-socieceonomic-deprivation-quartiles-per-country",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Plot Results of Mean PM2.5 in Socieceonomic Deprivation Quartiles per country",
    "text": "Plot Results of Mean PM2.5 in Socieceonomic Deprivation Quartiles per country\nSimilarly, we create a list of tuples of how we want to display the data, and create a figure based on the tuples. This plot would show each country in the y axis and the Log of Mean PM2.5 values in each country’s GRDI quarties.\n\n# List of GRDI values and their corresponding properties\n#column, value name, color, symbol\nplot_data =[\n    ('PM25_Q1', 'Q1', '#440154', '6'),  # Light Blue\n    ('PM25_Q2', 'Q2', '#31688E', '5'),  # Light Green\n    ('PM25_Q3', 'Q3', '#35B779', '7'),  # Yellow\n    ('PM25_Q4', 'Q4', '#FDE725', '8'),  # Orange\n    ('PM25_Q5', 'Q5', '#FF0000', '1')   # Red\n]\n\n# Create a figure\nfig = go.Figure()\n\n# Add traces to the figure.\nfor col, name, color, symbol in plot_data:\n    xlog  = np.log(stats_results[col])\n    fig.add_trace(go.Scatter(\n        x=xlog,\n        y=stats_results['Country_Name'],\n        mode='markers+text',  # Add 'text' to mode\n        text=[f'<b>{name}</b>' for _ in stats_results[col]],  # Repeat name for each point\n        name=name,\n        textfont=dict(color=color, size=12),\n        textposition='top center',  # Position the text above the symbol\n        marker_color=color,\n        marker_line_color=\"midnightblue\",\n        marker_symbol=symbol,\n        marker_size=14,\n        marker_line_width=2,\n        marker_opacity=0.6\n        ))\nfig.update_traces(textposition='top center')\n\n    # Customize layout\nfig.update_layout(\n    title='Mean PM2.5 in each GRDI Quartile by Country',\n    xaxis_title='Log of PM2.5 Mean Values',\n    yaxis_title='Country Name',\n    yaxis=dict(tickmode='linear'),\n    legend_title='Statistics',\n    yaxis_type='category',\n    xaxis=dict(rangemode=\"tozero\"),\n    \n    #xaxis=dict(tickvals=[0, 20, 40, 60, 80, 100])\n    )\n\n# Show plot\nfig.show()\n\n\n                                                \n\n\nUse the plotly controls to take a closer look at the results.\nWith this shapely plot, We can examine differences between countires and PM2.5 values. The plot displays the coutnries on the Y axis and log values of the average PM2.5 value on the X axis. Each country displays PM2.5 values averaged within the quartile areas based on GRDI values of each country. A higher quartile (Q) implies a higher degree of deprivation, 1 being the lowest and 5 the highest.\nCongratulations! …. Now you should be able to:\n\nTest test…"
  },
  {
    "objectID": "m202-svi-tri-icis-places.html",
    "href": "m202-svi-tri-icis-places.html",
    "title": "Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit",
    "section": "",
    "text": "In this lesson, we will introduce you to 3 public health and air quality datasets: the United States Environmental Protection Agency’s (EPA’s) Toxic Release Inventory (TRI), the EPA’s Integrated Compliance Information System for Air (ICIS-AIR), and the U.S. Centers For Diesease Control and Prevention’s (CDC’s) PLACES health outcomes dataset. You will learn how to form queries and retrieve each dataset from their respective API, process the returned object into a useable format, create visualizations and perform simple analysis. You will also use the CDC’s Social Vulnerability Index (SVI), available through NASA, to examine socioeconomic patterns in the greater Detroit metropolitan area and explore relationships with public health.\n\n\n\n\n\n\nProgramming Reminder\n\n\n\nThis lesson uses the Python programming environment.\n\n\n\n\n\n\n\n\n\n\nData Science Review\n\n\n\nAn Application Programming Interface (API) is a set of protocols, tools, and definitions that enable different software applications to communicate with each other. It acts as an intermediary, allowing one application to request specific data or actions from another application or service without needing access to its entire codebase. APIs define the methods and data formats that applications can use to request and exchange information, typically through specific endpoints (URLs) that accept requests and return responses in standardized formats like JSON. They are crucial for integrating external services, accessing databases, and enabling communication between different parts of larger open science software systems."
  },
  {
    "objectID": "m202-svi-tri-icis-places.html#overview",
    "href": "m202-svi-tri-icis-places.html#overview",
    "title": "Exploring SVI, TRI, and Health Outcomes",
    "section": "",
    "text": "In this lesson, we will introduce you to 3 public health and air quality datasets. These include the EPA’s Toxic Release Inventory (TRI), the EPA’s Integrated Compliance Information System for Air (ICIS-AIR), and the CDC’s PLACES health outcomes dataset. You will learn how to form queries and retrieve each dataset from their respective API, process the returned object into a useable format, create visualizations and perform simple analysis. You will also use NASA’s Social Vulnerability Index (SVI) to examine socioeconomic patterns in the greater Detroit metropolitan area and explore relationships with public health.\n\n\n\n\n\n\nProgramming Reminder\n\n\n\nThis lesson uses the Python programming environment."
  },
  {
    "objectID": "m202-svi-tri-icis-places.html#learning-objectives",
    "href": "m202-svi-tri-icis-places.html#learning-objectives",
    "title": "Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this lesson, you should be able to:\n\nRetrieve multiple public health datasets through the EPA and CDC’s APIs.\nCreate multipanel raster maps from multilayered SVI data.\nGeolocate tabular data using latitude and longitude.\nCreate maps joining geolocated points and basemaps.\nCreate maps with graduated point symbols to visualize point source pollution releases.\nInterpolate a raster surface from point spatial data.\nPerform raster math to create specialized indexes.\nPerform spatial correlation analysis.\n\n\n\n\n\n\n\nPreface: A Note on Data Interpretation and Ethical Considerations\n\n\n\nThis lesson is designed to introduce you to various environmental, public health, and socioeconomic datasets, and to provide you with the tools to analyze and visualize this information; however, it is crucial to approach this material with an understanding of its sensitive nature and potential implications.\nThe topics of environmental justice, institutional racism, socioeconomic conditions, and pollution are complex and multifaceted. The data and analyses presented in this lesson are not intended to draw definitive conclusions or suggest scientific evidence of cause and effect relationships. Rather, they are meant to equip you with the skills to investigate data and perform analyses that could be applied to your local communities.\nAs you work through this lesson, remember that correlation does not imply causation. The patterns you may observe in the data could be influenced by factors not represented in these datasets. Approach your findings with caution, and consider the broader historical, social, and economic contexts that shape environmental and health outcomes in different communities.\nThis lesson will empower you with data literacy and analytical skills. We encourage you to use these tools and skills responsibly. Consider the ethical implications of your analyses and the potential impact on the communities represented in the data. When drawing insights or making decisions based on such analyses, it’s crucial to involve community stakeholders, consider multiple perspectives, and seek additional expertise when necessary."
  },
  {
    "objectID": "m202-svi-tri-icis-places.html#introduction",
    "href": "m202-svi-tri-icis-places.html#introduction",
    "title": "Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit",
    "section": "Introduction",
    "text": "Introduction\nIn many urban areas, air quality issues disproportionately affect low-income communities and communities of color (Tessum et al. 2021). This disparity is a key focus of environmental justice efforts. In cities like Detroit, Chicago, and Cleveland industrial facilities, highways, and other pollution sources are often concentrated in disadvantaged neighborhoods. Detroit has a history of industrial pollution and is working to address air quality issues in areas like Southwest Detroit, where residents face higher exposure to pollutants from nearby industries and heavy truck traffic. Similarly, Chicago has seen community efforts to address air pollution in areas like the Southeast Side, where residents have fought against polluting industries and advocated for stricter regulations.\nThe EPA is the primary federal agency responsible for environmental protection in the United States. It sets and enforces air quality standards, including the National Ambient Air Quality Standards (NAAQS) for six criteria pollutants. The EPA also maintains the AirNow system, which provides real-time air quality data to the public. While primarily focused on public health, the CDC plays a crucial role in understanding the health impacts of air pollution. It conducts research on the relationships between air quality and various health outcomes, and provides guidance on protecting public health from environmental hazards.\nIn response to these challenges, community-driven science initiatives have emerged. These efforts involve local residents in collecting data on air quality and other environmental factors, often using low-cost sensors and mobile monitoring techniques. This approach helps fill gaps in official monitoring networks and empowers communities to advocate for themselves. Open data is crucial for community-driven solutions in several ways:\n\nTransparency: Open data allows communities to verify official reports and hold authorities accountable.\nAccessibility: When air quality data is freely available, communities can use it to inform local decision-making and advocacy efforts.\nInnovation: Open data enables researchers, activists, and tech developers to create new tools and analyses that can benefit communities.\nCollaboration: Open data facilitates collaboration between communities, scientists, and policymakers, leading to more effective solutions.\n\nWhile the EPA and CDC provide federal networks of open-access datasets like the Air Quality System (AQS), TRI, ICIS-AIR, and Population Level Analysis and Community Estimates (PLACES) collections, community driven data collection is a large part of driving change in traditionally underrepresented communities. In Chicago, the Array of Things project has installed sensors throughout the city, providing open data on various environmental factors including air quality, while Detroit’s Community Air Monitoring Project uses low-cost sensors to collect and share air quality data in areas underserved by official monitoring stations.\nWith access to open data, communities can:\n\nIdentify local air quality hotspots that may be missed by sparse official monitoring networks.\nCorrelate air quality data with health outcomes to strengthen advocacy efforts.\nDevelop targeted interventions, such as promoting indoor air filtration on high-pollution days.\nCreate custom alerts and information systems tailored to local needs.\n\nAlthough open data provides many benefits, challenges still remain. These include: 1) ensuring data quality and consistency, especially when integrating data from various sources; 2) addressing the “digital divide” to ensure all community members can access and use the data; 3) balancing the need for detailed local data with privacy concerns; and 4) building capacity within communities to effectively use and interpret complex environmental data.\n\n\n\n\n\n\nEJ in the News\n\n\n\nThe Clear the Air Coalition, a new environmental justice advocacy group in Michigan, is calling for stronger protection of vulnerable communities from pollution. Launched in Detroit, the coalition argues that state regulators focus too much on technical compliance with environmental laws rather than public health. They claim the current permitting process favors polluters and fails to consider the cumulative impacts of pollution on overburdened communities. The group seeks changes in state law and a shift in mindset from environmental regulators.\n\n\n\nClear the Air Coalition\n\n\nThe Michigan Department of Environment, Great Lakes, and Energy (EGLE) emphasizes its commitment to protecting underserved communities and notes improvements in air quality over recent decades, but Coalition members argue that the current approach is insufficient. They call for regulators to consider the combined effects of multiple pollution sources in marginalized areas and to give local governments more power to reject new polluting facilities in already burdened communities.\n\n\nThe Michigan EGLE and the Office of the Environmental Justice Public Advocate are two of the more active state departments aiming to address issues of the environment, public health, and social justice. Here are some of the projects enacted in greater Detroit area:\n\nDetroit Environmental Agenda: This community-led initiative works closely with EGLE to address environmental concerns in Detroit. It focuses on issues like air quality, water quality, and waste management.\n48217 Community Air Monitoring Project: Named after the zip code of a heavily industrialized area in Southwest Detroit, this project involves community members working with EGLE to monitor air quality using low-cost sensors.\nDetroit Climate Action Plan: Developed in partnership with EGLE, this plan addresses climate change impacts on the city, with a focus on vulnerable communities.\nDelray Neighborhood Initiatives: EGLE has been involved in efforts to address air quality concerns in the Delray neighborhood, which is impacted by industrial emissions and heavy truck traffic.\nGreen Door Initiative: This Detroit-based organization collaborates with EGLE on various environmental justice projects, including lead abatement and air quality improvement efforts.\nDetroit River Sediment Cleanup: EGLE has been involved in efforts to clean up contaminated sediments in the Detroit River, which disproportionately affects nearby low-income communities.\nAsthma Prevention Programs: EGLE supports community-based asthma prevention programs in Detroit, recognizing the link between air quality and asthma rates in disadvantaged neighborhoods.\n\nThese are just a few examples that demonstrate the ongoing collaboration between community groups, local government, and EGLE to address environmental justice concerns in Detroit.\nAir quality issues in urban areas disproportionately affect low-income communities and communities of color, making it a critical focus for environmental justice efforts. Federal agencies like the EPA and CDC play crucial roles in setting standards and conducting research, but community-driven science initiatives have emerged as powerful tools for local action. Open data is key to these efforts, enabling transparency, accessibility, innovation, and collaboration. Cities like Detroit and Chicago are at the forefront of these initiatives, with projects that empower residents to monitor and advocate for better air quality. While challenges remain, particularly in data management and accessibility, the collaboration between community groups, local governments, and state agencies like Michigan’s EGLE demonstrates a promising path forward."
  },
  {
    "objectID": "m202-svi-tri-icis-places.html#data-analysis-and-exercises",
    "href": "m202-svi-tri-icis-places.html#data-analysis-and-exercises",
    "title": "Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit",
    "section": "Data Analysis and Exercises",
    "text": "Data Analysis and Exercises\nTo better understand and address these complex issues, we’ll work through a series of coding and data science exercises. These hands-on activities will allow users to explore some of the open datasets and tools that are available to community members and stakeholders that offer practical insights into air quality, public health, and social vulnerability.\nWe’ll begin with the ICIS-AIR dataset, then move on to the TRI Facility dataset, and finally work our way down to TRI Form A. After exploring these 3 air pollution datasets, we will take a look at the CDC PLACES health outcomes data and perform some simple analysis integrating the pollution and health outcomes data.\n\n\n\n\n\n\nData Science Review\n\n\n\nThis lesson uses the following Python modules:\n\npandas: Essential for data manipulation and analysis.\ngeopandas: Extends pandas functionality to handle geospatial data.\nmatplotlib: Used for creating static, animated, and interactive visualizations.\nnumpy: Provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays.\nrequests: Allows you to send HTTP requests and interact with APIs easily.\ncontextily: Adds basemaps to your plots, enhancing the visual context of your geospatial data.\npygris: Simplifies the process of working with US Census Bureau TIGER/Line shapefiles.\nrasterio: Reads and writes geospatial raster datasets.\nxarray: Introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays, making it easier to work with labeled multi-dimensional arrays.\nshapely: Manipulation and analysis of geometric objects in the Cartesian plane.\nscipy: Used for scientific and technical computing, particularly the stats and interpolate submodules.\nrioxarray: Extends xarray to make it easier to handle geospatial raster data.\ntime: Provides various time-related functions (part of Python’s standard library).\ntabulate: Used for creating nicely formatted tables in various output formats.\npysal: A library of spatial analysis functions.\nsplot: Provides statistical plots for spatial analysis.\n\nIf you’d like to learn more about the functions used in this lesson, you can refer to the documentation on their respective websites.\nThe pandas module is essential for data manipulation and analysis, while geopandas extends its functionality to handle geospatial data. matplotlib is used for creating static, animated, and interactive visualizations. numpy provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\nThe requests module allows you to send HTTP requests and interact with APIs easily. contextily adds basemaps to your plots, enhancing the visual context of your geospatial data. pygris simplifies the process of working with US Census Bureau TIGER/Line shapefiles.\nrasterio and xarray are used for working with geospatial raster data. rasterio reads and writes geospatial raster datasets, while xarray introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays, making it easier to work with labeled multi-dimensional arrays.\nshapely is used for manipulation and analysis of geometric objects. scipy provides additional tools for scientific computing, including statistical functions and interpolation methods. rioxarray combines the functionality of rasterio and xarray for easier handling of geospatial raster data.\npysal is a library for spatial analysis, providing tools for exploratory spatial data analysis. splot is a visualization module that works with pysal to create statistical plots for spatial analysis.\nMake sure these modules are installed before you begin working with the code in this document.\n\n\n\nICIS Air\nICIS-AIR (Integrated Compliance Information System for Air) is a comprehensive database maintained by the U.S. Environmental Protection Agency (EPA) that focuses specifically on air quality compliance and enforcement data. It tracks information related to stationary sources of air pollution, including their compliance with various Clean Air Act regulations, permit data, and enforcement actions. While ICIS-AIR and the Toxic Release Inventory (TRI) are separate systems, they have significant overlap in their coverage of industrial facilities and air emissions. Many facilities that report to TRI also have data in ICIS-AIR, providing complementary information.\nWhere TRI focuses on the quantities of specific toxic chemicals released into the air, ICIS-AIR offers a broader picture of a facility’s overall air quality compliance status, including non-toxic pollutants regulated under the Clean Air Act. Together, these systems provide a more comprehensive view of industrial air pollution sources, enabling researchers, regulators, and the public to assess both the types and quantities of air pollutants emitted (from TRI) and the regulatory compliance status of the emitting facilities (from ICIS-AIR).\n\nData Processing\n\nDefining Our Study Area\nBefore we can query the API for ICIS-AIR regulated facilities in the greater Detroit area, we need to create a spatial object that defines our study area and that we can use throughout this analysis. The U.S. Census Bureau defines the Detroit, MI Metropolitan Statistical Area (MSA) as including 6 counties (Wayne, Macomb, Oakland, Lapeer, Livingston, and St. Clair), but for this lesson we will focus on the core 3 counties (Wayne, Macomb, and Oakland); both population and air emissions related facilities are sparser in the outer counties.\nWe can us pygris to get vector boundaries for the counties and dissolve them into a single boundary we can use for cropping data and restricting API searches.\n\nimport geopandas as gpd\nimport pandas as pd\nimport pygris\nfrom shapely.geometry import box\n\n\ncounties = ['Wayne', 'Oakland', 'Macomb']\n\n# Fetch the county boundaries for Michigan (MI) from the pygris dataset for the year 2022.\n# Then, filter the DataFrame to only include the counties in the Detroit metro area (Wayne, Oakland, Macomb).\nmetro_counties = pygris.counties(state=\"MI\", year=2022)\ndetroit_metro = metro_counties[metro_counties['NAME'].isin(counties)]\n\n# Combine the geometries of the selected counties into a single polygon (dissolve by state code).\ndetroit_metro = detroit_metro.dissolve(by='STATEFP')\n\n# Convert the dissolved geometries into a GeoDataFrame and ensure the coordinate reference system (CRS) is EPSG:4269.\ndetroit_metro = gpd.GeoDataFrame(detroit_metro, geometry='geometry', crs='EPSG:4269')\n\n# Obtain the total bounding box for the Detroit metro area polygon.\nbbox = detroit_metro.total_bounds\n\n# Create a bounding box polygon from the bounding coordinates and store it in a GeoDataFrame with the same CRS.\nbbox_polygon = gpd.GeoDataFrame(\n    geometry=[box(*bbox)],\n    crs=detroit_metro.crs\n)\n\nUsing FIPS code '26' for input 'MI'\n\n\nLet’s verify this by overlaying it on a basemap.\n\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\n# Initialize a figure and axis with a 7x7 inch size for plotting.\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Reproject the Detroit metro area and bounding box to Web Mercator (EPSG:3857) to align with the basemap.\ndetroit_metro_bm = detroit_metro.to_crs(epsg=3857)\nbbox_polygon_bm = bbox_polygon.to_crs(epsg=3857)\n\n# Plot the Detroit metro area with a light blue fill and dark blue edges at 30% opacity.\ndetroit_metro_bm.plot(ax=ax, color='lightblue', edgecolor='darkblue', alpha=0.3)\n\n# Plot the boundary of the bounding box with a grey outline and a thicker line width.\nbbox_polygon_bm.boundary.plot(ax=ax, color='grey', linewidth=2)\n\n# Add a basemap from OpenStreetMap (Mapnik) to provide background geographic context.\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Adjust the map's visible area (extent) to fit the bounding box.\nax.set_xlim(bbox_polygon_bm.total_bounds[0], bbox_polygon_bm.total_bounds[2])\nax.set_ylim(bbox_polygon_bm.total_bounds[1], bbox_polygon_bm.total_bounds[3])\n\n# Remove axis lines and labels for a cleaner map visualization.\nax.set_axis_off()\n\n# Add a title to the plot.\nplt.title(\"Detroit Metro Study Area\", fontsize=16)\n\n# Optimize layout to ensure proper spacing around the plot.\nplt.tight_layout()\n\n# Display the final plot.\nplt.show()\n\n\n\n\nThis looks correct, and we’ll use this boundary object repeatedly throughout our analysis.\n\n\n\nQuerying the API\nThe EPA’s Enforcement and Compliance History Online (ECHO) website serves as a comprehensive portal for environmental compliance and enforcement data. It includes the ECHO Data Service, which provides programmatic access to EPA data through RESTful APIs. Among these is the ICIS-AIR API, which specifically offers access to air quality compliance and enforcement data. This API allows users to retrieve detailed information about stationary sources of air pollution, including facility details, permit data, emissions reports, and compliance status.\nThe ECHO API requires that users follow a multi-step workflow to acquire data via the Facility Air API:\n\nUse get_facilities to validate parameters, get summary statistics, and obtain a query_id (QID), valid for about 30 minutes.\nUse get_qid with the obtained QID to paginate through facility results.\nUse get_map with the QID to visualize and navigate facility locations.\nUse get_download with the QID to generate a CSV file of facility information.\n\nThe get_facility_info endpoint operates independently, returning either clustered summary statistics or an array of individual facilities. This API structure allows for efficient querying, visualization, and extraction of air quality compliance data, making it a valuable resource for environmental analysis and research.\nWe’ll begin by querying this database for just Michigan facilities in the workflow detailed above. From there we’ll subset with our list of counties.\nWe start with the base url for the ECHO API and our parameter list.\n\n# Base URL for the ECHO ICIS-AIR API, which provides access to air quality and compliance data from the EPA.\nbase_url = \"https://echodata.epa.gov/echo/air_rest_services\"\n\n# Query parameters for the API call:\n# - \"output\": Specifies the response format as JSON.\n# - \"p_st\": Filters results to include only the state of Michigan (MI).\nparams = {\n    \"output\": \"JSON\",\n    \"p_st\": \"MI\"\n}\n\nNow we define 2 functions that will carry out the first 2 steps; 1) identifying the facilities in our search and 2) retrieve the facility data.\n\nimport requests\n\n# Define a function to query the ECHO ICIS-AIR API for facilities of interest in Michigan.\n# If successful, extract the Query ID (qid) and total number of facilities from the API response.\ndef get_facilities():\n    response = requests.get(f\"{base_url}.get_facilities\", params=params)\n    if response.status_code == 200:\n        data = response.json()\n        if 'Results' in data:\n            qid = data['Results']['QueryID'] # Unique Query ID to fetch detailed facility data.\n            print(f\"Query ID: {qid}\")\n            print(f\"Total Facilities: {data['Results']['QueryRows']}\")  # Number of facilities in the query result.\n            return qid\n    # Log an error if the API request fails or the data is incomplete.\n    print(\"Failed to get facilities and QID\")\n    return None\n\n# Define a function to retrieve detailed facility data using the Query ID (qid).\n# Retrieves paginated data, adding all facility results to the all_facilities list.\ndef get_facility_data(qid):\n    all_facilities = []\n    page = 1\n    while True:\n        # Query parameters include the Query ID and the page number for paginated results.\n        params = {\"qid\": qid, \"pageno\": page, \"output\": \"JSON\"}\n        response = requests.get(f\"{base_url}.get_qid\", params=params)\n        if response.status_code == 200:\n            data = response.json()\n            if 'Results' in data and 'Facilities' in data['Results']:\n                facilities = data['Results']['Facilities'] # List of facilities on the current page.\n                if not facilities:   # Break if no more facilities to retrieve.\n                    break\n                all_facilities.extend(facilities) # Add the facilities from the current page to the result list.\n                print(f\"Retrieved page {page}\")\n                page += 1\n            else:\n                break # Break if 'Results' or 'Facilities' key is missing.\n        else:\n            print(f\"Failed to retrieve page {page}\")  # Log an error if the API request fails for the current page.\n            break\n    return all_facilities\n\nNow that we’ve defined the functions we can make a request to the API with our Detroit metro counties.\n\n# Step 1: Retrieve the Query ID (qid) by querying the API for ICIS-AIR facilities in Michigan.\nqid = get_facilities()\n\nif qid:\n    # Step 2: If the Query ID is retrieved, use it to get all relevant facility data.\n    print(\"Retrieving facility data...\")\n    facilities = get_facility_data(qid)\n    \n    # Convert the list of facilities into a pandas DataFrame for easier data analysis and manipulation.\n    df_icis_air = pd.DataFrame(facilities)\n    \n    # Print the total number of facilities retrieved and display the column names in the dataset.\n    print(f\"\\nSuccessfully retrieved {len(df_icis_air)} ICIS-AIR facilities for Michigan\")\n    print(\"\\nColumns in the dataset:\")\n    print(df_icis_air.columns)\n    \nelse:\n    # If the Query ID is not retrieved, output an error message indicating the failure.\n    print(\"Failed to retrieve facility data\")\n\nQuery ID: 2\nTotal Facilities: 3408\nRetrieving facility data...\n\n\nRetrieved page 1\n\n\n\nSuccessfully retrieved 3408 ICIS-AIR facilities for Michigan\n\nColumns in the dataset:\nIndex(['AIRName', 'SourceID', 'AIRStreet', 'AIRCity', 'AIRState',\n       'LocalControlRegionCode', 'AIRZip', 'RegistryID', 'AIRCounty',\n       'AIREPARegion', 'FacFederalAgencyCode', 'FacFederalAgencyName',\n       'FacDerivedHuc', 'FacFIPSCode', 'FacIndianCntryFlg',\n       'AIRIndianCntryFlg', 'FacIndianSpatialFlg', 'FacDerivedTribes',\n       'FacUsMexBorderFlg', 'FacSICCodes', 'AIRNAICS', 'FacLat', 'FacLong',\n       'AIRPrograms', 'AIRMacts', 'AIRStatus', 'AIRUniverse',\n       'AIRClassification', 'AIRCmsCategoryCode', 'AIRCmsCategoryDesc',\n       'FacDerivedWBD', 'FacDerivedWBDName', 'ChesapeakeBayFlag', 'AIRIDs',\n       'CWAIDs', 'RCRAIDs', 'RmpIDs', 'SDWAIDs', 'TRIIDs', 'GHGIDs', 'EisIDs',\n       'CamdIDs', 'AIRComplStatus', 'AIRHpvStatus', 'AIRMnthsWithHpv',\n       'AIRQtrsWithHpv', 'AIRQtrsWithViol', 'AIRPollRecentViol',\n       'AIRRecentViolCnt', 'AIRLastViolDate', 'AIREvalCnt', 'AIRDaysLastEval',\n       'AIRLastEvalDate', 'AIRLastEvalDateEPA', 'AIRLastEvalDateState',\n       'AIRFceCnt', 'AIRDaysLastFce', 'AIRLastFceDate', 'AIRLastFceDateEPA',\n       'AIRLastFceDateState'],\n      dtype='object')\n\n\nThere are over three thousand facilities in the ICIS-AIR Michigan dataset. Here are the descriptions for some key columns.\n\n\nKey Columns in the ICIS-AIR ECHO API Dataset\n\n\n\n\n\n\n\nKey Column\nDescription\n\n\n\n\nAIRName\nThe name of the facility\n\n\nSourceID\nA unique identifier for the air pollution source\n\n\nAIRStreet, AIRCity, AIRState, AIRZip\nAddress components of the facility\n\n\nRegistryID\nA unique identifier for the facility in the EPA’s registry\n\n\nAIRCounty\nThe county where the facility is located\n\n\nAIREPARegion\nThe EPA region responsible for the facility\n\n\nAIRNAICS\nNorth American Industry Classification System code(s) for the facility\n\n\nFacLat, FacLong\nLatitude and longitude coordinates of the facility\n\n\nAIRPrograms\nAir quality programs applicable to the facility\n\n\nAIRStatus\nCurrent operational status of the facility\n\n\nAIRUniverse\nCategorization of the facility within air quality regulation\n\n\nAIRComplStatus\nCurrent compliance status under the Clean Air Act\n\n\nAIRHpvStatus\nHigh Priority Violator status\n\n\nAIRQtrsWithViol\nNumber of quarters with violations\n\n\nAIRLastViolDate\nDate of the most recent violation\n\n\nAIREvalCnt\nCount of evaluations conducted\n\n\nAIRLastEvalDate\nDate of the most recent evaluation\n\n\nAIRFceCnt\nCount of Full Compliance Evaluations\n\n\nAIRLastFceDate\nDate of the last Full Compliance Evaluation\n\n\n\nThese columns provide information about each facility’s location, operational characteristics, compliance history, and regulatory oversight under the Clean Air Act. It includes information on violations, evaluations, and compliance status that assess a facility’s environmental performance and regulatory compliance.\nWe can check some basic information like how many facilities are in Detroit metro and the breakdown by our 3 counties.\n\n# Subset the DataFrame to include only the ICIS-AIR facilities located in the Detroit metro counties (Wayne, Oakland, Macomb).\nicis_air_detroit = df_icis_air[df_icis_air['AIRCounty'].isin(counties)]\n\n# Print a summary of the total number of ICIS-AIR facilities in Michigan and those specifically in the Detroit metro area.\nprint(f\"Total ICIS-AIR facilities in Michigan: {len(df_icis_air)}\")\nprint(f\"ICIS-AIR facilities in Detroit metro area: {len(icis_air_detroit)}\")\n\n# Display the count of ICIS-AIR facilities for each county within the Detroit metro area.\nprint(\"\\nFacilities per county:\")\nprint(icis_air_detroit['AIRCounty'].value_counts())\n\nTotal ICIS-AIR facilities in Michigan: 3408\nICIS-AIR facilities in Detroit metro area: 907\n\nFacilities per county:\nAIRCounty\nWayne      434\nOakland    273\nMacomb     200\nName: count, dtype: int64\n\n\nNext we should check for facilities with missing latitude or longitude coordinates.\n\n# Identify and count the records in the Detroit metro subset where latitude or longitude values are missing.\nmissing_coords = icis_air_detroit[(icis_air_detroit['FacLat'].isnull()) | (icis_air_detroit['FacLong'].isnull())]\nprint(f\"Number of ICIS-AIR records with missing coordinates: {len(missing_coords)}\")\n\n# Remove records with missing latitude or longitude from the dataset to ensure only facilities with valid coordinates remain.\nicis_air_detroit = icis_air_detroit.dropna(subset=['FacLat', 'FacLong'])\n\nNumber of ICIS-AIR records with missing coordinates: 0\n\n\nNo missing records! That’s great. As you’ll find out later, you won’t always be so lucky.\nNow we can create a spatial object, reproject it so it matches the basemap of Detroit, and make a map showing the location of all the ICIS-AIR facilities in Detroit metro.\n\n# Convert the ICIS-AIR facilities data into a GeoDataFrame with valid point geometries (using longitude and latitude).\ngdf_icis_air = gpd.GeoDataFrame(\n    icis_air_detroit, \n    geometry=gpd.points_from_xy(icis_air_detroit.FacLong, icis_air_detroit.FacLat),\n    crs=\"EPSG:4326\" # Coordinate reference system set to WGS 84 (EPSG:4326)\n)\n\n# Reproject the ICIS-AIR facilities' GeoDataFrame to Web Mercator (EPSG:3857) for alignment with basemap data.\ngdf_icis_air_bm = gdf_icis_air.to_crs(epsg=3857)\n\n# Set up the plot with a 7x7 inch figure.\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Plot the Detroit metro area boundary and bounding box, using previously reprojected objects.\ndetroit_metro_bm.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=2)\nbbox_polygon_bm.boundary.plot(ax=ax, color='#315c86', linewidth=3)\n\n# Plot the ICIS-AIR facilities as cyan markers, with transparency (alpha=0.5) and grey outlines.\ngdf_icis_air_bm.plot(ax=ax, color='cyan', markersize=50, alpha=0.5, label='ICIS-AIR Facilities', edgecolor = \"grey\")\n\n# Add an OpenStreetMap basemap for geographical context.\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Adjust the map's extent to fit the bounding box around the Detroit metro area.\nax.set_xlim(bbox_polygon_bm.total_bounds[0], bbox_polygon_bm.total_bounds[2])\nax.set_ylim(bbox_polygon_bm.total_bounds[1], bbox_polygon_bm.total_bounds[3])\n\n# Hide the axes to make the plot cleaner.\nax.set_axis_off()\n\n# Add a legend to the plot to label the ICIS-AIR facilities.\nax.legend()\n\n# Add a title and improve the layout of the plot.\nplt.title(\"Detroit Metro Area ICIS-AIR Facilities\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Output the total number of ICIS-AIR facilities plotted.\nprint(f\"Number of ICIS-AIR facilities plotted: {len(gdf_icis_air)}\")\n\n\n\n\nNumber of ICIS-AIR facilities plotted: 907\n\n\nThese are distributed along business and industrial sectors as expected, but we can’t infer much else from this map.\nLet’s look for regulated facilities that have been in violation status. We can plot all the facilities with at least one violation during a reporting quarter and use graduated symbols that reflect the total number of quarters with violations. This information is in the AIRQtrsWithViol column.\n\n# Convert the 'AIRQtrsWithViol' column to numeric, replacing any non-numeric values with NaN.\ngdf_icis_air_bm['AIRQtrsWithViol'] = pd.to_numeric(gdf_icis_air_bm['AIRQtrsWithViol'], errors='coerce')\n\n# Subset the dataset to include only ICIS-AIR facilities with reported violations (quarters with violations > 0).\ngdf_icis_air_violators = gdf_icis_air_bm = gdf_icis_air_bm[gdf_icis_air_bm['AIRQtrsWithViol'] > 0]\n\n# Set up the plot with a 7x7 inch figure.\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Plot the Detroit metro area boundary and bounding box for geographical context.\ndetroit_metro_bm.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=2)\nbbox_polygon_bm.boundary.plot(ax=ax, color='#315c86', linewidth=3)\n\n# Plot the violators (facilities with violations) using graduated symbol sizes based on 'AIRQtrsWithViol' values.\n# The symbol size is scaled (multiplied by 10) to make differences more visible.\nscatter = ax.scatter(gdf_icis_air_violators.geometry.x,\n                     gdf_icis_air_violators.geometry.y, \n                     s=gdf_icis_air_violators['AIRQtrsWithViol']*10,  # Adjust marker size by violation count\n                     c='orangered', # Color the violators in 'orangered' for visibility\n                     edgecolor='yellow', # Add a yellow edge to make the points stand out\n                     linewidth=1, \n                     alpha=0.7) # Set transparency for better map visibility\n\n# Add an OpenStreetMap basemap to enhance the visual context of the plot.\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the map's extent to match the bounding box around the Detroit metro area.\nax.set_xlim(bbox_polygon_bm.total_bounds[0], bbox_polygon_bm.total_bounds[2])\nax.set_ylim(bbox_polygon_bm.total_bounds[1], bbox_polygon_bm.total_bounds[3])\n\n# Hide axes for a cleaner map.\nax.set_axis_off()\n\n# Add a legend to label and distinguish facilities with violations.\nax.legend()\n\n# Set the plot title and finalize the layout for professional presentation.\nplt.title(\"Detroit Metro Area ICIS-AIR Facilities w/ Violations\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe can also create a table showing which businesses have been in violation status, their name, address, and the date of their most recent violation using the tabulate module.\n\nfrom tabulate import tabulate\n\n# Filter the dataset to include only facilities with 10 or more quarters of violations.\ngdf_icis_air_violators = gdf_icis_air_bm = gdf_icis_air_bm[gdf_icis_air_bm['AIRQtrsWithViol'] >= 10]\n\n# Define the columns to be displayed in the final table.\ntable_columns = ['AIRName', 'AIRStreet', 'AIRCity', 'AIRQtrsWithViol', 'AIRLastViolDate']\n\n# Create a new DataFrame with only the selected columns for table generation.\ntable_data = gdf_icis_air_violators[table_columns].copy()\n\n# Convert the 'AIRLastViolDate' column to a datetime format for proper sorting and display.\ntable_data['AIRLastViolDate'] = pd.to_datetime(table_data['AIRLastViolDate'])\n\n# Sort the DataFrame by the 'AIRLastViolDate' column in descending order to show the most recent violations first.\ntable_data = table_data.sort_values('AIRLastViolDate', ascending=False)\n\n# Define a dictionary to map the original column names to more readable/pretty names for the table.\npretty_names = {\n    'AIRName': 'Name',\n    'AIRStreet': 'Street',\n    'AIRCity': 'City',\n    'AIRQtrsWithViol': 'Violations',\n    'AIRLastViolDate': 'Violation Date'\n}\n\n# Apply the new readable column names.\ntable_data.rename(columns=pretty_names, inplace=True)\n\n# Optionally, format the 'Violation Date' column to display dates in a 'YYYY-MM-DD' format.\ntable_data['Violation Date'] = table_data['Violation Date'].dt.strftime('%Y-%m-%d')\n\n# Generate the table using the 'tabulate' library and format it as an HTML table for presentation.\ntable = tabulate(table_data, headers='keys', tablefmt='html', showindex=False)\n\n# Display the resulting table.\ntable \n\n\n\n\nName                                     Street                  City           ViolationsViolation Date  \n\n\nEES COKE BATTERY L.L.C.                  1400 ZUG ISLAND ROAD    RIVER ROUGE            122024-04-23      \nROSATI SPECIALTIES                       24200 CAPITAL BLVD.     CLINTON TWP            122024-03-21      \nCARMEUSE LIME INC,  RIVER ROUGE OPERATION25  MARION AVE          RIVER ROUGE            122023-08-17      \nBASF CORPORATION - CHEMICAL PLANTS       1609  BIDDLE AVE        WYANDOTTE              122023-03-24      \nMARATHON PETROLEUM COMPANY LP            1001 S OAKWOOD          DETROIT                122023-03-15      \nFCA US LLC - DETROIT ASSEMBLY COMPLEX    2101  CONNER AVE        DETROIT                122023-01-25      \nU S STEEL GREAT LAKES WORKS              1 QUALITY DR            ECORSE                 122022-11-07      \nFCA US LLC WARREN TRUCK ASSEMBLY PLANT   21500  MOUND ROAD       WARREN                 122022-10-21      \nNYLOK LLC                                15260 HALLMARK COURT    MACOMB                 122022-08-29      \nAJAX METAL PROCESSING INC.               4651 BELLEVUE AVE       DETROIT                112022-06-07      \nSTERLING ENGINE HOLDINGS LLC             54420 PONTIAC TRAIL     MILFORD                122021-06-10      \nDETROIT RENEWABLE POWER, LLC             5700 RUSSELL ST         DETROIT                122019-08-29      \nJVISFH, LLC                              23944 FREEWAY PARK DRIVEFARMINGTN HLS          122019-03-22      \nLEAR CORPORATION DBA EAGLE OTTAWA        2930 WEST AUBURN RD     ROCHESTER HLS          102014-11-10      \nBASF CORPORATION - PLASTIC PLANTS        1609 BIDDLE AVE.        WYANDOTTE              122014-06-24      \nWYANDOTTE DEPT MUNI POWER PLANT          2555  VAN ALSTYNE       WYANDOTTE              122014-04-21      \nPOWER SOLUTIONS INTERNATIONAL            32505 INDUSTRIAL DRIVE  MADISON HTS            122013-12-09      \nHENRY FORD WEST BLOOMFIELD HOSPITAL      6777 WEST MAPLE ROAD    W BLOOMFIELD           122012-05-03      \nHD INDUSTRIES                            19455 GLENDALE          DETROIT                122012-04-18      \n\n\n\n\nICIS-AIR is a powerful tool for communities and policy-makers to identify regulated facilities and their compliance status, but the dataset does have limitations.\nNext, we’ll take a look at facilities required to report to the Toxic Release Inventory (TRI).\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhich of the following best describes the ICIS-AIR database?\n\nA database of air quality measurements from monitoring stations\nA comprehensive database of air quality compliance and enforcement data\nA public health database tracking respiratory illnesses\nA database of weather patterns affecting air quality\n\n\n\n\n\n\nToxic Release Inventory\nThe Toxic Release Inventory (TRI) is a crucial resource in understanding and addressing air quality issues in the United States. Established under the Emergency Planning and Community Right-to-Know Act of 1986, the TRI is a publicly accessible database maintained by the Environmental Protection Agency (EPA). It requires certain industrial facilities to report annually on their releases of toxic chemicals into the environment, including air emissions. The TRI provides valuable data on over 770 chemicals and chemical categories, offering insights into the types and quantities of pollutants released into the air by various industries. This information is vital to researchers, policymakers, and community advocates in assessing local air quality, identifying potential health risks, and developing targeted strategies to reduce toxic air emissions. By making this data publicly available, the TRI plays an important role in promoting transparency and supporting environmental justice initiatives focused on improving air quality in communities across the nation.\n\n\n\n\n\n\nData Review\n\n\n\nThe Toxics Release Inventory (TRI) and the Integrated Compliance Information System for Air (ICIS-AIR) are two important but distinct environmental reporting systems maintained by the U.S. Environmental Protection Agency (EPA). They have several key differences:\n\nRegulatory Basis\n\nTRI: Established under the Emergency Planning and Community Right-to-Know Act (EPCRA) of 1986\nICIS-AIR: Part of the Clean Air Act (CAA) compliance and enforcement program\n\nFocus\n\nTRI: Tracks the management of certain toxic chemicals that may pose a threat to human health and the environment\nICIS-AIR: Focuses specifically on air quality and emissions from facilities regulated under the Clean Air Act\n\nReported Information\n\nTRI: Facilities report on releases, waste management, and pollution prevention activities for specific toxic chemicals\nICIS-AIR: Tracks emissions data, compliance status, and enforcement actions related to air quality regulations\n\nFacility Coverage\n\nTRI: Covers facilities in specific industries that manufacture, process, or use TRI-listed chemicals above certain thresholds\nICIS-AIR: Includes a broader range of facilities that emit air pollutants, regardless of the specific chemicals involved\n\nReporting Thresholds\n\nTRI: Has specific chemical thresholds that trigger reporting requirements\nICIS-AIR: Generally doesn’t have chemical-specific thresholds; requirements are based on overall emissions and facility type\n\nPublic Accessibility\n\nTRI: Designed with a strong focus on public right-to-know, with data easily accessible to the public\nICIS-AIR: While public, it’s primarily designed for regulatory and enforcement purposes\n\nData Frequency\n\nTRI: Annual reporting is required for covered facilities\nICIS-AIR: May involve more frequent reporting, depending on permit requirements and compliance status\n\nScope of Pollutants\n\nTRI: Focuses on a specific list of toxic chemicals and chemical categories\nICIS-AIR: Covers a wider range of air pollutants, including criteria air pollutants and hazardous air pollutants\n\nUse in Environmental Management\n\nTRI: Often used for assessing long-term trends in toxic chemical releases and waste management practices\nICIS-AIR: More commonly used for day-to-day air quality management and enforcement activities\n\nGeographic Coverage\n\nTRI: Nationwide program with consistent reporting across states\nICIS-AIR: While national, implementation can vary more by state or local air quality management district\n\n\n\n\n\nEnvirofacts API\nEnviroFacts is a comprehensive online database and information system maintained by the U.S. Environmental Protection Agency (EPA). It serves as a centralized hub for accessing a wide range of environmental data collected by the EPA and other federal agencies. EnviroFacts integrates information from multiple EPA databases, covering various aspects of environmental health, including air quality, water quality, hazardous waste, and toxic releases. One of the key components of EnviroFacts is the Toxic Release Inventory (TRI) data. Through EnviroFacts, users can easily access and query TRI information, allowing them to investigate toxic chemical releases and waste management activities in their local areas. The integration of TRI data within the broader EnviroFacts system enables researchers, policymakers, and community members to contextualize toxic release information alongside other environmental indicators.\nEnvirofacts is available as a web based search and data platform and also as a programmatic API. The web platform is a great way to familiarize yourself with the available datasets and create simple downloads, however, for analytical purposes we recommend learning to navigate their API so you can create repeatable and reliable analysis.\n\n\n\n\n\n\nEnvironmental Justice In the News\n\n\n\nThe Michigan Department of Environment, Great Lakes and Energy (EGLE) has settled a civil rights complaint regarding a hazardous waste facility in Detroit. The complaint, filed in 2020 by environmental groups and local residents, challenged the renewal and expansion of U.S. Ecology North’s license, arguing it was unjust to increase hazardous waste storage in a predominantly low-income and minority neighborhood. As part of the settlement, EGLE will now consider environmental justice concerns in future licensing decisions for hazardous waste facilities.\nThe agreement, described as “groundbreaking” by the Sierra Club, introduces several new measures. EGLE will conduct environmental justice and cumulative impact analyses for future licensing decisions, potentially denying licenses that would have an unlawful impact on human health and the environment. The settlement also includes provisions for improved community engagement, such as better translation services, public input processes, and the installation of air monitors around U.S. Ecology North. Additionally, the state will work with local residents to conduct a community health assessment. While some details remain to be clarified, environmental advocates view this as a significant step towards advancing environmental justice in Michigan.\n\n\n\n\nQuerying the API\nWe’ll continue to use the Detroit metro boundary we created earlier. We assigned them the names detroit_metro and detroit_metro_bm (projected to mercator to match basemaps for plotting).\n\n# Print the coordinates of the bounding box, which defines the rectangular boundary for the area of interest\nprint(\"Bounding Box:\")\n\n# Display the minimum longitude (X-axis) of the bounding box\nprint(f\"Minimum X (Longitude): {bbox[0]}\")\n\n# Display the minimum latitude (Y-axis) of the bounding box\nprint(f\"Minimum Y (Latitude): {bbox[1]}\")\n\n# Display the maximum longitude (X-axis) of the bounding box\nprint(f\"Maximum X (Longitude): {bbox[2]}\")\n\n# Display the maximum latitude (Y-axis) of the bounding box\nprint(f\"Maximum Y (Latitude): {bbox[3]}\")\n\nBounding Box:\nMinimum X (Longitude): -83.689438\nMinimum Y (Latitude): 42.02793\nMaximum X (Longitude): -82.705966\nMaximum Y (Latitude): 42.897541\n\n\nNow we’ll create an empty list to store the TRI data, loop through each county making an API query, and place the retrieved data in the empty list.\n\n# Initialize a list to hold TRI (Toxic Release Inventory) facility data for each county in Michigan\ntri_data = []\n\n# Loop through each county in the specified list to fetch TRI data separately\n# (avoiding issues encountered when querying all counties at once)\nfor county in counties:\n    # Construct the API URL to query TRI facilities for the current county\n    api_url = f\"https://data.epa.gov/efservice/tri_facility/state_abbr/MI/county_name/{county}/JSON\"\n    response = requests.get(api_url)\n\n    # Check if the response was successful\n    if response.status_code == 200:\n        county_data = response.json()\n        # Append the current county's TRI data to the overall list\n        tri_data.extend(county_data)\n    else:\n        # Print an error message if data retrieval failed for the county\n        print(f\"Failed to fetch data for {county} County. Status code: {response.status_code}\")\n\n\n\nProcessing the Data\nThe TRI data comes in json format. This format can be confusing to interpret, because each record (traditionally a row in a table) is viewed with columns structured vertically. Let’s look at the first record.\n\n# Access and print the second entry in the TRI data list to inspect a sample facility's data structure\ntri_data[1]\n\n{'tri_facility_id': '48007GRWGR3155W',\n 'facility_name': 'PPG GROW DETROIT',\n 'street_address': '14000 STANSBURY',\n 'city_name': 'DETROIT',\n 'county_name': 'WAYNE',\n 'state_county_fips_code': '26163',\n 'state_abbr': 'MI',\n 'zip_code': '48227',\n 'region': '5',\n 'fac_closed_ind': '0',\n 'mail_name': 'PPG INDS.',\n 'mail_street_address': '1330 PIEDMONT ATTN:  RICH MCCURDY',\n 'mail_city': 'TROY',\n 'mail_state_abbr': 'MI',\n 'mail_province': None,\n 'mail_country': None,\n 'mail_zip_code': '48083',\n 'asgn_federal_ind': 'C',\n 'asgn_agency': None,\n 'frs_id': None,\n 'parent_co_db_num': '001344803',\n 'parent_co_name': 'PPG INDS INC',\n 'fac_latitude': 422146,\n 'fac_longitude': 831255,\n 'pref_latitude': 42.3903,\n 'pref_longitude': 83.182,\n 'pref_accuracy': 150,\n 'pref_collect_meth': 'A1',\n 'pref_desc_category': 'PG',\n 'pref_horizontal_datum': '1',\n 'pref_source_scale': 'J',\n 'pref_qa_code': '1000',\n 'asgn_partial_ind': '0',\n 'asgn_public_contact': None,\n 'asgn_public_phone': None,\n 'asgn_public_contact_email': None,\n 'bia_code': None,\n 'standardized_parent_company': None,\n 'asgn_public_phone_ext': None,\n 'epa_registry_id': '110041981807',\n 'asgn_technical_contact': None,\n 'asgn_technical_phone': None,\n 'asgn_technical_phone_ext': None,\n 'mail': None,\n 'asgn_technical_contact_email': None,\n 'foreign_parent_co_name': None,\n 'foreign_parent_co_db_num': None,\n 'standardized_foreign_parent_company': None}\n\n\nThis record shows all the information for PPG GROW DETROIT with columns and values listed side-by-side (‘city_name’: ‘DETROIT’). This may seem difficult to deal with, but most programming languages have tools to easily parse json data into traditional tables. pd.DataFrame does it automatically when you attempt to create a pandas data frame.\n\n# Convert the collected TRI data list into a Pandas DataFrame for easier data manipulation and analysis\ntri_df = pd.DataFrame(tri_data)\n\n# Output the total count of facilities retrieved to confirm successful data fetching\nprint(f\"Number of facilities fetched: {len(tri_df)}\")\n\n# Display the first few rows of the DataFrame for a quick data preview (uncomment if needed for inspection)\n# tri_df.head()\n\nNumber of facilities fetched: 791\n\n\nChecking the first few rows–everything looks as it should. We want to geocode the facility locations into a point layer using the latitude and longitude values. Therefore, we should start by making sure all the facilities have coordinate values.\nWe’ll check and remove those without.\n\n# Create a copy of the TRI DataFrame to prevent modifications that could trigger a SettingWithCopyWarning\ntri_df_clean = tri_df.copy()\n\n# Remove rows where latitude or longitude is missing to ensure data completeness\ntri_df_clean = tri_df_clean.dropna(subset=['pref_latitude', 'pref_longitude'])\n\nprint(f\"Number of facilities after removing empty coordinates: {len(tri_df_clean)}\")\n\n# Convert latitude and longitude columns to numeric, handling non-numeric values by setting them to NaN\ntri_df_clean['pref_latitude'] = pd.to_numeric(tri_df_clean['pref_latitude'], errors='coerce')\ntri_df_clean['pref_longitude'] = pd.to_numeric(tri_df_clean['pref_longitude'], errors='coerce')\n\nNumber of facilities after removing empty coordinates: 478\n\n\nOuch! We lost roughly 300 records due to missing coordinate information (789 vs. 478). If this were an important analysis for policy-making or scientific research we would take the time to geocode the listed addresses into latitude/longitude coordinates, but for the purposes of this exercise we’ll move on.\nIn my personal investigations of this data, I noticed that some longitude coordinates were not negative, but had an absolute value (e.g. 83.25) that made sense for the Detroit metro area, therefore we will flip the sign on these records.\n\n# Define a function to correct longitudes that are mistakenly positive, converting them to negative values for North America\ndef correct_longitude(lon):\n    if lon > 0:\n        return -lon\n    return lon\n\n# Apply longitude correction to all facilities in the DataFrame\ntri_df_clean['pref_longitude'] = tri_df_clean['pref_longitude'].apply(correct_longitude)\n\nAdditionally, there are some records with wild longitudinal values that are not simply from a missing negative sign. We’ll identify these outliers using the interquantile range. Anything outside the range will be tossed. One of the downsides of the TRI database is that many aspects are provided directly by the facility (and therefore subject to errors). Again, if this were a more important analysis we would carefully explore all of these missing records and possibly geocode using the address.\nToss the quartile outliers.\n\n# Calculate the Interquartile Range (IQR) for longitude to identify and remove outlier values\nQ1 = tri_df_clean['pref_longitude'].quantile(0.25)\nQ3 = tri_df_clean['pref_longitude'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define acceptable range for longitude, filtering out points beyond 1.5 * IQR\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nlower_bound, upper_bound\n\n# Remove facilities with longitudes outside of the defined bounds to eliminate extreme outliers\ntri_df_clean = tri_df_clean[(tri_df_clean['pref_longitude'] >= lower_bound) & \n                            (tri_df_clean['pref_longitude'] <= upper_bound)]\n\nprint(f\"Number of facilities after removing longitude outliers: {len(tri_df_clean)}\")\n\nNumber of facilities after removing longitude outliers: 473\n\n\nThankfully we only lost 2 more records from outliers.\n\n\nVisualizing the Data\nThe data has been cleaned up a bit so now lets create a spatial point object from the data with geopandas and use matplotlib to plot the values on top of a basemap of Detroit we’ll acquire using contextily and our original Wayne, Macomb, and Oakland counties boundary.\n\n# Create a GeoDataFrame from the cleaned TRI data, converting latitude and longitude into geometric points\ndetroit_tri = gpd.GeoDataFrame(\n    tri_df_clean, \n    geometry=gpd.points_from_xy(tri_df_clean.pref_longitude, tri_df_clean.pref_latitude),\n    crs=\"EPSG:4326\" # Set the coordinate reference system to WGS 84 (latitude/longitude)\n)\n\n# Reproject TRI data to the Web Mercator projection (EPSG:3857) for compatibility with the basemap\ndetroit_tri = detroit_tri.to_crs(epsg=3857)\n\n# Initialize a plot with a fixed size\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Plot the Detroit metro area boundaries and the bounding box as background layers\ndetroit_metro_bm.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=2)\nbbox_polygon_bm.boundary.plot(ax=ax, color='#315c86', linewidth=3)\n\n# Plot TRI facilities as purple points with a grey edge, representing facilities' locations in the metro area\ndetroit_tri.plot(ax=ax, color='purple', edgecolor='grey', markersize=50, alpha=0.5, label='TRI Facilities')\n\n# Plot ICIS-AIR facilities as cyan points with a grey edge, indicating the locations of air-quality-monitored facilities\ngdf_icis_air_bm.plot(ax=ax, color='cyan', edgecolor='grey', markersize=50, alpha=0.5, label='ICIS-AIR Facilities')\n\n# Overlay a basemap for geographical context, using OpenStreetMap tiles\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the map's viewing area to the bounding box’s coordinates, zooming in on the Detroit metro area\nax.set_xlim(bbox_polygon_bm.total_bounds[0], bbox_polygon_bm.total_bounds[2])\nax.set_ylim(bbox_polygon_bm.total_bounds[1], bbox_polygon_bm.total_bounds[3])\n\n# Hide axis labels and ticks for a cleaner map presentation\nax.set_axis_off()\n\n# Add a legend to differentiate TRI and ICIS-AIR facilities on the map\nax.legend()\n\n# Add a title for context and adjust layout for better spacing\nplt.title(\"Detroit Metro Area TRI and ICIS-AIR Facilities\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Print the number of facilities included in the plot for confirmation\nprint(f\"Number of TRI facilities plotted: {len(detroit_tri)}\")\nprint(f\"Number of ICIS-AIR facilities plotted: {len(gdf_icis_air)}\")\n\n\n\n\nNumber of TRI facilities plotted: 473\nNumber of ICIS-AIR facilities plotted: 907\n\n\nWe can see that there are far fewer facilities being tracked in TRI compared to ICIS-AIR. Some of this can be attributed to the records we tossed after checking for valid geographic coordinates and outliers, but generally speaking ICIS-AIR will contain far more as it tracks every facility regulated by the Clean Air Act as opposed to only those that are part of TRI.\nYou may have also noticed that this dataset does not contain any actual pollution data; only facility information for those that are regulated by TRI.\nTRI is made up of numerous tables containing a wealth of information regarding the facilities themselves and all the regulated chemicals that are handled at each site. The TRI_FACILITY table we requested is the starter table used in most of the examples in the Envirofacts API documentation, however, it does not contain any chemical release data.\nNavigating the TRI Envirofacts interface can be overwhelming at times. Multiple tables can be queried with a single URL, however, for a proper join to be carried out both tables must contain at least one column in common. In most cases this can be accomplished using TRI_FACILITY_ID. That said, air pollution release data (column name AIR_TOTAL_RELEASE) is only found in the TRI_FORM_R table, which does not contain the FACILITY_ID column.\nYou can querry the V_TRI_FORM_R_EZ using the API even though it’s not a documented table listed in the API website. This form contains facility information and AIR_TOTAL_RELEASE reporting, but it only goes until 2021. To get the most recent data (2023) you would have to manually download the individual tables with separate calls perform the joins, or use the custom form search web portal. We’ll demonstrate a quick API call for the V_TRI_FORM_R_EZ, but move forward with the most recent data available in the custom form search web portal.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhat is the primary purpose of the Toxic Release Inventory (TRI)?\n\nTo track emissions of greenhouse gases\nTo monitor compliance with the Clean Air Act\nTo provide data on toxic chemical releases from industrial facilities\nTo record air quality index values for major cities\n\n\n\n\n\n\nToxic Release Inventory: Form R\nWe can make the query to Envirofacts. Let’s break down the call.\n\n# Define the URL for the CSV file containing Toxic Release Inventory Form R data\n# This query requests data from the EPA Envirofacts API, specifying:\n# - Reporting Year: 2021\n# - Air releases greater than 0\n# - State: Michigan (MI)\n# - Counties: Wayne, Oakland, and Macomb\nurl = \"https://data.epa.gov/efservice/V_TRI_FORM_R_EZ/REPORTING_YEAR/2021/AIR_TOTAL_RELEASE/>/0/STATE_ABBR/MI/COUNTY_NAME/CONTAINING/WAYNE/CONTAINING/OAKLAND/CONTAINING/MACOMB/CSV\"\n\n\nBase url: https://data.epa.gov/efservice/\nThe table we’re requesting: V_TRI_FORM_R_EZ/\nFor just 2021: REPORTING_YEAR/2021/\nOnly facilities that report some air release: AIR_TOTAL_RELEASE/>/0/\nJust Michigan: STATE_ABBR/MI/\nOur counties: COUNTY_NAME/CONTAINING/WAYNE/CONTAINING/OAKLAND/CONTAINING/MACOMB/\nWe want it as a csv: CSV\n\n\n# Read the TRI Form R data CSV file directly into a pandas DataFrame\ntri_form_r = pd.read_csv(url)\n\n# Confirm successful data loading by printing the number of records\nprint(f\"Successfully read CSV. Number of records: {len(tri_form_r)}\")\n\n# Display column names to understand the structure of the dataset\nprint(\"\\nColumns in the dataset:\")\nprint(tri_form_r.columns)\n\nSuccessfully read CSV. Number of records: 413\n\nColumns in the dataset:\nIndex(['tri_facility_id', 'facility_name', 'street_address', 'city_name',\n       'county_name', 'state_county_fips_code', 'state_abbr', 'zip_code',\n       'region', 'fac_closed_ind',\n       ...\n       'additional_text_9_1', 'srs_id', 'media_type', 'prod_ratio_or_activity',\n       'industry_code', 'industry_description', 'source', 'method', 'adjusted',\n       'covered_naics'],\n      dtype='object', length=568)\n\n\nWe have roughly the same number of facilities as before, but this table has a lot of columns (568). We should make it more manageable.\n\n# The dataset contains numerous columns (568), so we will retain only essential columns for analysis\n# Select columns: facility name, address, city, reporting year, and air release amounts\nkeeps = [\n    'facility_name', 'street_address', 'city_name',\n    'reporting_year', 'air_total_release']\n\n# Update DataFrame to only include the selected columns\ntri_form_r = tri_form_r[keeps]\n\nThis works well but we can get more recent data that is already filtered using the web portal. The user selects location, years of interest, and columns of interests. The portal serves up your requested dataset in an Amazon S3 cloud storage. We can import the dataset directly from the address they provide.\n\n# Read a secondary TRI Form R CSV file directly from the URL\n# This may contain additional or alternative data for further analysis\ntri_form_r = pd.read_csv(\"https://dmap-epa-enviro-prod-export.s3.amazonaws.com/396975438.CSV\")\n\n# Confirm successful reading by printing the number of records and dataset structure\nprint(f\"Successfully read CSV. Number of records: {len(tri_form_r)}\")\nprint(\"\\nColumns in the dataset:\")\nprint(tri_form_r.columns)\n\nSuccessfully read CSV. Number of records: 994\n\nColumns in the dataset:\nIndex(['AIR_TOTAL_RELEASE', 'CHEM_NAME', 'FACILITY_NAME', 'LATITUDE',\n       'LONGITUDE', 'STREET_ADDRESS', 'TRI_FACILITY_ID'],\n      dtype='object')\n\n\nThis is a bit more manageable, but we have multiple records per facility because each chemical release has a separate record. This allows the user to investigate specific chemicals, but for the purposes of this exercise we will aggregate AIR_TOTAL_RELEASE for each facility, to get a single row per facility.\n\n# Display the first few rows of the dataset for a quick inspection\n# Note: this printout is limited to avoid excessive output in a web environment\n# tri_form_r.head\n\nLet’s aggregate it.\n\n# Group the dataset by facility name and location details, summing the total air releases per facility\n# This aggregation step consolidates all entries for each unique facility based on name, latitude, longitude, and address\ntri_form_r = tri_form_r.groupby(['FACILITY_NAME', 'LATITUDE','LONGITUDE','STREET_ADDRESS'], as_index=False).agg({\n    'AIR_TOTAL_RELEASE': 'sum' # Sum the air release values for each facility\n})\n# Display the total number of unique facilities after grouping\nprint(f\"Successfully read CSV. Number of records: {len(tri_form_r)}\")\n\nSuccessfully read CSV. Number of records: 211\n\n\nThere were only 211 unique listings in the 2023 data.\nAs before, we should check for valid coordinates and missing air release data.\n\n# Define column names for latitude, longitude, and air release values\n# Assumes column names match; adjust 'LATITUDE', 'LONGITUDE', and 'AIR_TOTAL_RELEASE' if necessary\nlat_col = 'LATITUDE'\nlon_col = 'LONGITUDE'\nrelease_col = 'AIR_TOTAL_RELEASE'  # Column for the total air release per facility\n\n# Remove any records with missing coordinates or air release data to ensure data integrity for spatial analysis\ndf_tri_clean = tri_form_r.dropna(subset=[lat_col, lon_col, release_col])\n\n# Output the number of records remaining after removing rows with missing data\nprint(f\"\\nNumber of records after removing missing data: {len(df_tri_clean)}\")\n\n\nNumber of records after removing missing data: 211\n\n\nThere were no missing coordinates or missing air release data.\nNow we can create a spatial object with GeoPandas using the latitude and longitude coordinates in the table.\n\n# Convert the cleaned TRI data to a GeoDataFrame for spatial analysis\n# Creates a 'geometry' column based on longitude and latitude, setting the coordinate system to EPSG:4326 (WGS 84)\ngdf_tri_form_r = gpd.GeoDataFrame(\n    df_tri_clean, \n    geometry=gpd.points_from_xy(df_tri_clean[lon_col], df_tri_clean[lat_col]),\n    crs=\"EPSG:4326\"\n)\n\nLet’s check the distribution of the air release data for outliers.\n\n# Plot a histogram to visualize the distribution of total air release values across facilities\nplt.figure(figsize=(8, 8))  # Define plot size\nplt.hist(gdf_tri_form_r['AIR_TOTAL_RELEASE'], bins=10, edgecolor='black') # Set bin count and edge color\nplt.title('Distribution of Air Total Release Values') # Plot title\nplt.xlabel('Air Total Release Sum') # X-axis label for air release values\nplt.ylabel('Frequency') # Y-axis label for frequency of values\nplt.grid(True, alpha=0.3) # Add grid with slight transparency for readability\n\n# Display the histogram\nplt.show()\n\n\n\n\nLooks like there are outliers around 100,000 lbs and 400,000 lbs. Before we map the data, it would be a good idea to perform a log transformation of AIR_TOTAL_RELEASE to smooth out the visuals. You can’t take the log of negative or zero values, so we’ll use the log1p function, which adds 1 to every value before taking the log.\n\nimport numpy as np\n\n# Calculate the natural logarithm of air release values +1 to handle any zero values safely\n# This transformation helps normalize the data, reducing skew for clearer visualization\ngdf_tri_form_r['LOG_AIR_RELEASE'] = np.log1p(gdf_tri_form_r['AIR_TOTAL_RELEASE'])\n\n# Plot a histogram to visualize the distribution of the log-transformed air release values\nplt.figure(figsize=(8, 8))\nplt.hist(gdf_tri_form_r['LOG_AIR_RELEASE'], bins=10, edgecolor='black') # Set bin count and edge color\nplt.title('Distribution of Log-Transformed Air Release Values') # Plot title\nplt.xlabel('Log of Air Total Release Sum') # X-axis label for log air release values\nplt.ylabel('Frequency') # Y-axis label for frequency of values\nplt.grid(True, alpha=0.3) # Add grid with slight transparency for readability\n\n# Display the histogram\nplt.show()\n\n\n\n\nThat looks much better.\nNow we can visualize the data, by overlaying the points on a basemap of Detroit while using graduated symbols to illustrate the amount of air releases for a given facility.\n\n# Reproject TRI facility data to Web Mercator (EPSG:3857) to align with the basemap projection\ngdf_tri_form_r_bm = gdf_tri_form_r.to_crs(epsg=3857)\n\n# Create the map plot\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Plot the Detroit metro area boundary and bounding box with customized colors\ndetroit_metro_bm.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=2)\nbbox_polygon_bm.boundary.plot(ax=ax, color='#315c86', linewidth=2)\n\n# Scatter plot TRI facilities with symbol sizes proportional to the log of air releases\n# Scaling factor applied for better visibility; adjust as needed\nscatter = ax.scatter(gdf_tri_form_r_bm.geometry.x, gdf_tri_form_r_bm.geometry.y, \n                     s=gdf_tri_form_r_bm['LOG_AIR_RELEASE']*20,  # Scale size of symbols by log air releases\n                     c='orangered',  # Color fill of symbols\n                     edgecolor='yellow',  # Outline color of symbols\n                     linewidth=1,  # Outline thickness for clarity\n                     alpha=0.7) # Adjust transparency level\n\n# Add OpenStreetMap basemap for spatial context\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set map view to focus on the bounding box area\nax.set_xlim(bbox_polygon_bm.total_bounds[0], bbox_polygon_bm.total_bounds[2])\nax.set_ylim(bbox_polygon_bm.total_bounds[1], bbox_polygon_bm.total_bounds[3])\n\n# Remove axes for a cleaner map view\nax.set_axis_off()\n\n# Add a legend with sample sizes for symbol scaling reference\nlegend_sizes = [0,4,8,12]  # Adjust sample sizes based on data scale\nlegend_elements = [plt.scatter([], [], s=size*20, c='orangered', edgecolor='yellow', \n                               linewidth=1, alpha=1, label=f'{size:,}') \n                   for size in legend_sizes]\nax.legend(handles=legend_elements, title='Log Total Air Releases (lbs)', \n          loc='lower right', title_fontsize=12, fontsize=10) # Customize legend title and text\n\n# Set the plot title\nplt.title(\"Detroit Metro Area TRI Facilities - Total Air Releases (Custom Data)\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Output summary statistics for TRI facilities plotted and air release values\nprint(f\"\\nNumber of TRI facilities plotted: {len(gdf_tri_form_r)}\")\nprint(f\"Total air releases: {gdf_tri_form_r[release_col].sum():,.2f} lbs\")\nprint(f\"Average air release per facility: {gdf_tri_form_r[release_col].mean():,.2f} lbs\")\n\n\n\n\n\nNumber of TRI facilities plotted: 211\nTotal air releases: 2,117,167.80 lbs\nAverage air release per facility: 10,033.97 lbs\n\n\nThe point source air release data provides a good snapshot of the locations and relative amounts of chemical releases. With some basic understanding of the demographic dynamics of Detroit one might theorize who is being subjected to high levels of pollution, however, we’ll need additional data to develop any type of systematic analysis.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhat key information does Form R of the Toxic Release Inventory provide?\n\nFacility names and locations only\nChemical storage capacities of facilities\nDetailed data on chemical releases, including air releases\nEmployee health records related to chemical exposure\n\n\n\n\n\nSocial Vulnerability Index\nNASA’s Social Vulnerability Index (SVI) raster dataset, available through the Socioeconomic Data and Applications Center (SEDAC), is a high-resolution geospatial resource that quantifies social vulnerability across the United States. This dataset is based on the CDC’s Social Vulnerability Index but is presented in a raster format, providing continuous coverage at a 250-meter resolution. The SVI incorporates various socioeconomic and demographic factors such as poverty, lack of vehicle access, crowded housing, and minority status to assess communities’ capacity to prepare for, respond to, and recover from hazards, including environmental threats like poor air quality.\nThe raster format allows for more detailed spatial analysis and integration with other environmental datasets. This makes it particularly valuable for researchers and policymakers studying the intersection of social vulnerability and environmental risks, such as air pollution exposure. By overlaying this SVI data with air quality information, for instance, analysts can identify areas where socially vulnerable populations may be disproportionately affected by poor air quality, supporting environmental justice initiatives and targeted intervention strategies.\nSEDAC, as part of NASA’s Earth Observing System Data and Information System (EOSDIS), hosts this dataset along with other socioeconomic and environmental data, facilitating interdisciplinary research on human-environment interactions. The SVI raster dataset’s high resolution and comprehensive coverage make it a powerful tool for assessing environmental equity and informing policy decisions at various geographic scales.\nThe SVI dataset is free to users with a NASA Earth Data account. The Earth Data account is free and gives you access to all SEDAC data as well as a wide range of NASA Earth science data. It’s a valuable resource for researchers, students, and anyone interested in environmental and socioeconomic data.\nIf you don’t already have an Earth Data account you can follow these steps to download the SVI dataset on your local computer:\n\nVisit the NASA Earthdata website. Click on the “Register” button on the top right corner of the page.\nFill out the registration form with the required information, including your name, email address, and a password. Create a username.\nVerify your email. NASA will send a verification email to the address you provided. Click on the link in this email to confirm your account.\nOnce your account is verified, log in to Earth Data using your username and password.\nVisit the SEDAC website and search for the U.S. Social Vulnerability Index Grids, v1.01 dataset by entering “SVI” in the search bar.\nDownload the 2020 WGS84 SVI grids; enter your Earth Data credentials to log in when prompted.\n\n\nData Processing\nOnce you’ve downloaded the dataset to your working directory, you can proceed with the analysis.\nThe different layers of SVI are provided as individual files, but sometimes it’s easier to work with a multilayer object. We can create one using xarray. To begin, we’ll read in each file individually, clip it to our border of Detroit metro, and create an individual data array.\n\nimport xarray as xr\nimport rasterio\nimport rasterio.mask\n\n# Set xarray display options for enhanced attribute visibility when viewing data\nxr.set_options(\n    keep_attrs=True,  # Retain attributes in computations\n    display_expand_attrs=True, # Expand attribute display\n    display_expand_coords=True, # Show all coordinates\n    display_expand_data=False, # Collapse data display to save space\n    display_expand_data_vars=True # Expand variable display\n)\n\n# Define paths to SVI (Social Vulnerability Index) TIF files, each representing a different aspect of vulnerability\ntif_files = [\n    \"data/svi/svi_2020_tract_overall_wgs84.tif\",\n    \"data/svi/svi_2020_tract_minority_wgs84.tif\",\n    \"data/svi/svi_2020_tract_socioeconomic_wgs84.tif\",\n    \"data/svi/svi_2020_tract_housing_wgs84.tif\",\n    \"data/svi/svi_2020_tract_household_wgs84.tif\"\n]\n\n# Initialize an empty list to store DataArrays created from each raster\ndata_arrays = []\n\n# Iterate over each TIF file to read, clip to Detroit metro area, and convert to xarray DataArray\nfor file in tif_files:\n    with rasterio.open(file) as src:\n        # Align Detroit metro boundary to raster CRS for compatibility\n        metro_reprojected = detroit_metro.to_crs(src.crs)\n        \n        # Clip the raster to the Detroit metro area based on its geometry\n        out_image, out_transform = rasterio.mask.mask(src, metro_reprojected.geometry, crop=True)\n        out_meta = src.meta.copy() # Copy metadata for adjustments\n        \n        # Update metadata with new dimensions and transformation details after cropping\n        out_meta.update({\"driver\": \"GTiff\",\n                         \"height\": out_image.shape[1],\n                         \"width\": out_image.shape[2],\n                         \"transform\": out_transform})\n        \n        # Set up mesh grid coordinates to match the raster's new dimensions\n        height, width = out_meta['height'], out_meta['width']\n        cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n        xs, ys = rasterio.transform.xy(out_transform, rows, cols)\n        \n        # Convert lists to numpy arrays and reshape to match the raster dimensions\n        xs, ys = np.array(xs).reshape(height, width), np.array(ys).reshape(height, width)\n        \n        # Create an xarray DataArray with coordinates for each cell based on the raster grid\n        da = xr.DataArray(out_image[0],  # Extract the first band of the clipped image\n                          coords={'y': ('y', ys[:, 0]),  # Latitude\n                                  'x': ('x', xs[0, :])}, # Longitude\n                          dims=['y', 'x'])\n        da.attrs['crs'] = str(src.crs)  # Store CRS as an attribute in string format\n        da.attrs['transform'] = out_transform # Add transform details to attributes\n        data_arrays.append(da) # Append DataArray to list\n\nNow we can combine them together and give the layers “pretty” names.\n\n# Combine all DataArrays from different SVI files into a single xarray DataSet\nsvi_detroit = xr.concat(data_arrays, dim='layer')\n\n# Define and assign layer names to clarify SVI aspect for each data layer in the combined dataset\nlayer_names = ['Overall', 'Minority', 'Socioeconomic', 'Housing', 'Household']\nsvi_detroit = svi_detroit.assign_coords(layer=('layer', layer_names))\nsvi_detroit # Display the DataSet for verification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (layer: 5, y: 105, x: 119)>\n-3.4e+38 -3.4e+38 -3.4e+38 -3.4e+38 ... -3.4e+38 -3.4e+38 -3.4e+38 -3.4e+38\nCoordinates:\n  * y        (y) float64 42.9 42.89 42.88 42.87 ... 42.05 42.05 42.04 42.03\n  * x        (x) float64 -83.69 -83.68 -83.67 -83.66 ... -82.72 -82.71 -82.7\n  * layer    (layer) <U13 'Overall' 'Minority' ... 'Housing' 'Household'\nAttributes:\n    crs:        EPSG:4326\n    transform:  | 0.01, 0.00,-83.69|\\n| 0.00,-0.01, 42.90|\\n| 0.00, 0.00, 1.00|xarray.DataArraylayer: 5y: 105x: 119-3.4e+38 -3.4e+38 -3.4e+38 -3.4e+38 ... -3.4e+38 -3.4e+38 -3.4e+38array([[[-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        ...,\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38]],\n\n       [[-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n...\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38]],\n\n       [[-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        ...,\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38],\n        [-3.4e+38, -3.4e+38, -3.4e+38, ..., -3.4e+38, -3.4e+38,\n         -3.4e+38]]], dtype=float32)Coordinates: (3)y(y)float6442.9 42.89 42.88 ... 42.04 42.03array([42.895833, 42.887499, 42.879166, 42.870833, 42.862499, 42.854166,\n       42.845833, 42.837499, 42.829166, 42.820833, 42.812499, 42.804166,\n       42.795833, 42.787499, 42.779166, 42.770833, 42.762499, 42.754166,\n       42.745833, 42.737499, 42.729166, 42.720833, 42.712499, 42.704166,\n       42.695833, 42.687499, 42.679166, 42.670833, 42.662499, 42.654166,\n       42.645833, 42.637499, 42.629166, 42.620833, 42.612499, 42.604166,\n       42.595833, 42.587499, 42.579166, 42.570833, 42.562499, 42.554166,\n       42.545833, 42.537499, 42.529166, 42.520833, 42.512499, 42.504166,\n       42.495833, 42.487499, 42.479166, 42.470833, 42.462499, 42.454166,\n       42.445833, 42.437499, 42.429166, 42.420833, 42.412499, 42.404166,\n       42.395833, 42.387499, 42.379166, 42.370833, 42.362499, 42.354166,\n       42.345833, 42.337499, 42.329166, 42.320833, 42.312499, 42.304166,\n       42.295833, 42.287499, 42.279166, 42.270833, 42.262499, 42.254166,\n       42.245833, 42.237499, 42.229166, 42.220833, 42.212499, 42.204166,\n       42.195833, 42.187499, 42.179166, 42.170833, 42.162499, 42.154166,\n       42.145833, 42.137499, 42.129166, 42.120833, 42.112499, 42.104166,\n       42.095833, 42.087499, 42.079166, 42.070833, 42.062499, 42.054166,\n       42.045833, 42.037499, 42.029166])x(x)float64-83.69 -83.68 ... -82.71 -82.7array([-83.6875  , -83.679167, -83.670834, -83.6625  , -83.654167, -83.645834,\n       -83.6375  , -83.629167, -83.620834, -83.6125  , -83.604167, -83.595834,\n       -83.5875  , -83.579167, -83.570834, -83.5625  , -83.554167, -83.545834,\n       -83.5375  , -83.529167, -83.520834, -83.5125  , -83.504167, -83.495834,\n       -83.4875  , -83.479167, -83.470834, -83.4625  , -83.454167, -83.445834,\n       -83.4375  , -83.429167, -83.420834, -83.4125  , -83.404167, -83.395834,\n       -83.3875  , -83.379167, -83.370834, -83.3625  , -83.354167, -83.345834,\n       -83.3375  , -83.329167, -83.320834, -83.3125  , -83.304167, -83.295834,\n       -83.2875  , -83.279167, -83.270834, -83.2625  , -83.254167, -83.245834,\n       -83.2375  , -83.229167, -83.220834, -83.2125  , -83.204167, -83.195834,\n       -83.1875  , -83.179167, -83.170834, -83.1625  , -83.154167, -83.145834,\n       -83.1375  , -83.129167, -83.120834, -83.1125  , -83.104167, -83.095834,\n       -83.0875  , -83.079167, -83.070834, -83.0625  , -83.054167, -83.045834,\n       -83.0375  , -83.029167, -83.020834, -83.0125  , -83.004167, -82.995834,\n       -82.9875  , -82.979167, -82.970834, -82.9625  , -82.954167, -82.945834,\n       -82.9375  , -82.929167, -82.920834, -82.9125  , -82.904167, -82.895834,\n       -82.8875  , -82.879167, -82.870834, -82.8625  , -82.854167, -82.845834,\n       -82.8375  , -82.829167, -82.820834, -82.8125  , -82.804167, -82.795834,\n       -82.7875  , -82.779167, -82.770834, -82.7625  , -82.754167, -82.745834,\n       -82.7375  , -82.729167, -82.720834, -82.7125  , -82.704167])layer(layer)<U13'Overall' ... 'Household'array(['Overall', 'Minority', 'Socioeconomic', 'Housing', 'Household'],\n      dtype='<U13')Indexes: (3)yPandasIndexPandasIndex(Index([ 42.89583280200428,   42.8874994687043,  42.87916613540431,\n       42.870832802104324, 42.862499468804344,  42.85416613550436,\n        42.84583280220438,  42.83749946890439,  42.82916613560441,\n        42.82083280230442,\n       ...\n        42.10416613850579,   42.0958328052058,  42.08749947190582,\n       42.079166138605835, 42.070832805305855,  42.06249947200587,\n        42.05416613870588,   42.0458328054059, 42.037499472105914,\n       42.029166138805934],\n      dtype='float64', name='y', length=105))xPandasIndexPandasIndex(Index([-83.68750038476503, -83.67916705146503, -83.67083371816501,\n       -83.66250038486501, -83.65416705156501,   -83.645833718265,\n         -83.637500384965, -83.62916705166498, -83.62083371836498,\n       -83.61250038506496,\n       ...\n       -82.77916705506422,  -82.7708337217642,  -82.7625003884642,\n        -82.7541670551642, -82.74583372186419, -82.73750038856419,\n       -82.72916705526417, -82.72083372196417, -82.71250038866415,\n       -82.70416705536415],\n      dtype='float64', name='x', length=119))layerPandasIndexPandasIndex(Index(['Overall', 'Minority', 'Socioeconomic', 'Housing', 'Household'], dtype='object', name='layer'))Attributes: (2)crs :EPSG:4326transform :| 0.01, 0.00,-83.69|\n| 0.00,-0.01, 42.90|\n| 0.00, 0.00, 1.00|\n\n\nNow we can plot each layer.\n\n# Define minimum and maximum values for the color scale; SVI ranges from 0 (low) to 1 (high vulnerability)\nvmin, vmax = 0, 1\n\n# Create a multi-panel plot with subplots arranged in a 3x2 grid\nfig, axes = plt.subplots(3, 2, figsize=(8, 10))\naxes = axes.flatten()  # Flatten the 2D array of axes for easier iteration\n\n# Plot each SVI layer in a separate subplot\nfor i, layer in enumerate(svi_detroit.layer.values):\n    # Render the layer with a plasma colormap and fixed color range for consistency\n    im = svi_detroit.sel(layer=layer).plot(ax=axes[i], add_colorbar=False, vmin=vmin, vmax=vmax, cmap='plasma')\n    axes[i].set_title(layer) # Title each subplot with the SVI aspect\n    axes[i].axis('off') # Turn off axis ticks for cleaner appearance\n    \n    # Overlay Detroit metro boundary on each subplot\n    metro_reprojected.boundary.plot(ax=axes[i], color='red', linewidth=1)\n\n# Remove the unused subplot in the 3x2 grid layout\nfig.delaxes(axes[5])\n\n# Add a single shared colorbar to indicate the SVI score scale across all subplots\ncbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7]) # Position colorbar at right side of plot\ncbar = fig.colorbar(im, cax=cbar_ax, label='SVI Score', \n                    fraction=0.047, pad=0.04, aspect=20) # Customize colorbar properties\n\nplt.tight_layout()  # Adjust layout for clarity and spacing\nplt.show() # Display the final plot\n\n\n\n\nThis provides an excellent look at demographic trends in the Detroit metro area. Overall vulnerability is highest in the inner city. The most striking drivers are the concentration of minorities and socioeconomic vulnerability in the downtown area. The housing and household components are slightly more varied throughout the region.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhat does the Social Vulnerability Index (SVI) primarily measure?\n\nAir pollution levels in different communities\nEconomic growth rates of different regions\nCommunities’ capacity to prepare for and respond to hazards\nPopulation density in urban areas\n\n\n\n\n\n\nIntegrating TRI and SVI\nThere are numerous ways to assess the relationships between the SVI and TRI data. For this lesson we’ll identify areas where air releases and vulnerability are highest together by creating a new rasterized index that combines both layers.\nTo begin we need to rasterize the air release point data. We will create an empty raster grid and “fill” the cells with the sum of the air release values from any points that are overlapped by the cells.\nStart by getting the bounds from our boundary object, specify the desired resolution, and set up the transform.\n\nfrom rasterio.transform import from_origin\n\n# Retrieve the spatial boundaries (min/max x and y coordinates) of the Detroit metro area\nminx, miny, maxx, maxy = detroit_metro_bm.total_bounds\n\n# Define the resolution of each cell in meters (5,000 meters = 5km) to align with SVI data resolution\nresolution = 5000\n\n# Calculate the number of cells needed to cover the Detroit metro area extent\n# in both x (width) and y (height) directions based on the resolution\nnx = int((maxx - minx) / resolution) # Number of cells horizontally\nny = int((maxy - miny) / resolution) # Number of cells vertically\n\n# Create the transformation for converting between the spatial extent and pixel grid of the raster\ntransform = from_origin(minx, maxy, resolution, resolution)\n\nNow we have to prepare the point values and rasterize them. A key thing to note here is the merge_alg=rasterio.enums.MergeAlg.add argument passed to the rasterize function. This argument makes sure that cells with more than 1 point will add the values together (the default is to simply replace).\n\nfrom rasterio import features\n\n# Prepare the geometries (points) and their associated values for rasterization\n# Each tuple contains a geometry (location) and the AIR_TOTAL_RELEASE value for raster cell assignment\nshapes = ((geom, value) for geom, value in zip(gdf_tri_form_r_bm.geometry, gdf_tri_form_r_bm.AIR_TOTAL_RELEASE))\n\n# Rasterize the point data into a grid, with each cell reflecting the air release values\nair_release_raster = features.rasterize(\n    shapes=shapes, # Geometries and values to rasterize\n    out_shape=(ny, nx), # Output raster shape, matching previously calculated cell count\n    transform=transform, # Transformation matrix for raster alignment with spatial extent\n    fill=0,  # Default fill value (0) for cells without data\n    all_touched=True, # Includes all cells touched by a geometry to ensure full coverage\n    merge_alg=rasterio.enums.MergeAlg.add) # Merge algorithm to sum overlapping values in cells\n\nWe rasterized with rasterio, but we need to convert this to an xarray object to continue and we need rioxarray to write the crs information for the xarray object.\n\nimport rioxarray\n\n# Convert the rasterized array to an xarray DataArray for enhanced data handling\n# Set coordinates to spatial extent using latitude (y) and longitude (x) for geospatial consistency\nair_release_raster_da = xr.DataArray(\n    air_release_raster, # Rasterized air release data\n    coords={'y': np.linspace(maxy, miny, ny), # Y-axis coordinates, from top to bottom\n        'x': np.linspace(minx, maxx, nx)}, # X-axis coordinates, from left to right\n    dims=['y', 'x']) # Dimension labels for DataArray\n\n# Write the Detroit metro area CRS to the DataArray for georeferencing\nair_release_raster_da.rio.write_crs(detroit_metro_bm.crs, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 26, x: 21)>\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.97 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nCoordinates:\n  * y            (y) float64 5.296e+06 5.291e+06 ... 5.17e+06 5.165e+06\n  * x            (x) float64 -9.316e+06 -9.311e+06 ... -9.212e+06 -9.207e+06\n    spatial_ref  int32 0xarray.DataArrayy: 26x: 210.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        4.9699998e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 5.1500000e+02, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00],\n       [0.0000000e+00, 0.0000000e+00, 4.9599999e-01, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 4.0000000e+01,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 3.5800001e-01,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n...\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 6.8400000e+02, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 1.9386000e+04, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n        0.0000000e+00]], dtype=float32)Coordinates: (3)y(y)float645.296e+06 5.291e+06 ... 5.165e+06array([5296389.522777, 5291140.507514, 5285891.492251, 5280642.476988,\n       5275393.461725, 5270144.446462, 5264895.431199, 5259646.415937,\n       5254397.400674, 5249148.385411, 5243899.370148, 5238650.354885,\n       5233401.339622, 5228152.324359, 5222903.309096, 5217654.293833,\n       5212405.27857 , 5207156.263307, 5201907.248044, 5196658.232781,\n       5191409.217518, 5186160.202255, 5180911.186993, 5175662.17173 ,\n       5170413.156467, 5165164.141204])x(x)float64-9.316e+06 ... -9.207e+06array([-9316265.622935, -9310791.642823, -9305317.66271 , -9299843.682598,\n       -9294369.702485, -9288895.722373, -9283421.74226 , -9277947.762148,\n       -9272473.782035, -9266999.801923, -9261525.821811, -9256051.841698,\n       -9250577.861586, -9245103.881473, -9239629.901361, -9234155.921248,\n       -9228681.941136, -9223207.961023, -9217733.980911, -9212260.000798,\n       -9206786.020686])spatial_ref()int320crs_wkt :PROJCS[\"WGS 84 / Pseudo-Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Mercator_1SP\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"scale_factor\",1],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],AUTHORITY[\"EPSG\",\"3857\"]]spatial_ref :PROJCS[\"WGS 84 / Pseudo-Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Mercator_1SP\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"scale_factor\",1],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],AUTHORITY[\"EPSG\",\"3857\"]]array(0)Indexes: (2)yPandasIndexPandasIndex(Index([ 5296389.522777066,  5291140.507514133,  5285891.492251201,\n        5280642.476988269, 5275393.4617253365,  5270144.446462404,\n       5264895.4311994715,  5259646.415936539,  5254397.400673607,\n        5249148.385410675,  5243899.370147742,   5238650.35488481,\n        5233401.339621877,  5228152.324358946,  5222903.309096013,\n        5217654.293833081,  5212405.278570148,  5207156.263307216,\n        5201907.248044284,  5196658.232781352,  5191409.217518419,\n       5186160.2022554865,  5180911.186992554,  5175662.171729622,\n         5170413.15646669,  5165164.141203757],\n      dtype='float64', name='y'))xPandasIndexPandasIndex(Index([ -9316265.62293524, -9310791.642822767, -9305317.662710294,\n       -9299843.682597823,  -9294369.70248535, -9288895.722372878,\n       -9283421.742260408, -9277947.762147935, -9272473.782035463,\n        -9266999.80192299, -9261525.821810517, -9256051.841698047,\n       -9250577.861585574, -9245103.881473102, -9239629.901360631,\n       -9234155.921248158, -9228681.941135686, -9223207.961023213,\n        -9217733.98091074,  -9212260.00079827, -9206786.020685798],\n      dtype='float64', name='x'))Attributes: (0)\n\n\nThe spaces between county borders can create odd values because there are no points right next to each other. We can clip the raster to our Detroit metro boundary to fix this.\n\n# Clip the rasterized DataArray to the Detroit metro boundary to limit data to the target area\nair_release_raster_da = air_release_raster_da.rio.clip(\n    detroit_metro_bm.geometry.values,  # Boundary geometry for clipping\n    detroit_metro_bm.crs,  # Coordinate reference system of the boundary\n    drop=False,   # Retain original raster size; cells outside geometry are masked\n    all_touched=True)  # Include any cells touched by the boundary for full coverage\n\nNow we can see how our rasterized air release values look.\n\nfrom matplotlib.colors import BoundaryNorm, ListedColormap\n\n# Define class intervals (breaks) for categorizing air release values into discrete color bands\nbreaks = [0, 1, 10, 100, 1000, 10000, 100000, 250000, 500000]\n\n# Create a custom colormap to represent different levels of air release with distinct colors\ncolors = ['#fffff3', '#FFFFCC', '#FFEDA0', '#FED976', '#FEB24C', '#FD8D3C', '#FC4E2A', '#E31A1C', '#B10026']\ncmap = ListedColormap(colors)\n\n# Create a normalization scheme to map values to the defined color breaks\nnorm = BoundaryNorm(breaks, cmap.N)\n\n# Set up the plot with a defined size\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Plot TRI facility points in blue, with adjusted transparency (alpha) for visibility\ngdf_tri_form_r_bm.plot(ax=ax, color='blue', markersize=10, alpha=0.7)\n\n# Plot the air release raster, using the custom colormap and normalization scheme\nim = ax.imshow(\n    air_release_raster_da,  # Raster data for air release\n    extent=[minx, maxx, miny, maxy],  # Spatial extent to match the Detroit metro bounds\n    origin='upper',  # Ensure raster is oriented correctly (top-down)\n    cmap=cmap,  # Apply custom colormap\n    norm=norm)  # Normalize color according to defined breaks\n\n# Add a color bar with labels indicating air release levels (discrete)\ncbar = plt.colorbar(\n    im, ax=ax, extend='max',  # Display max extension on color bar\n    label='Total Air Releases (pounds)',  # Label for color bar\n    ticks=breaks,  # Set breaks as tick positions\n    fraction=0.047, pad=0.04, aspect=20)  # Adjust color bar size and position\ncbar.ax.set_yticklabels([f'{b:,}' for b in breaks])  # Format tick labels with thousands separator\n\n# Plot Detroit metro area boundary as a black outline\ndetroit_metro_bm.boundary.plot(ax=ax, color='black', linewidth=2)\n\n# Set plot extent to focus on Detroit metro area\nax.set_xlim(minx, maxx)\nax.set_ylim(miny, maxy)\n\n# Add a title and axis labels\n\nax.set_title('TRI Air Total Release (100m resolution sum) with Facility Locations', fontsize=16)\nax.set_xlabel('X Coordinate')\nax.set_ylabel('Y Coordinate')\n\n# Optimize layout and display the plot\nplt.tight_layout()\nplt.show()\n\n# Output summary information\nprint(f\"Number of TRI facilities plotted: {len(gdf_tri_form_r_bm)}\")\nprint(f\"Total air releases: {gdf_tri_form_r_bm['AIR_TOTAL_RELEASE'].sum():,.2f}\")\nprint(f\"Maximum cell value in raster: {air_release_raster_da.max().values:,.2f}\")\n\n\n\n\nNumber of TRI facilities plotted: 211\nTotal air releases: 2,117,167.80\nMaximum cell value in raster: 434,344.84\n\n\nThe raster looks as expected with the highest values in locations where our previous map had the largest circles. We overlaid the points to see the sources of the values. The cell resolution is set to 5000m. This was selected so we can evaluate the map and the process we used to create the raster, but any value could be set depending on the intended use.\n\nAir Release Vulnerability Index\nNow that we’ve created our air release raster layer we can combine it with the SVI raster to create a new index identifying areas with the highest combination of air releases and vulnerability.\nStart by selecting just the SVI Overall layer and converting it to a rioxarray object so we can perform raster calculations. We’ll also ensure the 2 raster layers “line up” by reprojecting into the same coordinate reference system and matching their resolutions.\n\nfrom rasterio.enums import Resampling\n\n# Select the 'Overall' layer from the Social Vulnerability Index (SVI) dataset specific to Detroit\nsvi_overall = svi_detroit.sel(layer='Overall')\n\n# Convert SVI layer to rioxarray for CRS-based geospatial operations\nsvi_overall = svi_overall.rio.write_crs(\"EPSG:4326\")\n\n# Reproject the SVI layer to match the CRS and spatial alignment of the air release raster\nsvi_reprojected = svi_overall.rio.reproject_match(air_release_raster_da)\n\n# Adjust the resolution of air release data to match the SVI resolution using bilinear resampling\nair_release_disaggregated = svi_reprojected.rio.reproject_match(\n    svi_reprojected,\n    resampling=Resampling.bilinear\n)\n\nThere are a few more steps that will aid in interpretation.\n\nWe will take the log of the air release data to reduce the impact of major outliers.\nWe will scale the logged air release data to 0-1 to match the SVI data. Now the resulting index we create will have equal contributions from both datasets.\n\n\n# Apply log transformation (log1p) to compress wide-ranging air release values, then normalize to a 0-1 scale\nair_release_log = np.log1p(air_release_disaggregated)\nair_release_scaled = (air_release_log - air_release_log.min()) / (air_release_log.max() - air_release_log.min())\n\n\n# Multiply the scaled air release data with the reprojected SVI data to create a vulnerability indicator\n# This indicator combines environmental impact (air releases) with social vulnerability\nvulnerability_indicator = air_release_scaled * svi_reprojected\n\nNow let’s visualize our index along with the inputs.\n\n# Create a figure with 3 subplots stacked vertically for SVI, Air Release, and Vulnerability Indicator\nfig, axs = plt.subplots(3, 1, figsize=(8, 20))\n\n# Plot SVI Overall data with a color map (viridis), normalized between 0 and 1\nim1 = svi_reprojected.plot(ax=axs[0], cmap='viridis', vmin=0, vmax=1, add_colorbar=False)\nplt.colorbar(im1, ax=axs[0], label='SVI Overall', \n                    fraction=0.047, pad=0.04, aspect=20)\naxs[0].set_title('Social Vulnerability Index (Overall)', fontsize=16)\ndetroit_metro.boundary.plot(ax=axs[0], color='black', linewidth=2) # Add Detroit metro boundary\n\n# Plot the log-transformed Air Release data for enhanced visual differentiation\nim2 = np.log1p(air_release_disaggregated).plot(ax=axs[1], cmap='YlOrRd', add_colorbar=False)\nplt.colorbar(im2, ax=axs[1], label='Log(Air Release + 1)', \n                    fraction=0.047, pad=0.04, aspect=20)\naxs[1].set_title('Air Release (Log-transformed)', fontsize=16)\ndetroit_metro.boundary.plot(ax=axs[1], color='black', linewidth=2)\n\n# Plot the Vulnerability Indicator as the product of SVI and scaled air release data\nim3 = vulnerability_indicator.plot(ax=axs[2], cmap='YlOrRd', vmin=0, vmax=1, add_colorbar=False)\nplt.colorbar(im3, ax=axs[2], label='Air Release Vulnerability Indicator', \n                    fraction=0.047, pad=0.04, aspect=20)\naxs[2].set_title('Air Release Vulnerability Indicator\\n(Scaled Air Release * SVI)', fontsize=16)\ndetroit_metro.boundary.plot(ax=axs[2], color='black', linewidth=2)\n\n# Set consistent axis limits and labels across all subplots\nfor ax in axs:\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    ax.set_xlim(svi_reprojected.x.min(), svi_reprojected.x.max())\n    ax.set_ylim(svi_reprojected.y.min(), svi_reprojected.y.max())\n\n# Adjust layout and display the plots\nplt.tight_layout()\nplt.show()\n\n# Print out the maximum value of the vulnerability indicator for reference\nprint(f\"Maximum vulnerability indicator: {vulnerability_indicator.max().values:.4f}\")\n\n\n\n\nMaximum vulnerability indicator: 0.9773\n\n\nAs one would expect, there are several areas in the downtown river corridor with very high combinations of SVI and TRI releases. The maximum score for our index is 0.9286! That is an extremely vulnerable community facing extraordinary levels of air pollution.\nWe can get a better sense of where these communities are by extracting the location of the cells with the highest scores and placing them on a basemap of Detroit.\nWe’ll convert the raster into a data frame, sort the values in descending order, and then extract the coordinates.\n\n# Convert vulnerability indicator to a DataFrame for easy sorting and manipulation\nvulnerability_df = vulnerability_indicator.to_dataframe(name='index').reset_index()\n\n# Sort by index value (descending) to identify top 10 most vulnerable areas\ntop_10 = vulnerability_df.sort_values('index', ascending=False).head(10)\n\n# Create GeoDataFrame from the top 10 points using coordinates as geometry\ntop_10['geometry'] = gpd.points_from_xy(top_10.x, top_10.y)\ntop_10_gdf = gpd.GeoDataFrame(top_10, geometry='geometry', crs=vulnerability_indicator.rio.crs)\n\nNow let’s make a map.\n\n# Create the final map plot to display top 10 vulnerable areas with basemap and boundaries\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot Detroit metro area boundary in black\ndetroit_metro_bm.boundary.plot(ax=ax, color='black', linewidth=2)\n\n# Plot the top 10 highest vulnerability points with blue markers\ntop_10_gdf.plot(ax=ax, color='blue', markersize=100, alpha=0.7)\n\n# Annotate each top point with its rank for clarity\nfor idx, row in top_10_gdf.iterrows():\n    ax.annotate(f\"#{idx+1}\", (row.geometry.x, row.geometry.y), \n                xytext=(3, 3), textcoords=\"offset points\", \n                color='black', fontweight='bold')\n\n# Add a basemap to provide geographic context, using OpenStreetMap tiles\nctx.add_basemap(ax, crs=vulnerability_indicator.rio.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Overlay a bounding box in blue as a border around the plotted area\nbbox_polygon_bm.boundary.plot(ax=ax, color='#315c86', linewidth=3)\n\n# Limit map extent to focus on Detroit metro\nax.set_xlim(vulnerability_indicator.x.min(), vulnerability_indicator.x.max())\nax.set_ylim(vulnerability_indicator.y.min(), vulnerability_indicator.y.max())\n\n# Add title and remove axis display for a cleaner map\nax.set_title('Top 10 Areas with Highest Air Release Vulnerability Index', fontsize=16)\nax.set_axis_off()\n\n# Display the final map\nplt.tight_layout()\nplt.show()\n\n# Output index and vulnerability value of the top 10 areas for review\nprint(\"Row index and index value of the top 10 points:\")\nfor i, (idx, row) in enumerate(top_10_gdf.iterrows(), 1):\n    print(f\"Row index = {idx}, Index value = {round(row['index'], 2)}\")\n\n\n\n\nRow index and index value of the top 10 points:\nRow index = 347, Index value = 0.98\nRow index = 134, Index value = 0.97\nRow index = 155, Index value = 0.97\nRow index = 306, Index value = 0.96\nRow index = 369, Index value = 0.96\nRow index = 305, Index value = 0.94\nRow index = 285, Index value = 0.94\nRow index = 330, Index value = 0.93\nRow index = 348, Index value = 0.92\nRow index = 324, Index value = 0.9\n\n\nAs the input layers suggest, the most vulnerable and polluted areas are along the river corridor in Lincoln Park, Melvindale, Mexicantown, and River Rouge. There are multiple locations with indexes greater than 0.70. This is valuable information for targeted relief or mitigation programs.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhat was the main purpose of combining the TRI and SVI data in this analysis?\n\nTo calculate total pollution levels for each county\nTo identify areas with both high air releases and high social vulnerability\nTo determine the most populous areas in Detroit\nTo predict future industrial development zones\n\n\n\n\n\n\nCDC PLACES\nThe CDC PLACES (Population Level Analysis and Community Estimates) dataset is a collaboration between the Centers for Disease Control and Prevention (CDC), the Robert Wood Johnson Foundation, and the CDC Foundation. It provides model-based population-level analysis and community estimates of health indicators for all counties, places (incorporated and census designated places), census tracts, and ZIP Code Tabulation Areas (ZCTAs) across the United States. Some key points to consider when working with CDC PLACES:\n\nSpatial Extent: Entire United States, including all 50 states, the District of Columbia, and Puerto Rico.\nSpatial Resolution: Multiple levels including counties, cities/towns, census tracts, and ZIP codes.\nIndicators: Wide range of chronic disease measures related to health outcomes, prevention, and health risk behaviors.\nData Sources:\n\nBehavioral Risk Factor Surveillance System (BRFSS)\nU.S. Census Bureau’s American Community Survey (ACS)\n\nMethodology: Uses small area estimation methods for small geographic areas.\nHealth Measures Include:\n\nChronic diseases: e.g., asthma, COPD, heart disease, diabetes\nHealth risk behaviors: e.g., smoking, physical inactivity, binge drinking\nPrevention practices: e.g., health insurance coverage, dental visits, cholesterol screening\n\nSocioeconomic Data: Includes some socioeconomic and demographic variables.\nAnnual Updates: Providing recent estimates for local areas.\n\nThis dataset is valuable for public health researchers, policymakers, and community organizations. It provides a standardized way to compare health indicators across different geographic areas and can be used to inform targeted interventions and policy decisions, especially in addressing health disparities at a local level.\nAs with ICIS-AIR and TRI, PLACES is available through a web search interface and a programmatic API. We will focus on the API in this example.\n\nProcessing\nWe’ll start by accessing the CDC PLACES data through their API and processing it for our analysis. First we set up our API request. We specify the endpoint URL, define our target counties, and create a filter to select data only for these counties in Michigan.\n\n# Define the GeoJSON API endpoint URL, specific to CDC health data\nurl = \"https://data.cdc.gov/resource/cwsq-ngmh.geojson\"\n\n# Define the list of Detroit metro area counties of interest\ndetroit_counties = ['Wayne', 'Oakland', 'Macomb']\n\n# Construct the query filter string to specify only Detroit metro counties\ncounty_filter = \" OR \".join([f\"countyname = '{county}'\" for county in detroit_counties])\n\n# Define query parameters to retrieve data for Detroit metro counties in Michigan\nparams = {\n    \"$where\": f\"stateabbr = 'MI' AND ({county_filter})\",\n    \"$limit\": 50000   # Limit the results to a manageable number\n}\n\nNext, let’s make the API request and process the response:\n\n# Perform the API request to retrieve GeoJSON data for Detroit area health statistics\nresponse = requests.get(url, params=params)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json() # Parse the JSON data\n    print(f\"Successfully retrieved data\")\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    print(response.text)\n\n# Convert the GeoJSON text to a GeoDataFrame for spatial analysis\ngdf = gpd.read_file(response.text)\n\nSuccessfully retrieved data\n\n\nYou can inspect the gdf return object on your own; it’s too large to print here. Let’s just take a look at the available health measures in CDC PLACES.\n\n# Display unique health measures available in the dataset to understand data variety\nprint(\"\\nAvailable health measures:\")\nprint(gdf['measure'].unique())\n\n# Print essential details of the GeoDataFrame structure and content\nprint(\"\\nGeoDataFrame Info:\")\nprint(gdf.info())\n\n\nAvailable health measures:\n['Frequent physical distress among adults'\n 'Cancer (non-skin) or melanoma among adults'\n 'Visited dentist or dental clinic in the past year among adults'\n 'Cognitive disability among adults' 'Coronary heart disease among adults'\n 'Current asthma among adults'\n 'Current lack of health insurance among adults aged 18-64 years'\n 'Lack of social and emotional support among adults'\n 'Arthritis among adults' 'Obesity among adults'\n 'Self-care disability among adults' 'Mobility disability among adults'\n 'Short sleep duration among adults'\n 'Frequent mental distress among adults'\n 'Fair or poor self-rated health status among adults'\n 'Lack of reliable transportation in the past 12 months among adults'\n 'All teeth lost among adults aged >=65 years'\n 'Taking medicine to control high blood pressure among adults with high blood pressure'\n 'Colorectal cancer screening among adults aged 45–75 years'\n 'Vision disability among adults'\n 'Visits to doctor for routine checkup within the past year among adults'\n 'Diagnosed diabetes among adults'\n 'Mammography use among women aged 50-74 years'\n 'Independent living disability among adults'\n 'Hearing disability among adults' 'Stroke among adults'\n 'Chronic obstructive pulmonary disease among adults'\n 'Food insecurity in the past 12 months among adults'\n 'Feeling socially isolated among adults'\n 'High cholesterol among adults who have ever been screened'\n 'High blood pressure among adults'\n 'No leisure-time physical activity among adults'\n 'Cholesterol screening among adults'\n 'Housing insecurity in the past 12 months among adults'\n 'Utility services shut-off threat in the past 12 months among adults'\n 'Binge drinking among adults'\n 'Received food stamps in the past 12 months among adults'\n 'Current cigarette smoking among adults' 'Any disability among adults'\n 'Depression among adults']\n\nGeoDataFrame Info:\n<class 'geopandas.geodataframe.GeoDataFrame'>\nRangeIndex: 46560 entries, 0 to 46559\nData columns (total 24 columns):\n #   Column                      Non-Null Count  Dtype   \n---  ------                      --------------  -----   \n 0   measure                     46560 non-null  object  \n 1   low_confidence_limit        46560 non-null  object  \n 2   data_value_unit             46560 non-null  object  \n 3   data_value                  46560 non-null  object  \n 4   short_question_text         46560 non-null  object  \n 5   statedesc                   46560 non-null  object  \n 6   totalpop18plus              46560 non-null  object  \n 7   locationid                  46560 non-null  object  \n 8   countyname                  46560 non-null  object  \n 9   year                        46560 non-null  object  \n 10  high_confidence_limit       46560 non-null  object  \n 11  categoryid                  46560 non-null  object  \n 12  stateabbr                   46560 non-null  object  \n 13  data_value_footnote         0 non-null      object  \n 14  data_value_type             46560 non-null  object  \n 15  data_value_footnote_symbol  0 non-null      object  \n 16  locationname                46560 non-null  object  \n 17  category                    46560 non-null  object  \n 18  datavaluetypeid             46560 non-null  object  \n 19  measureid                   46560 non-null  object  \n 20  countyfips                  46560 non-null  object  \n 21  datasource                  46560 non-null  object  \n 22  totalpopulation             46560 non-null  object  \n 23  geometry                    46560 non-null  geometry\ndtypes: geometry(1), object(23)\nmemory usage: 8.5+ MB\nNone\n\n\nThere’s a lot of great data here considering the rarity of health outcomes data; especially at this resolution (census tracts). Because we’re dealing with air quality in this lesson we’ll use “Current asthma among adults.”\nLet’s make a quick map to see how it looks.\n\n# Create a sample map to visualize data for a specific health measure, such as asthma prevalence\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Filter the data for the asthma measure and ensure numeric conversion of data values\ngdf_asthma = gdf[gdf['measure'] == 'Current asthma among adults'].copy()\ngdf_asthma['data_value'] = pd.to_numeric(gdf_asthma['data_value'], errors='coerce')\n\n# Plot asthma data with color variation based on prevalence\ngdf_asthma.plot(column='data_value', \n                ax=ax, \n                legend=True, \n                legend_kwds={'label': 'Asthma Prevalence (%)', 'orientation': 'horizontal'},\n                cmap='YlOrRd',\n                missing_kwds={'color': 'lightgrey'}) # Specify color for missing data\n\n# Add basemap for geographic context\nctx.add_basemap(ax, crs=gdf_asthma.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Overlay the Detroit metro area boundary\nbbox_polygon_bm.boundary.plot(ax=ax, color='#315c86', linewidth=3)\n\n# Set map extent to focus on the Detroit metro area\nax.set_xlim(gdf_asthma.total_bounds[0], gdf_asthma.total_bounds[2])\nax.set_ylim(gdf_asthma.total_bounds[1], gdf_asthma.total_bounds[3])\n\n# Add map title and hide axis lines for a cleaner visualization\nplt.title('Asthma Prevalence in Detroit Metro Area', fontsize=16)\nax.axis('off')\n\n# Adjust layout and show the map\nplt.tight_layout()\nplt.show()\n\n\n\n\nIf you hadn’t already noticed while inspecting the object we returned from the API, the census tract data are actually centroid points–not census tract boundaries.\nWe created a choropleth map showing asthma prevalence across the Detroit metro area. With the YlOrRd colormap darker red indicates higher asthma prevalence.\nFinally, let’s print some statistics about the asthma data:\n\n# Calculate and display key statistics for asthma prevalence in the dataset\nprint(\"\\nAsthma Statistics:\")\nprint(f\"Average asthma prevalence: {gdf_asthma['data_value'].mean():.2f}%\")\nprint(f\"Minimum asthma prevalence: {gdf_asthma['data_value'].min():.2f}%\")\nprint(f\"Maximum asthma prevalence: {gdf_asthma['data_value'].max():.2f}%\")\n\n# Count and display the number of census tracts in each county to understand data distribution\nprint(\"\\nNumber of census tracts per county:\")\nprint(gdf_asthma['countyname'].value_counts())\n\n\nAsthma Statistics:\nAverage asthma prevalence: 12.06%\nMinimum asthma prevalence: 7.20%\nMaximum asthma prevalence: 17.20%\n\nNumber of census tracts per county:\ncountyname\nWayne      583\nOakland    345\nMacomb     236\nName: count, dtype: int64\n\n\nWow. Some census tracts along the river front and central downtown area have adult asthma prevalence of nearly 20%! I’m not an epidemiologist, but that seems extremely high; especially when you compare to the outer suburbs and more affluent areas in the Royal Oak-Ferndale area. However, in light of our exploration of TRI and ICIS-AIR data it might not be surprising to see elevated asthma rates in these areas.\nGiven that we’ve mostly been working with and creating raster data in the previous sections, it might be easier to draw systematic comparisons between pollution and health outcomes if the CDC data is also in a raster format.\n\n\nInterpolate a Surface w/ IDW\nWhile the choropleth map provides a good overview, it can be affected by the varying sizes of census tracts. To get a smoother representation of the data that resembles our air quality data, we can use Inverse Distance Weighting (IDW) interpolation to create a continuous surface.\nFirst, let’s prepare our data for interpolation by reprojecting, extracting the coordinates and the asthma value, and masking the data for any asthma values that may be NA.\n\nfrom scipy.interpolate import griddata\nfrom rasterio.transform import from_origin\nfrom rasterio.warp import transform_bounds\n\n# Ensure the GeoDataFrame for asthma data is in EPSG:4326 (latitude and longitude)\ngdf_asthma = gdf_asthma.to_crs(epsg=4326)\n\n# Extract longitude (X), latitude (Y), and asthma prevalence values (Z)\nX = gdf_asthma.geometry.x.values\nY = gdf_asthma.geometry.y.values\nZ = gdf_asthma['data_value'].values\n\n# Mask out any NaN values in the prevalence data to ensure valid input for interpolation\nmask = ~np.isnan(Z)\nX, Y, Z = X[mask], Y[mask], Z[mask]\n\n\n\n\n\n\n\n\n\nData Science Review\n\n\n\nInverse Distance Weighting (IDW) is a spatial interpolation method used to estimate values at unknown locations based on known values at nearby points. The fundamental principle of IDW is that points closer to the location being estimated have more influence than those farther away. IDW calculates the estimated value as a weighted average of nearby known values, with weights inversely proportional to the distance between the known point and the estimation location. IDW is popular in geographic information systems (GIS) and environmental sciences due to its simplicity and effectiveness in creating continuous surfaces from point data, such as elevation, temperature, or pollution levels. While IDW has advantages like preserving local variation and working well with evenly distributed points, it also has limitations; including potential “bull’s-eye” patterns around data points, inability to account for spatial trends or barriers, and sensitivity to outliers and clustering of input points.\n\n\nNow let’s create the structure of the raster we’ll create. This is based on our desired resolution and the extent of our data.\n\n# Define grid properties to perform interpolation over a defined spatial extent and resolution\n\n# Set the grid resolution in degrees for interpolation\ngrid_resolution = 0.025\n# Define the spatial bounds (extent) of the grid based on asthma data coverage\nx_min, y_min, x_max, y_max = gdf_asthma.total_bounds\n\n# Generate equally spaced coordinates based on grid resolution for X and Y dimensions\ngrid_x = np.arange(x_min, x_max, grid_resolution)\ngrid_y = np.arange(y_min, y_max, grid_resolution)\n\n# Create a 2D grid of coordinates by meshing grid_x and grid_y\ngrid_xx, grid_yy = np.meshgrid(grid_x, grid_y)\n\nNow we’re ready to interpolate the surface.\n\n# Perform interpolation using Inverse Distance Weighting (IDW)\n\n# Stack longitude and latitude into coordinate pairs for IDW interpolation\npoints = np.column_stack((X, Y))\n\n# Use linear interpolation to estimate asthma prevalence values on the grid\ngrid_z = griddata(points, Z, (grid_xx, grid_yy), method='linear')\n\nFirst we created a regular grid over our study area and then used the griddata function to perform linear interpolation (which is a form of IDW) on our asthma prevalence data.\nNow, let’s convert our interpolated data into an xarray Dataset so it’s easier to manipulate.\n\n# Convert the interpolated grid into an xarray Dataset for easier handling and export\n\n# Create an xarray dataset with interpolated asthma prevalence data, labeled by grid coordinates\nds = xr.Dataset({\n    'asthma': (['y', 'x'], grid_z),  # 'asthma' is the interpolated variable over the grid\n    'x': grid_x,  # X-coordinates (longitude)\n    'y': grid_y  # Y-coordinates (latitude)\n})\n\n# Assign a coordinate reference system (CRS) to the dataset for spatial consistency\nds.rio.write_crs(\"EPSG:4326\", inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (y: 33, x: 38)\nCoordinates:\n  * x            (x) float64 -83.67 -83.64 -83.62 ... -82.79 -82.77 -82.74\n  * y            (y) float64 42.06 42.09 42.11 42.14 ... 42.79 42.81 42.84 42.86\n    spatial_ref  int32 0\nData variables:\n    asthma       (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nanxarray.DatasetDimensions:y: 33x: 38Coordinates: (3)x(x)float64-83.67 -83.64 ... -82.77 -82.74array([-83.665503, -83.640503, -83.615503, -83.590503, -83.565503, -83.540503,\n       -83.515503, -83.490503, -83.465503, -83.440503, -83.415503, -83.390503,\n       -83.365503, -83.340503, -83.315503, -83.290503, -83.265503, -83.240503,\n       -83.215503, -83.190503, -83.165503, -83.140503, -83.115503, -83.090503,\n       -83.065503, -83.040503, -83.015503, -82.990503, -82.965503, -82.940503,\n       -82.915503, -82.890503, -82.865503, -82.840503, -82.815503, -82.790503,\n       -82.765503, -82.740503])y(y)float6442.06 42.09 42.11 ... 42.84 42.86array([42.060038, 42.085038, 42.110038, 42.135038, 42.160038, 42.185038,\n       42.210038, 42.235038, 42.260038, 42.285038, 42.310038, 42.335038,\n       42.360038, 42.385038, 42.410038, 42.435038, 42.460038, 42.485038,\n       42.510038, 42.535038, 42.560038, 42.585038, 42.610038, 42.635038,\n       42.660038, 42.685038, 42.710038, 42.735038, 42.760038, 42.785038,\n       42.810038, 42.835038, 42.860038])spatial_ref()int320crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]array(0)Data variables: (1)asthma(y, x)float64nan nan nan nan ... nan nan nan nanarray([[        nan,         nan,         nan, ...,         nan,\n                nan,         nan],\n       [        nan,         nan,         nan, ...,         nan,\n                nan,         nan],\n       [        nan,         nan,         nan, ...,         nan,\n                nan,         nan],\n       ...,\n       [        nan,         nan, 11.45419847, ..., 11.48577223,\n        11.91445179,         nan],\n       [        nan,         nan, 11.36882026, ..., 11.54773592,\n                nan,         nan],\n       [        nan,         nan,         nan, ...,         nan,\n                nan,         nan]])Indexes: (2)xPandasIndexPandasIndex(Index([        -83.665503,         -83.640503, -83.61550299999999,\n       -83.59050299999998, -83.56550299999998, -83.54050299999997,\n       -83.51550299999997, -83.49050299999996, -83.46550299999996,\n       -83.44050299999995, -83.41550299999994, -83.39050299999994,\n       -83.36550299999993, -83.34050299999993, -83.31550299999992,\n       -83.29050299999992, -83.26550299999991,  -83.2405029999999,\n        -83.2155029999999,  -83.1905029999999, -83.16550299999989,\n       -83.14050299999988, -83.11550299999988, -83.09050299999987,\n       -83.06550299999986, -83.04050299999986, -83.01550299999985,\n       -82.99050299999985, -82.96550299999984, -82.94050299999984,\n       -82.91550299999983, -82.89050299999982, -82.86550299999982,\n       -82.84050299999981, -82.81550299999981,  -82.7905029999998,\n        -82.7655029999998, -82.74050299999979],\n      dtype='float64', name='x'))yPandasIndexPandasIndex(Index([        42.0600376,         42.0850376,         42.1100376,\n               42.1350376, 42.160037599999995, 42.185037599999994,\n        42.21003759999999,  42.23503759999999,  42.26003759999999,\n        42.28503759999999,  42.31003759999999, 42.335037599999986,\n       42.360037599999984,  42.38503759999998,  42.41003759999998,\n        42.43503759999998,  42.46003759999998,  42.48503759999998,\n       42.510037599999976, 42.535037599999974,  42.56003759999997,\n        42.58503759999997,  42.61003759999997,  42.63503759999997,\n        42.66003759999997, 42.685037599999966, 42.710037599999964,\n        42.73503759999996,  42.76003759999996,  42.78503759999996,\n        42.81003759999996,  42.83503759999996, 42.860037599999956],\n      dtype='float64', name='y'))Attributes: (0)\n\n\nThis step allows us to work with our interpolated data using xarray, which provides powerful tools for working with multi-dimensional arrays.\nNext, we’ll clip our interpolated surface to the Detroit metro area. This ensures our data matches our other rasters. Then we’ll project it to match basemaps so we can create a map of the data.\n\n# Clip the asthma dataset to the boundaries of the Detroit metro area\n# Ensures data outside of the metro area is excluded\nds_clipped = ds.rio.clip(detroit_metro.geometry, drop=False)\n\n# Reproject the clipped dataset to Web Mercator (EPSG:3857) for basemap compatibility\nds_3857 = ds_clipped.rio.reproject(\"EPSG:3857\")\n\nFinally, let’s create a map of our interpolated asthma prevalence:\n\n# Initialize the plot for visualizing interpolated asthma prevalence\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Display the interpolated asthma prevalence data, adjusting transparency and color map\nim = ds_3857.asthma.plot(ax=ax, cmap='YlOrRd', \n                         alpha=0.7, add_colorbar=False)\n\n# Add a colorbar with custom size and label for asthma prevalence\ncbar = plt.colorbar(im, ax=ax, label='Asthma Prevalence', \n                    fraction=0.047, pad=0.04, aspect=20)\n\n# Add a basemap for geographical context using Web Mercator projection\nctx.add_basemap(ax, crs=ds_3857.rio.crs, \n                source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Plot Detroit metro area boundary for clear region distinction\nbbox_polygon_bm.boundary.plot(ax=ax, color='#315c86', linewidth=3)\n\n# Set plot boundaries to focus on the Detroit metro area only\nbounds_3857 = transform_bounds(\"EPSG:4326\", \"EPSG:3857\", \n                               x_min, y_min, x_max, y_max)\nax.set_xlim(bounds_3857[0], bounds_3857[2])\nax.set_ylim(bounds_3857[1], bounds_3857[3])\n\n# Add title and remove default axis for clean presentation\nplt.title('IDW Interpolated Asthma Prevalence in Detroit Metro Area', fontsize=16)\nax.axis('off')\n\n# Optimize layout\nplt.tight_layout()\nplt.show()\n\n\n\n\nThis code creates a map of the interpolated asthma prevalence, providing a smooth surface that shows the spatial variation in asthma rates across the Detroit metro area. The use of IDW interpolation helps to create a continuous surface from our discrete census tract data, potentially revealing patterns that might be less apparent in the choropleth map.\nBy comparing this interpolated surface with our earlier maps of air pollution and social vulnerability, we can start to explore potential relationships between environmental factors, social conditions, and health outcomes in the Detroit metro area.\nWe can start with a simple correlation analysis and then visualize it with a scatterplot. The simplest way to perform the analysis is to extract the raw value pairs and calculate the Pearson correlation.\n\nfrom scipy import stats\n\n# Reproject the air release data to align with the asthma dataset for comparison\nair_release_aligned = air_release_raster_da.rio.reproject_match(ds_clipped)\n\n# Flatten the raster arrays for correlation analysis\nair_release_flat = air_release_aligned.values.flatten()\n# Apply logarithmic transformation to air release data to reduce impact of extreme values\nair_release_flat = np.log1p(air_release_flat)\nasthma_flat = ds_clipped.asthma.values.flatten()\n\n# Mask to exclude NaN values for accurate correlation calculation\nmask = ~np.isnan(air_release_flat) & ~np.isnan(asthma_flat)\n\n# Compute Pearson correlation coefficient to measure association between air releases and asthma prevalence\ncorrelation, p_value = stats.pearsonr(air_release_flat[mask], asthma_flat[mask])\nprint(f\"Correlation coefficient: {correlation}\")\nprint(f\"P-value: {p_value}\")\n\nCorrelation coefficient: 0.3295258832567963\nP-value: 4.7142164620235564e-23\n\n\nCreating a scatterplot of the paired values can provide a better look at what’s going on.\n\n# Initialize the figure for scatter plot\nplt.figure(figsize=(7, 7))\n\n# Create a scatter plot comparing TRI Air Releases against Asthma Prevalence\nplt.scatter(air_release_flat[mask], asthma_flat[mask], alpha=0.5)\n\n# Label axes for clarity\nplt.xlabel('TRI Air Releases')\nplt.ylabel('Asthma Prevalence')\n\n# Add a descriptive title to convey the analysis purpose\nplt.title('TRI Air Releases vs Asthma Prevalence')\n\n# Display the scatter plot\nplt.show()\n\n\n\n\nThis paints a clearer picture. Although there is a modest positive correlation, there are numerous cells with 0 TRI Air Release and a wide variety of Asthma prevalence values, which negates any correlation we might be seeing at higher release levels. This is a simple pixel by pixel comparison of matching values. We can perform a more robust analysis accounting for the spatial structure using a global autocorrelation analysis and Moran’s I.\nPearson’s correlation and Moran’s I are both measures of association, but they serve different purposes and account for different aspects of data. Pearson’s correlation measures the linear relationship between two variables without considering their spatial context. It simply looks at how two variables change together, regardless of where the data points are located in space. In contrast, Moran’s I is specifically designed for spatial data and measures spatial autocorrelation – the degree to which a variable is correlated with itself across geographic space. Moran’s I takes into account both the values of observations and their spatial relationships, typically using a spatial weights matrix. While Pearson’s correlation can tell you if two variables are related overall, Moran’s I can reveal whether high or low values of a variable tend to cluster together in space, or if they’re randomly distributed. This makes Moran’s I particularly useful for analyzing geographical patterns and spatial dependencies, which are crucial in fields like environmental science, epidemiology, and urban planning.\nWe can perform this analysis in Python using pysal. We already know our data is spatially aligned from the previous code chunks. We can create a spatial data frame and continue on.\n\nfrom pysal.explore import esda\nfrom pysal.lib import weights\nimport shapely\n\n# Convert aligned raster data to a GeoDataFrame for spatial analysis\ndf = gpd.GeoDataFrame(\n    {\n        # Include air release and asthma prevalence data as columns\n        'air_release': air_release_aligned.values.flatten(),\n        'asthma': ds_clipped.asthma.values.flatten(),\n\n        # Create geometry points for each raster cell\n        'geometry': [\n            shapely.geometry.Point(x, y) \n            for x, y in zip(\n                np.repeat(air_release_aligned.x.values, len(air_release_aligned.y)),\n                np.tile(air_release_aligned.y.values, len(air_release_aligned.x))\n            )\n        ]\n    }\n)\n\n\n\n\n\n\n\n\n\nStatistics Review\n\n\n\nA p-value, or probability value, is a statistical concept used to determine the significance of results in hypothesis testing. It represents the probability of obtaining results at least as extreme as the observed results, assuming that the null hypothesis is true. In simpler terms, it measures the strength of evidence against the null hypothesis. P-values range from 0 to 1, with lower values indicating stronger evidence against the null hypothesis. Typically, a p-value below a predetermined threshold (often 0.05) is considered statistically significant, suggesting that the observed results are unlikely to have occurred by chance alone. However, it’s important to note that while p-values can indicate the strength of evidence against the null hypothesis, they do not prove or disprove hypotheses, nor do they measure the size or importance of an effect. They should be interpreted in conjunction with other factors such as effect size, sample size, and practical significance.\n\n\nCheck for missing values and create the spatial weights matrix that determines how wide of a net we want to cast as we search for a local dependence structure (you could change these values and see how it impacts the results). You can read more about settings for spatial autocorrelation and Moran’s I analysis here.\n\n# Remove rows with any NaN values to ensure valid spatial statistics\ndf = df.dropna()\n\n# Create a spatial weights matrix based on the 8 nearest neighbors for each observation\n# The row-standardization ('r') ensures that each row sums to 1, which is useful for Moran's I calculation\nw = weights.distance.KNN.from_dataframe(df, k=8)  \nw.transform = 'r'  \n\nNow we’ll cycle through the Moran function for air release, current asthma among adults, and the 2 combined (bivariate). First, air releases.\n\n# Import the moran_scatterplot function for visualizing spatial autocorrelation\nfrom splot.esda import moran_scatterplot\n\n# Calculate Moran's I statistic to measure spatial autocorrelation in air release data\nmoran_air = esda.Moran(df['air_release'], w)\n\n# Print Moran's I value and the associated p-value for significance assessment\nprint(\"Moran's I for Air Release:\")\nprint(f\"I: {moran_air.I}\")\nprint(f\"p-value: {moran_air.p_sim}\")\n\n# Generate a Moran scatter plot to visualize spatial autocorrelation for air release values\nfig, ax = plt.subplots(figsize=(7, 7))\nmoran_scatterplot(moran_air, ax=ax)\n\n# Set plot limits to ensure both axes have the same range for accurate spatial comparison\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\nvmin = min(xlim[0], ylim[0])\nvmax = max(xlim[1], ylim[1])\nax.set_xlim(vmin, vmax)\nax.set_ylim(vmin, vmax)\n\n# Set the aspect ratio to 1:1 to maintain equal scaling on both axes\nax.set_aspect('equal', adjustable='box')\n\n# Add a descriptive title to the plot\nax.set_title(\"Moran Scatter Plot: Air Release\")\nplt.show()\n\nMoran's I for Air Release:\nI: 0.11613297412675305\np-value: 0.001\n\n\n\n\n\nWhen evaluating Moran’s I for single variables we are assessing how clustered like values are, i.e. do high or low values of air release tend to appear next to each other. On a scale of -1 to 1 a 0.12 suggests a slight positive correlation. High values of air release tend to cluster in similar spots. That said the correlation is lower and we can the same feature as before in that there are a large amount of 0 values that are mixed in all throughout the data. You could attempt to address this issue by using a larger resolution when creating the raster layer depicting air release sums (we used 5000m), or by including different sources of air pollution.\n\n# Calculate Moran's I statistic for spatial autocorrelation in asthma prevalence values\nmoran_asthma = esda.Moran(df['asthma'], w)\n\n# Display Moran's I value and p-value to assess the spatial pattern in asthma prevalence\nprint(\"\\nMoran's I for Asthma Prevalence:\")\nprint(f\"I: {moran_asthma.I}\")\nprint(f\"p-value: {moran_asthma.p_sim}\")\n\n# Generate a Moran scatter plot to visualize spatial autocorrelation in asthma prevalence data\nfig, ax = plt.subplots(figsize=(7, 7))\nmoran_scatterplot(moran_asthma, ax=ax)\n\n# Set plot limits and aspect ratio to ensure a 1:1 scale\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\nvmin = min(xlim[0], ylim[0])\nvmax = max(xlim[1], ylim[1])\nax.set_xlim(vmin, vmax)\nax.set_ylim(vmin, vmax)\nax.set_aspect('equal', adjustable='box')\n\n# Set title for the plot\nax.set_title(\"Moran Scatter Plot: Asthma Prevalence\")\nplt.show()\n\n\nMoran's I for Asthma Prevalence:\nI: 0.5172838306608204\np-value: 0.001\n\n\n\n\n\nAsthma prevalence has a much stronger correlation structure. High asthma levels are clustered together spatially and have a value of 0.52. Also note that the scatterplot shows a nice fit without any odd artifacts or outliers.\n\n# Calculate Bivariate Moran's I to examine spatial correlation between air release and asthma prevalence\nmoran_bv = esda.Moran_BV(df['air_release'], df['asthma'], w)\n\n# Display Bivariate Moran's I value and p-value to assess significance\nprint(\"\\nBivariate Moran's I (Air Release vs Asthma):\")\nprint(f\"I: {moran_bv.I}\")\nprint(f\"p-value: {moran_bv.p_sim}\")\n\n# Generate a bivariate Moran scatter plot to visualize spatial correlation between air release and asthma prevalence\nfig, ax = plt.subplots(figsize=(7, 7))\nmoran_scatterplot(moran_bv, ax=ax)\n\n# Set plot limits and aspect ratio for consistent scaling\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\nvmin = min(xlim[0], ylim[0])\nvmax = max(xlim[1], ylim[1])\nax.set_xlim(vmin, vmax)\nax.set_ylim(vmin, vmax)\nax.set_aspect('equal', adjustable='box')\n\n# Set title for the bivariate Moran scatter plot\nax.set_title(\"Bivariate Moran Scatter Plot: Air Release vs Asthma\")\nplt.show()\n\n\nBivariate Moran's I (Air Release vs Asthma):\nI: 0.10452507854493782\np-value: 0.001\n\n\n\n\n\nSimilar to the Pearson analysis, when we analyze their combined spatial dependence the relationship is not as strong; largely due to the high levels of 0 air release values. That said, the relationship is positive and it is statistically significant (p-value 0.001).\nThis could be due to several factors that went unaccounted for in our simple analysis.\n\nOur choice of size/resolution when creating our summed air release raster.\nThe impacts of TRI air releases could be much more widespread than the 5km resolution we specified.\nWe only analyze TRI regulated facilities and their emissions; we do not have data from unregulated facilities\nWe did not account for traffic density or other sources of particulate matter emissions.\nWe did not control for socioeconomic status, preventative healthcare access, and a whole host of other confounding variables.\nThere is no underlying relationship. This is unlikely, based on large amounts of peer reviewed literature, but a possibility nonetheless.\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhat type of data does the CDC PLACES dataset provide?\n\nReal-time air quality measurements\nEstimates of various health indicators at local levels\nHospital admission rates for air pollution-related illnesses\nLocations of healthcare facilities in each county"
  },
  {
    "objectID": "m202-svi-tri-icis-places.html#lesson-3",
    "href": "m202-svi-tri-icis-places.html#lesson-3",
    "title": "Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit",
    "section": "Lesson 3",
    "text": "Lesson 3\nIn this lesson, we explored ….\nLesson 3"
  },
  {
    "objectID": "m202-svi-tri-icis-places.html#conclusion",
    "href": "m202-svi-tri-icis-places.html#conclusion",
    "title": "Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit",
    "section": "Conclusion",
    "text": "Conclusion\nThis lesson provided a comprehensive exploration of four environmental and public health datasets, focusing on the Detroit metropolitan area. By analyzing data from ICIS-AIR, the Toxic Release Inventory (TRI), the Social Vulnerability Index (SVI), and CDC PLACES, we gained insights into the complex relationships between industrial air pollution, social vulnerability, and health outcomes. The integration of these datasets, combined with geospatial analysis techniques, allowed us to identify areas where environmental hazards and social vulnerabilities intersect, highlighting potential environmental justice concerns. This approach demonstrates the power of combining diverse datasets and utilizing geospatial tools to inform policy decisions and target interventions in areas of greatest need."
  },
  {
    "objectID": "m202-svi-tri-icis-places.html#key-learning-points",
    "href": "m202-svi-tri-icis-places.html#key-learning-points",
    "title": "Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit",
    "section": "Key Learning Points",
    "text": "Key Learning Points\nCongratulations! In this lesson you:\n\nAccessed and processed data from multiple EPA and CDC APIs, including ICIS-AIR, TRI, and PLACES datasets\nCreated and manipulated geospatial objects using libraries such as GeoPandas and rasterio\nVisualized point data on maps using Matplotlib and Contextily\nPerformed data cleaning and preprocessing, including handling missing coordinates and outliers\nCreated choropleth maps to visualize health outcomes data at the census tract level\nUtilized Inverse Distance Weighting (IDW) interpolation to create continuous surfaces from point data\nIntegrated multiple datasets to create a composite index (Air Release Vulnerability Index)\nAnalyzed spatial patterns of air pollution, social vulnerability, and health outcomes\nApplied raster math and resampling techniques to align and compare different spatial datasets\nInterpreted results in the context of environmental justice and public health concerns\nPerformed Pearson and Moran’s I correlation analyses"
  },
  {
    "objectID": "m204-grdiv1-pm25.html",
    "href": "m204-grdiv1-pm25.html",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "",
    "text": "In this lesson, you will use NASA socioeconomic and environmental Earthdata available at NASA Socioeconomic Data and Applications Center (SEDAC) to examine air quality by measuring concentrations of particulate matter (PM) data in different international administrative areas. We will focus on countries not meeting international health guidelines, and we will compare the PM concentration between areas with different levels of socioeconomic deprivation, a proxy for poverty, to determine if there is a correlation between exposure to air pollutants and poverty.\nThis lesson walks through the process of calculating and visualizing zonal statistics for a set of countries using spatial raster data, focusing on PM2.5 concentrations, Global Gridded Relative Deprivation Index (GRDI) data for each country these different socioecomnomic areas. We begins by subsetting data by country and iterating over each country to extract relevant zonal statistics like mean, median, and various percentiles forPM2.5 concentrations. These statistics are stored in a GeoDataFrame, which is later used to create a choropleth map that visualizes specific PM2.5 concentration metrics across countries. The lesson includes a detailed analysis of PM2.5 concentrations within different GRDI quartiles for selected countries. This involves clipping the raster data to each country’s geometry, filtering the data based on the GRDI quartiles, and calculating the mean PM2.5 levels for each quartile. The results are then visualized using customized plots to highlight the relationship between air quality and GRDI metrics across the selected countries."
  },
  {
    "objectID": "m204-grdiv1-pm25.html#learning-objectives",
    "href": "m204-grdiv1-pm25.html#learning-objectives",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this lesson, you should be able to:\n\nGain a general understanding of what is particulate matter (PM) in the air and how it impacts human health.\nLearn about global socioeconomic dimensions of deprivation and how they are spatially represented.\nFind statistical thresholds in socioeconomic data.\nPerform zonal statistics to summarize spatial data\nResample spatial data to harmoniza and compare socioeconomic data against environmental data.\nDisplay data on a maps to get a general understanding of the spatial distribution of data.\nSummarize spatial data into table plots to compare how air quality differs in different socioeconomic conditions of international administrative areas."
  },
  {
    "objectID": "m204-grdiv1-pm25.html#introduction",
    "href": "m204-grdiv1-pm25.html#introduction",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Introduction",
    "text": "Introduction\nAs we have learned from previous lessons in this module, air quality is an important factor that if unmitigated can pose significant health risks, particularly to children’s health. In this module, we will use global particulate matter (PM) spatial data and GADM administrative boundaries to determine the average PM2.5 concentrations in each country and determine if they are meeting the World Health Organization’s (WHO’s) air quality guidelines. Furthermore we will use spatial population data to normalize the air pollution concentrations per capita. We will subset 10 countries to analyze how areas with different levels of multidimensional deprivation, a proxy for poverty, compare with regards to PM2.5 concentrations.\n\nBackground\nAir pollution is now recognized as the single biggest environmental threat to human health. Air pollution affects different aspects of health and impacts everyone in low- middle- and high-income countries alike. It is estimated that pollution is responsible for 9 million deaths per year (Fuller et al. 2022). The burden of disease attributable to air pollution is now on a par with other major global health risks such as unhealthy diet and tobacco smoking (World Health Organization 2021, 2022). Air pollution also places risks upon several of the United Nations’ 2023 Sustainable Development Goals (SDGs), including SDG 3 (Good health and well-being), SDG 11 (Sustainable Communities), and SDG 15 (Life on Land), among others (United Nations Department of Economic and Social Affairs 2024). Monitoring air quality is crucial for the monitoring of well being of people around the world and for the advancement of global sustainable development policies that address these environmental risks.\nParticulate matter (PM₂.₅ and PM₁₀) can penetrate deep into the lungs and bloodstream, causing respiratory and cardiovascular issues and are linked to conditions like heart disease and lung cancer (World Health Organization 2022). Benefits from improved air quality include prevention of air pollution-related premature deaths, chronic diseases, damages to ecosystems and crops, as well as the economic benefits for human health from air quality improvement (Calvin et al. 2023). The United Nation’s Intergovernmental Panel on Climate Change’s (IPCC) latest Climate report acknowledges the largest adaptation gaps exist in projects that manage complex dynamics between air quality and climate risks, and that reductions in greenhouse gas (GHG) emissions would lead to improvements in air quality within a few years (Calvin et al. 2023). Progress has been made to improve air quality, particularly in high-income countries, however; 99% of the world’s population is living in places where the World Health Organization’s (WHO’s) air quality guidelines are not being met (World Health Organization 2021, 2022).\nThe WHO Global Air Quality Guidelines, first introduced in 1987, recommends safe levels for six key pollutants—PM₂.₅, PM₁₀, ozone (O₃), nitrogen dioxide (NO₂), sulfur dioxide (SO₂), and carbon monoxide (CO) which represent critical risks to human health globally. Recently updated guidelines respond to new scientific findings demonstrating that even lower concentrations of air pollutants than previously understood can lead to severe health impacts. Table 1 shows the latest Air Quality Guidelines (AQG) for PM2.5.\nWe focus on PM2.5 because we will be working with the SEDAC Global Annual PM2.5 annual data.\nTable 1. WHO Recommended AQG levels and interim targets\n\n\n\n\nPollutant\n\n\nAveraging Time\n\n\nInterim Target\n\n\nAQG Level\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\nPM2.5, µg/m³\n\n\nAnnual\n\n\n35\n\n\n25\n\n\n15\n\n\n10\n\n\n5\n\n\n\n\n24-hour*\n\n\n75\n\n\n50\n\n\n37.5\n\n\n25\n\n\n15\n\n\n\n\n*99th percentile (i.e. 3-4 exceedance days per year).\nInequality, political instability, and increased cost of living are some of the causes preventing significant progress to reduce poverty and meet the SDGs. This continued environmental inequality can also be seen in the significant majority (89%) of premature deaths attributed to air pollution, which occur in low- and middle- income countries (World Health Organization, 2022). In an account of 110 countries, 1.1 billion people (18%) are considered to live in “acute” multidimensional poverty, with half living in Sub-Saharan Africa and a third living in South Asia (UNDP (United Nations Development Programme), 2023). Studies have shown that persons with lower socioeconomic status disproportionately experience higher concentrations of pollutants (Flanagan et al., 2019). The level of poverty experienced by vulnerable communities can compound the risks posed by pollution leading to worsening well being and health outcomes."
  },
  {
    "objectID": "m204-grdiv1-pm25.html#data-collection-and-integration",
    "href": "m204-grdiv1-pm25.html#data-collection-and-integration",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Data Collection and Integration",
    "text": "Data Collection and Integration\nTo determine whether countries are meeting WHO recommended AQG, we can use data of PM2.5 concentrations from the Global (GL) Annual PM2.5 Grids from MODIS, MISR and SeaWiFS Aerosol Optical Depth (AOD), v4.03 (1998 – 2019) dataset, which can can be downloaded from the Socioeconomic Data and Applications Center (SEDAC)(Center For International Earth Science Information Network-CIESIN-Columbia University 2022a). This data can be summarized using GADM.org data, which provides administrative areas of many countries. We can determine the average concentration for the entire country area; however, many areas many not be populated by people, so we will use population data to subset areas where at least one person is living. This will also be used to normalize the data to a per-capita value and provide us with a better representation of the PM2.5 concentrations that people are experiencing.\nonce we determine the top 10 countries with the highest average per capita PM2.5 concentrations, we will use those results to further classify the data into socioeconomic strata in each country. We can compare the PM2.5 per capita concentrations in these different areas to observe the correlation between socioeconomic poverty and air pollution. We employ the Global Gridded Relative Deprivation Index (GRDI), v1 (2010 – 2020) dataset can be downloaded from SEDAC as well (Center For International Earth Science Information Network-CIESIN-Columbia University 2022b).\n\nPreparing Computing Environment and Variables\nImporting python packages required: The python packages are required for the remainder of the lesson. Please review the Python documentation of these packages for detailed information.\n\nimport xarray as xr\nimport rioxarray\nimport rasterstats\nfrom rasterio.enums import Resampling\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport geopandas as gpd\nimport pygadm\nimport plotly.graph_objects as go \n\nLoad the GRDIv1 and PM2.5 data from local sources:\n\n# Load rasters\ngrdi_path = r\"Z:\\Sedac\\GRDI\\data\\povmap-grdi-v1-geotiff\\final data\\povmap-grdi-v1.tif\"\npm25_path = r\"data\\sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-v4-gl-03-2019-geotiff\\sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-v4-gl-03-2019.tif\"\n\nUsing the package rasterio to load the data into memory. This allows us to read the data and use it for processing.\n\n# Open the input and reference rasters\ngrdi_raster = rioxarray.open_rasterio(grdi_path, mask_and_scale=True)\npm25_raster = rioxarray.open_rasterio(pm25_path, mask_and_scale=True)\n\n\n\nMatching Data Points using Bilinear Resample\nThe GRDI raster and PM2.5 rasters are incompatible in resolution. One method of harmonizing data is by using the Resampling tool with a bilinear method. In this case, we reduce, or coarsen, the resolution of the GRDI raster to match the PM2.5 raster.\n\n# Resample the input raster to match the reference raster\ngrdi_raster = grdi_raster.rio.reproject_match(pm25_raster,resampling=Resampling.bilinear)"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#previewing-spatial-data-in-a-plot",
    "href": "m204-grdiv1-pm25.html#previewing-spatial-data-in-a-plot",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Previewing Spatial Data in a Plot",
    "text": "Previewing Spatial Data in a Plot\nOnce the data rasters have been resampled to the same resolution, we can quickly plot them to view the data we are working with on a map.\n\n# Plotting the rasters\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 20))\n\n# Plot the original GRDI raster in the first subplot\nim1 = ax1.imshow(grdi_raster.values[0], cmap='viridis', interpolation='nearest')\nax1.set_title('Original GRDI Raster')\nfig.colorbar(im1, ax=ax1, orientation='horizontal', label='GRDI Values')\n\n# Plot the PM2.5 raster in the second subplot\nim2 = ax2.imshow(pm25_raster.values[0], cmap='hot', interpolation='nearest')\nax2.set_title('PM2.5 Raster')\nfig.colorbar(im2, ax=ax2, orientation='horizontal', label='PM2.5 Values')\n\n\n# Show the plots\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#working-with-administrative-data",
    "href": "m204-grdiv1-pm25.html#working-with-administrative-data",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Working with administrative Data",
    "text": "Working with administrative Data\npygadm is a package that has international administrative units from levels 0 to 2. We can search the available countries by listing the Names.\n\ncountry_table = gpd.GeoDataFrame(pygadm.Names())\nlen(country_table)\n\n263\n\n\nSome available areas with a unique GID_0 code share Names; therefore we drop the rows that contain digits.\n\ncountry_table = country_table[~country_table['GID_0'].str.contains('\\d', na=False)]\nlen(country_table)\n\n254\n\n\n\nSubset Data From a Table\nDoing Zonal statistics for more than 200 countries may take a while, therefore, we can subset the data randomly with the .sample() method.\n\ncountry_sample = country_table.sample(n=30)\ncountry_sample\n\n\n\n\n\n  \n    \n      \n      NAME_0\n      GID_0\n    \n  \n  \n    \n      97\n      Guam\n      GUM\n    \n    \n      177\n      Oman\n      OMN\n    \n    \n      157\n      Mozambique\n      MOZ\n    \n    \n      171\n      Niue\n      NIU\n    \n    \n      116\n      Iceland\n      ISL\n    \n    \n      188\n      North Korea\n      PRK\n    \n    \n      234\n      Tuvalu\n      TUV\n    \n    \n      247\n      Virgin Islands, U.S.\n      VIR\n    \n    \n      77\n      Falkland Islands\n      FLK\n    \n    \n      14\n      Australia\n      AUS\n    \n    \n      262\n      Zimbabwe\n      ZWE\n    \n    \n      244\n      Saint Vincent and the Grenadines\n      VCT\n    \n    \n      219\n      Sint Maarten\n      SXM\n    \n    \n      232\n      Tunisia\n      TUN\n    \n    \n      214\n      Suriname\n      SUR\n    \n    \n      261\n      Northern Cyprus\n      ZNC\n    \n    \n      205\n      Solomon Islands\n      SLB\n    \n    \n      89\n      Gambia\n      GMB\n    \n    \n      95\n      Guatemala\n      GTM\n    \n    \n      183\n      Philippines\n      PHL\n    \n    \n      241\n      United States\n      USA\n    \n    \n      47\n      Côte d'Ivoire\n      CIV\n    \n    \n      9\n      Armenia\n      ARM\n    \n    \n      128\n      Saint Kitts and Nevis\n      KNA\n    \n    \n      20\n      Bonaire, Sint Eustatius and Saba\n      BES\n    \n    \n      240\n      Uruguay\n      URY\n    \n    \n      6\n      Andorra\n      AND\n    \n    \n      17\n      Burundi\n      BDI\n    \n    \n      99\n      Heard Island and McDonald Island\n      HMD\n    \n    \n      231\n      Trinidad and Tobago\n      TTO"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#zonal-statistics-for-each-administrative-area",
    "href": "m204-grdiv1-pm25.html#zonal-statistics-for-each-administrative-area",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Zonal Statistics for Each Administrative Area",
    "text": "Zonal Statistics for Each Administrative Area\nrasterstats has a funcion zonal_stats() which allows you to use vectors to summarize raster data. We summarize GRDIv1 data to calculate the following statistics: count, minimum, mean, max, median, standard deviation, range, and percentiles 20, 40, 60, and 80.\nWe can define a custom function that can allow us to use the zonal statistics process multiple times. A custom function can be created using the def FUNCTION_NAME(PARAMETER1, PARAMETER2): fuction to define what the fucntion will do.\n\ndef calculate_country_stats(country_sample, grdi_raster, pm25_raster=None):\n    \"\"\"\n    Calculate statistics for each country in the sample.\n\n    Parameters:\n    - country_sample: A pandas DataFrame containing country information with 'NAME_0' and 'GID_0' columns, in this case the country_table.\n    - grdi_raster: A raster object with which to perform the zonal statistics.\n    - pm25_raster: (Optional) A raster object for PM2.5 data. If provided, statistics will also be calculated for this raster.\n\n    Returns:\n    - stats_results: A GeoDataFrame containing the statistics for each country.\n    \"\"\"\n    stats_results = gpd.GeoDataFrame()\n\n    for index, row in country_sample.iloc[:].iterrows():\n        country = row['NAME_0']\n        country_GID = row['GID_0']\n        try:\n            country_poly = pygadm.Items(admin=country_GID, content_level=0)\n            if isinstance(country_poly, gpd.GeoDataFrame):\n                country_geometry = country_poly.geometry.iloc[0]  # Ensure single geometry\n            else:\n                country_geometry = country_poly\n        except Exception as e:\n            print(country, \"skipped due to error:\", e)\n            continue\n\n        # Create a mask for the polygons and perform zonal statistics on GRDI raster\n        grdi_country_zs = rasterstats.zonal_stats(\n            country_geometry, grdi_raster.values[0], \n            affine=grdi_raster.rio.transform(), \n            stats=\" min mean max percentile_20 percentile_40 percentile_60 percentile_80\",\n            nodata_value=grdi_raster.rio.nodata\n        )\n\n        # Uncomment and update the following lines if you want to include PM2.5 statistics\n        if pm25_raster is not None:\n            pm25_country_zs = rasterstats.zonal_stats(\n                country_geometry, pm25_raster.values[0], \n                affine=pm25_raster.rio.transform(), \n                stats=\"mean\",\n                nodata_value=pm25_raster.rio.nodata\n                )\n\n        # Extract statistics into a dictionary\n        country_stats = {\n            'Country_Name': country,\n            'Country_GID' : country_GID,\n            # 'GRDI_Count': grdi_country_zs[0]['count'],\n            'GRDI_Min': grdi_country_zs[0]['min'],\n            'GRDI_Mean': grdi_country_zs[0]['mean'],\n            'GRDI_Max': grdi_country_zs[0]['max'],\n            # 'GRDI_Median': grdi_country_zs[0]['median'],\n            # 'GRDI_Std': grdi_country_zs[0]['std'],\n            # 'GRDI_Range': grdi_country_zs[0]['range'],\n            'GRDI_P20': grdi_country_zs[0]['percentile_20'],\n            'GRDI_P40': grdi_country_zs[0]['percentile_40'],\n            'GRDI_P60': grdi_country_zs[0]['percentile_60'],\n            'GRDI_P80': grdi_country_zs[0]['percentile_80']#,\n            # 'geometry' : country_poly['geometry'].iloc[0]\n        }\n\n        # If PM2.5 statistics are calculated, add them to the dictionary\n        if pm25_raster is not None:\n            country_stats.update({\n                'PM25_Mean': pm25_country_zs[0]['mean']\n            })\n        \n        # Filter out None values from `country_stats`\n        country_stats = {k: v for k, v in country_stats.items() if pd.notnull(v)}\n\n        country_stats_gdf = gpd.GeoDataFrame([country_stats], geometry=[country_geometry])\n\n        # Append to results if country_stats_gdf has non-empty columns\n        if not country_stats_gdf.empty and country_stats_gdf.notna().any().any():\n            stats_results = pd.concat([stats_results, country_stats_gdf], ignore_index=True)\n\n    # Drop any rows in `stats_results` that have NaN in any cell\n    stats_results = stats_results.dropna()\n    # Reset the index\n    stats_results = stats_results.reset_index(drop=True)\n    return stats_results\n\nLet’s put our defined function to use:\n\nstats_results = calculate_country_stats(country_sample, grdi_raster, pm25_raster)\n\nLet’s use the .head() method from Pandas to check the top of our table\n\nstats_results.head()\n\n\n\n\n\n  \n    \n      \n      Country_Name\n      Country_GID\n      GRDI_Min\n      GRDI_Mean\n      GRDI_Max\n      GRDI_P20\n      GRDI_P40\n      GRDI_P60\n      GRDI_P80\n      PM25_Mean\n      geometry\n    \n  \n  \n    \n      0\n      Guam\n      GUM\n      11.949570\n      50.121112\n      74.832214\n      38.143613\n      46.619795\n      55.826491\n      63.909902\n      6.458084\n      MULTIPOLYGON (((144.6448 13.2351, 144.6465 13....\n    \n    \n      1\n      Oman\n      OMN\n      5.814715\n      58.435711\n      75.041962\n      48.413793\n      61.939667\n      66.912010\n      68.876038\n      55.710693\n      MULTIPOLYGON (((54.806 16.9438, 54.8076 16.942...\n    \n    \n      2\n      Mozambique\n      MOZ\n      25.492849\n      93.008894\n      99.251961\n      90.699451\n      93.024376\n      95.013539\n      96.892366\n      8.546113\n      MULTIPOLYGON (((36.5176 -18.5138, 36.5149 -18....\n    \n    \n      3\n      Iceland\n      ISL\n      5.580927\n      62.195393\n      68.446869\n      61.777039\n      63.289841\n      63.712660\n      64.202600\n      3.619790\n      MULTIPOLYGON (((-20.3245 63.3891, -20.3214 63....\n    \n    \n      4\n      North Korea\n      PRK\n      17.416229\n      72.583755\n      75.944130\n      72.921800\n      73.774530\n      74.046808\n      74.252713\n      19.556963\n      MULTIPOLYGON (((129.5504 40.6474, 129.5513 40....\n    \n  \n\n\n\n\nSo far, we have collected the overall average PM2.5 for the countries. Based on the WHO Air Quality guidelines for PM2.5, we subset the dataframe to keep only countries with the mean PM2.5 value greater than or equal to 5.0:\n\nstats_results = stats_results[stats_results[\"PM25_Mean\"] >= 5.0]\nstats_results\n\n\n\n\n\n  \n    \n      \n      Country_Name\n      Country_GID\n      GRDI_Min\n      GRDI_Mean\n      GRDI_Max\n      GRDI_P20\n      GRDI_P40\n      GRDI_P60\n      GRDI_P80\n      PM25_Mean\n      geometry\n    \n  \n  \n    \n      0\n      Guam\n      GUM\n      11.949570\n      50.121112\n      74.832214\n      38.143613\n      46.619795\n      55.826491\n      63.909902\n      6.458084\n      MULTIPOLYGON (((144.6448 13.2351, 144.6465 13....\n    \n    \n      1\n      Oman\n      OMN\n      5.814715\n      58.435711\n      75.041962\n      48.413793\n      61.939667\n      66.912010\n      68.876038\n      55.710693\n      MULTIPOLYGON (((54.806 16.9438, 54.8076 16.942...\n    \n    \n      2\n      Mozambique\n      MOZ\n      25.492849\n      93.008894\n      99.251961\n      90.699451\n      93.024376\n      95.013539\n      96.892366\n      8.546113\n      MULTIPOLYGON (((36.5176 -18.5138, 36.5149 -18....\n    \n    \n      4\n      North Korea\n      PRK\n      17.416229\n      72.583755\n      75.944130\n      72.921800\n      73.774530\n      74.046808\n      74.252713\n      19.556963\n      MULTIPOLYGON (((129.5504 40.6474, 129.5513 40....\n    \n    \n      7\n      Australia\n      AUS\n      0.045566\n      55.153753\n      80.935013\n      44.820724\n      60.302293\n      62.995925\n      64.298001\n      6.095914\n      MULTIPOLYGON (((147.3607 -42.9324, 147.3562 -4...\n    \n    \n      8\n      Zimbabwe\n      ZWE\n      24.202816\n      89.134846\n      97.393387\n      87.505156\n      88.919373\n      90.362291\n      91.690747\n      8.106061\n      MULTIPOLYGON (((32.7043 -18.9602, 32.7081 -18....\n    \n    \n      9\n      Saint Vincent and the Grenadines\n      VCT\n      25.406565\n      56.915948\n      72.531509\n      47.869689\n      55.283461\n      61.859246\n      68.640604\n      5.773275\n      MULTIPOLYGON (((-61.4357 12.581, -61.4351 12.5...\n    \n    \n      10\n      Tunisia\n      TUN\n      9.235206\n      64.080152\n      77.956619\n      61.405814\n      67.805656\n      69.563014\n      70.437521\n      20.309617\n      MULTIPOLYGON (((8.3559 32.5176, 8.3233 32.823,...\n    \n    \n      11\n      Suriname\n      SUR\n      11.469023\n      70.097854\n      89.118263\n      60.473251\n      70.807611\n      76.486787\n      85.623878\n      15.385801\n      MULTIPOLYGON (((-55.9868 2.5013, -55.9839 2.49...\n    \n    \n      12\n      Northern Cyprus\n      ZNC\n      12.275703\n      52.244425\n      69.428528\n      39.145607\n      52.888069\n      59.829361\n      63.756794\n      16.152514\n      MULTIPOLYGON (((33.1077 35.1599, 33.0958 35.16...\n    \n    \n      14\n      Gambia\n      GMB\n      35.556244\n      85.304798\n      95.220772\n      81.411942\n      87.196838\n      89.766960\n      91.870148\n      44.006192\n      MULTIPOLYGON (((-16.6907 13.1653, -16.6897 13....\n    \n    \n      15\n      Guatemala\n      GTM\n      22.353275\n      76.311224\n      86.836212\n      72.367767\n      78.173495\n      80.656683\n      81.581622\n      14.661588\n      MULTIPOLYGON (((-91.0107 13.9143, -91.0551 13....\n    \n    \n      16\n      Philippines\n      PHL\n      6.626353\n      72.399834\n      91.909012\n      68.165047\n      73.975517\n      76.879448\n      79.703705\n      9.559131\n      MULTIPOLYGON (((119.4706 4.5911, 119.4689 4.58...\n    \n    \n      17\n      United States\n      USA\n      0.310731\n      54.860012\n      82.220383\n      50.243195\n      57.605118\n      59.765453\n      62.861000\n      6.659330\n      MULTIPOLYGON (((-155.8764 20.0956, -155.88 20....\n    \n    \n      18\n      Côte d'Ivoire\n      CIV\n      28.918406\n      85.756427\n      98.244072\n      83.640506\n      87.984097\n      89.342270\n      92.177962\n      44.198808\n      MULTIPOLYGON (((-4.2196 5.2199, -4.5435 5.1879...\n    \n    \n      19\n      Armenia\n      ARM\n      6.944193\n      59.871617\n      73.707901\n      52.692970\n      61.288190\n      66.743028\n      69.004848\n      15.958348\n      MULTIPOLYGON (((45.8319 39.8311, 45.8448 39.82...\n    \n    \n      22\n      Burundi\n      BDI\n      31.142191\n      89.535357\n      98.460892\n      86.663841\n      89.371594\n      91.512729\n      93.262587\n      19.887763\n      MULTIPOLYGON (((30.0452 -4.2568, 30.0473 -4.26...\n    \n    \n      24\n      Trinidad and Tobago\n      TTO\n      9.480251\n      42.446880\n      70.771553\n      24.782356\n      37.650338\n      50.780862\n      59.724503\n      5.107747\n      MULTIPOLYGON (((-61.5061 10.0715, -61.5146 10....\n    \n  \n\n\n\n\n\n# Assuming the column with names is called 'Name'\n# Define the bins and labels\nbins = [5, 10, 15, 25, 30, float('inf')]\nlabels = [\"5-10\", \"10-15\", \"15-25\", \"25-30\", \">30\"]\n\n# Use .loc to add the new column with binned ranges, avoiding SettingWithCopyWarning\nstats_results.loc[:, 'PM25_range'] = pd.cut(stats_results[\"PM25_Mean\"], bins=bins, labels=labels, right=False).astype(str)\n\n# Group by the 'PM25_range' and list names in each range\ngrouped = stats_results.groupby('PM25_range', observed=True)['Country_Name'].agg(['count', list]).reset_index()\ngrouped.columns = ['PM25_range', 'Count', 'Names']  # Rename columns for clarity\n\n# Set display option to prevent truncation of long lists in the output\npd.set_option('display.max_colwidth', None)\n\nprint(grouped)\n\n# Reset Pandas to default display options\npd.reset_option(\"display.max_colwidth\")\n\n  PM25_range  Count  \\\n0      10-15      1   \n1      15-25      6   \n2       5-10      8   \n3        >30      3   \n\n                                                                                                                        Names  \n0                                                                                                                 [Guatemala]  \n1                                                         [North Korea, Tunisia, Suriname, Northern Cyprus, Armenia, Burundi]  \n2  [Guam, Mozambique, Australia, Zimbabwe, Saint Vincent and the Grenadines, Philippines, United States, Trinidad and Tobago]  \n3                                                                                               [Oman, Gambia, Côte d'Ivoire]  \n\n\nFrom the table above, we can see how many and which countries are within the WHO AQG, or are within interim stages of air quality improvement.\nBelow, choose an attribute, or column, to display it in a map plot. In this case, I’m choosing the PM2.5 Mean.\n\ncolumn_chosen = 'PM25_Mean' # 'GRDI_Max' #GRDI_Max, GRDI_Min, GRDI_Median\n# Plotting\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nstats_results.plot(column=column_chosen, ax=ax, legend=True,\n    legend_kwds={'label': f\"{column_chosen} per country.\",\n                      'orientation': \"horizontal\"})\nax.set_title(f'Choropleth Map Showing {column_chosen} per country')\nax.set_axis_off()  # Turn off the axis numbers and ticks\nplt.show()\n\n\n\n\n\nSelecting Data by Column\nStart my creating a list of countries that you are interested in to Subset data from the DataFrame that match the values in the NAME_0 column. The .isin() mehthod checks each element in the DataFrame’s column for the item present in the list and returns matching rows.\n\n# selected_countries = [\"Algeria\", \"Somalia\", \"Colombia\", \"Timor Leste\", \"Finland\", \"Nicaragua\", \"United Kingdom\", \"Mali\"]\n# selected_countries = [\"Anguilla\", \"Armenia\", \"Angola\", \"Argentina\", \"Albania\", \"United Arab Emirates\", \"American Samoa\", \"Australia\" ]\nselected_countries = [\"Algeria\", \"Somalia\", \"Colombia\", \"Timor Leste\", \"Finland\", \"Nicaragua\", \"United Kingdom\", \"Mali\", \"Armenia\", \"Argentina\",  \"Albania\", \"United Arab Emirates\", \"Indonesia\", \"Qatar\"]\n\n#use the list above to subset the country_table DataFrame by the column NAME_0 \nselected_countries = country_table[country_table['NAME_0'].isin(selected_countries)]\n\n\n\nUsing a Defined Custom Function\nRecalling the defined fucntion calculate_country_stats, we can use our selected_countries list, and the GRDI and PM2.5 rasters, to create a new table of zonal statistics.\n\nstats_results = calculate_country_stats(selected_countries, grdi_raster, pm25_raster)\n\nShow the head of the table again:\n\nstats_results.head()\n\n\n\n\n\n  \n    \n      \n      Country_Name\n      Country_GID\n      GRDI_Min\n      GRDI_Mean\n      GRDI_Max\n      GRDI_P20\n      GRDI_P40\n      GRDI_P60\n      GRDI_P80\n      PM25_Mean\n      geometry\n    \n  \n  \n    \n      0\n      Albania\n      ALB\n      8.272310\n      61.513866\n      75.395561\n      55.861691\n      64.445587\n      67.104332\n      68.589424\n      15.063068\n      MULTIPOLYGON (((20.0541 39.6917, 20.0389 39.69...\n    \n    \n      1\n      United Arab Emirates\n      ARE\n      5.732072\n      42.347647\n      67.470955\n      24.688158\n      38.623690\n      50.710212\n      61.909930\n      49.507895\n      MULTIPOLYGON (((54.1541 22.7548, 53.3313 22.85...\n    \n    \n      2\n      Argentina\n      ARG\n      7.572341\n      66.341158\n      81.701645\n      66.774644\n      68.278297\n      69.335506\n      71.098463\n      7.083989\n      MULTIPOLYGON (((-66.5458 -55.061, -66.5486 -55...\n    \n    \n      3\n      Armenia\n      ARM\n      6.944193\n      59.871617\n      73.707901\n      52.692970\n      61.288190\n      66.743028\n      69.004848\n      15.958348\n      MULTIPOLYGON (((45.8319 39.8311, 45.8448 39.82...\n    \n    \n      4\n      Colombia\n      COL\n      11.956444\n      71.523674\n      84.922409\n      69.966937\n      72.190059\n      74.405400\n      76.502792\n      22.305299\n      MULTIPOLYGON (((-77.491 4.1451, -77.4985 4.140...\n    \n  \n\n\n\n\nPlot the map again choosing a column to plot:\n\ncolumn_chosen = 'PM25_Mean'  #'GRDI_Max' #GRDI_Max, GRDI_Min, GRDI_Median\nstats_results.plot(column=column_chosen, legend=True)\nplt.show()"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#defining-a-funtion",
    "href": "m204-grdiv1-pm25.html#defining-a-funtion",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Defining a Funtion",
    "text": "Defining a Funtion\nWe can create a custom function that can allow us to use the zonal statistics process multiple times. A custom function can be created using the def FUNCTION_NAME(PARAMETER1, PARAMETER2): fuction to define what the fucntion will do.\n\ndef calculate_country_stats(country_sample, grdi_raster, pm25_raster=None):\n    \"\"\"\n    Calculate statistics for each country in the sample.\n\n    Parameters:\n    - country_sample: A pandas DataFrame containing country information with 'NAME_0' and 'GID_0' columns, in this case the country_table.\n    - grdi_raster: A raster object with which to perform the zonal statistics.\n    - pm25_raster: (Optional) A raster object for PM2.5 data. If provided, statistics will also be calculated for this raster.\n\n    Returns:\n    - stats_results: A GeoDataFrame containing the statistics for each country.\n    \"\"\"\n    stats_results = gpd.GeoDataFrame()\n\n    for index, row in country_sample.iloc[:].iterrows():\n        country = row['NAME_0']\n        country_GID = row['GID_0']\n        try:\n            country_poly = pygadm.Items(admin=country_GID, content_level=0)\n        except Exception as e:\n            print(country, \"skipped due to error:\", e)\n            continue\n\n        # Create a mask for the polygons and perform zonal statistics on GRDI raster\n        grdi_country_zs = rasterstats.zonal_stats(\n            country_poly, grdi_raster.values[0], \n            affine=grdi_raster.rio.transform(), \n            stats=\"count min mean max median std range percentile_20 percentile_40 percentile_60 percentile_80\"\n        )\n\n        # Uncomment and update the following lines if you want to include PM2.5 statistics\n        # if pm25_raster is not None:\n        #     pm25_country_zs = rasterstats.zonal_stats(\n        #         country_poly, pm25_raster.values[0], \n        #         affine=pm25_raster.rio.transform(), \n        #         stats=\"count min mean max median std range percentile_20 percentile_40 percentile_60 percentile_80\", \n        #         nodata=pm25_raster.nodata\n        #     )\n\n        # Extract statistics into a dictionary\n        country_stats = {\n            'Country_Name': country,\n            'Country_GID' : country_GID,\n            'GRDI_Count': grdi_country_zs[0]['count'],\n            'GRDI_Min': grdi_country_zs[0]['min'],\n            'GRDI_Mean': grdi_country_zs[0]['mean'],\n            'GRDI_Max': grdi_country_zs[0]['max'],\n            'GRDI_Median': grdi_country_zs[0]['median'],\n            'GRDI_Std': grdi_country_zs[0]['std'],\n            'GRDI_Range': grdi_country_zs[0]['range'],\n            'GRDI_P20': grdi_country_zs[0]['percentile_20'],\n            'GRDI_P40': grdi_country_zs[0]['percentile_40'],\n            'GRDI_P60': grdi_country_zs[0]['percentile_60'],\n            'GRDI_P80': grdi_country_zs[0]['percentile_80'],\n            'geometry' : country_poly['geometry'].iloc[0]\n        }\n\n        # If PM2.5 statistics are calculated, add them to the dictionary\n        # if pm25_raster is not None:\n        #     country_stats.update({\n        #         'PM25_Count': pm25_country_zs[0]['count'],\n        #         'PM25_Min': pm25_country_zs[0]['min'],\n        #         'PM25_Mean': pm25_country_zs[0]['mean'],\n        #         'PM25_Max': pm25_country_zs[0]['max'],\n        #         'PM25_Median': pm25_country_zs[0]['median'],\n        #         'PM25_Std': pm25_country_zs[0]['std'],\n        #         'PM25_Range': pm25_country_zs[0]['range'],\n        #         'PM25_P20': pm25_country_zs[0]['percentile_20'],\n        #         'PM25_P40': pm25_country_zs[0]['percentile_40'],\n        #         'PM25_P60': pm25_country_zs[0]['percentile_60'],\n        #         'PM25_P80': pm25_country_zs[0]['percentile_80'],\n        #     })\n\n        country_stats_gdf = gpd.GeoDataFrame([country_stats], geometry='geometry')\n        stats_results = pd.concat([stats_results, country_stats_gdf], ignore_index=True)\n\n    return stats_results\n\nFrom the table above, we can choose an attribute, or column, to display it in a map plot. In this case, I’m choosing the GRDI Max\n\ncolumn_chosen = 'GRDI_Max' #GRDI_Max, GRDI_Min, GRDI_Median\n# Plotting\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nstats_results.plot(column=column_chosen, ax=ax, legend=True,\n    legend_kwds={'label': f\"{column_chosen} per country.\",\n                      'orientation': \"horizontal\"})\nax.set_title('Choropleth Map Showing GRDI Mean per country')\nax.set_axis_off()  # Turn off the axis numbers and ticks\nplt.show()\n\n\n\n\n\nSelecting Data by Column\nStart my creating a list of countries that you are interested in to Subset data from the DataFrame that match the values in the NAME_0 column. The .isin() mehthod checks each element in the DataFrame’s column for the item present in the list and returns matching rows.\n\n# selected_countries = [\"Algeria\", \"Somalia\", \"Colombia\", \"Timor Leste\", \"Finland\", \"Nicaragua\", \"United Kingdom\", \"Mali\"]\n# selected_countries = [\"Anguilla\", \"Armenia\", \"Angola\", \"Argentina\", \"Albania\", \"United Arab Emirates\", \"American Samoa\", \"Australia\" ]\nselected_countries = [\"Algeria\", \"Somalia\", \"Colombia\", \"Timor Leste\", \"Finland\", \"Nicaragua\", \"United Kingdom\", \"Mali\", \"Armenia\", \"Argentina\",  \"Albania\", \"United Arab Emirates\", \"Indonesia\", \"Qatar\"]\n\n#use the list above to subset the country_table DataFrame by the column NAME_0 \nselected_countries = country_table[country_table['NAME_0'].isin(selected_countries)]\n\n\n\nUsing a Defined Custom Function\nRecalling the defined fucntion calculate_country_stats, we can use our selected_countries list, and the GRDI and PM2.5 rasters, to create a new table of zonal statistics.\n\nstats_results = calculate_country_stats(selected_countries, grdi_raster)\n\nShow the head of the table again:\n\nstats_results.head()\n\n\n\n\n\n  \n    \n      \n      Country_Name\n      Country_GID\n      GRDI_Count\n      GRDI_Min\n      GRDI_Mean\n      GRDI_Max\n      GRDI_Median\n      GRDI_Std\n      GRDI_Range\n      GRDI_P20\n      GRDI_P40\n      GRDI_P60\n      GRDI_P80\n      geometry\n    \n  \n  \n    \n      0\n      Albania\n      ALB\n      19076\n      8.272310\n      61.513866\n      75.395561\n      66.220139\n      11.644754\n      67.123251\n      55.861691\n      64.445587\n      67.104332\n      68.589424\n      MULTIPOLYGON (((20.0541 39.6917, 20.0389 39.69...\n    \n    \n      1\n      United Arab Emirates\n      ARE\n      18229\n      5.732072\n      42.347647\n      67.470955\n      45.034630\n      17.949307\n      61.738883\n      24.688160\n      38.623692\n      50.710217\n      61.909931\n      MULTIPOLYGON (((54.1541 22.7548, 53.3313 22.85...\n    \n    \n      2\n      Argentina\n      ARG\n      474297\n      7.572341\n      66.341158\n      81.701645\n      68.789696\n      10.416561\n      74.129304\n      66.774643\n      68.278297\n      69.335510\n      71.098465\n      MULTIPOLYGON (((-66.5458 -55.061, -66.5486 -55...\n    \n    \n      3\n      Armenia\n      ARM\n      9108\n      6.944193\n      59.871617\n      73.707901\n      64.272858\n      12.315193\n      66.763708\n      52.692970\n      61.288189\n      66.743027\n      69.004845\n      MULTIPOLYGON (((45.8319 39.8311, 45.8448 39.82...\n    \n    \n      4\n      Colombia\n      COL\n      223557\n      11.956444\n      71.523674\n      84.922409\n      73.350586\n      8.762370\n      72.965965\n      69.966934\n      72.190056\n      74.405403\n      76.502792\n      MULTIPOLYGON (((-77.491 4.1451, -77.4985 4.140...\n    \n  \n\n\n\n\nPlot the map again choosing a column to plot:\n\ncolumn_chosen = 'GRDI_Max' #GRDI_Max, GRDI_Min, GRDI_Median\nstats_results.plot(column=column_chosen, legend=True)\nplt.show()"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#creating-a-table-with-results",
    "href": "m204-grdiv1-pm25.html#creating-a-table-with-results",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Creating a Table with Results",
    "text": "Creating a Table with Results\nWe can create a list of tuples that we can use to refer to the statistical values, and the name, color, and symbol we want to assign. In this case, we are using the GRDI zonal statistics of each country we selected that include the Mean, Minimum, Maximum, and interquartiles.\n\n# List of GRDI values and their corresponding properties\n#column, value name, color, symbol\ngrdi_data = [\n    ('GRDI_Mean', 'Mean', 'orange', 'diamond'),\n    ('GRDI_Min', 'Min', 'gray', '152'),\n    ('GRDI_Max', 'Max', 'gray', '151'),\n    ('GRDI_P20', 'Q20', 'blue', '142'),\n    ('GRDI_P40', 'Q40', 'purple', '142'),\n    ('GRDI_P60', 'Q60', 'green', '142'),\n    ('GRDI_P80', 'Q80', 'red', '142')\n]\n\nWe can create a figure to display the data based on the names colors and symbols we selected.\n\n# Create a figure\nfig = go.Figure()\n\n# Add traces to the figure based on the data\nfor col, name, color, symbol in grdi_data:\n    fig.add_trace(go.Scatter(\n        x=stats_results[col],\n        y=stats_results['Country_Name'],\n        mode='markers',\n        name=name,\n        marker=dict(color=color, size=10, symbol=symbol)\n    ))\n\n# Customize layout\nfig.update_layout(\n    title='GRDI Statistics by Country',\n    xaxis_title='GRDI Values',\n    yaxis_title='Country Name',\n    yaxis=dict(tickmode='linear'),\n    legend_title='Statistics',\n    yaxis_type='category',\n    xaxis=dict(tickvals=[0, 20, 40, 60, 80, 100])\n)\n\n# Show plot\nfig.show()"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#summarizing-pm2.5-values-by-socioeconomic-deprivation",
    "href": "m204-grdiv1-pm25.html#summarizing-pm2.5-values-by-socioeconomic-deprivation",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Summarizing PM2.5 Values by Socioeconomic Deprivation",
    "text": "Summarizing PM2.5 Values by Socioeconomic Deprivation\nConsidering the GRDI quartile values as a level of socieoeconomic deprivation within each country, we can use the stats_results GeoDataFrame, the GRDI raster, and the PM2.5 raster to calculate the Mean PM.25 value within each of those areas in each country. This can describe how the air quality for different socioeconomic strata compare within the country, as well as against other countries.\nThe results will be added to the stats_results with the corresponting columns.\n\n# iterate through the stats_results table rows\nfor index, row in stats_results.iloc[:].iterrows():\n    #isolate each country's respective row\n    row_df = gpd.GeoDataFrame([row], geometry='geometry').reset_index(drop=True)\n    print(row_df.loc[0,'Country_GID'])\n    try:\n        #use rioxarray to clip the GRDI and PM2.5 rasters by the geometry of the respective country.\n        grdi_country = grdi_raster.rio.clip(row_df.geometry, grdi_raster.rio.crs)\n        pm25_country = pm25_raster.rio.clip(row_df.geometry, grdi_raster.rio.crs)\n    except:\n        print('Error in clip')\n        continue\n\n    #Applying squeeze() to this array removes the singleton dimension, reducing it to a 2D array with dimensions (rows, columns)\n    grdi_country= grdi_country.squeeze()\n    pm25_country= pm25_country.squeeze()\n\n\n    # Subset the GRDI raster where values fall between each GRDI quintiles\n    grdi_countryQ1 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_Min']) & (grdi_country <= row_df.loc[0, 'GRDI_P20']))\n    grdi_countryQ2 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P20']) & (grdi_country <= row_df.loc[0, 'GRDI_P40']))\n    grdi_countryQ3 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P40']) & (grdi_country <= row_df.loc[0, 'GRDI_P60']))\n    grdi_countryQ4 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P60']) & (grdi_country <= row_df.loc[0, 'GRDI_P80']))\n    grdi_countryQ5 = grdi_country.where((grdi_country >= row_df.loc[0, 'GRDI_P80']) & (grdi_country <= row_df.loc[0, 'GRDI_Max']))\n\n\n    # Mask the PM2.5 raster using the above GRDI quartile rasters, keeping only the cells that intersect\n    pm25_countryQ1 = pm25_country.where(grdi_countryQ1.notnull())\n    pm25_countryQ2 = pm25_country.where(grdi_countryQ2.notnull())\n    pm25_countryQ3 = pm25_country.where(grdi_countryQ3.notnull())\n    pm25_countryQ4 = pm25_country.where(grdi_countryQ4.notnull())\n    pm25_countryQ5 = pm25_country.where(grdi_countryQ5.notnull())\n\n    #Find the mean value of of the intersected PM2.5 rasters in each quartile\n    pm25_countryQ1v = pm25_countryQ1.mean().item()\n    pm25_countryQ2v = pm25_countryQ2.mean().item()\n    pm25_countryQ3v = pm25_countryQ3.mean().item()\n    pm25_countryQ4v = pm25_countryQ4.mean().item()\n    pm25_countryQ5v = pm25_countryQ5.mean().item()\n\n    #add the resuts to the stats_results table in the respective column\n    stats_results.at[index, 'PM25_Q1'] = pm25_countryQ1v\n    stats_results.at[index, 'PM25_Q2'] = pm25_countryQ2v\n    stats_results.at[index, 'PM25_Q3'] = pm25_countryQ3v\n    stats_results.at[index, 'PM25_Q4'] = pm25_countryQ4v\n    stats_results.at[index, 'PM25_Q5'] = pm25_countryQ5v\n\nALB\n\n\nARE\n\n\nARG\n\n\nARM\n\n\nCOL\n\n\nDZA\n\n\nFIN\n\n\nGBR\n\n\nIDN\n\n\nMLI\n\n\nNIC\n\n\nQAT\n\n\nSOM"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#plot-results-of-mean-pm2.5-in-socieceonomic-deprivation-quartiles-per-country",
    "href": "m204-grdiv1-pm25.html#plot-results-of-mean-pm2.5-in-socieceonomic-deprivation-quartiles-per-country",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Plot Results of Mean PM2.5 in Socieceonomic Deprivation Quartiles per country",
    "text": "Plot Results of Mean PM2.5 in Socieceonomic Deprivation Quartiles per country\nSimilarly, we create a list of tuples of how we want to display the data, and create a figure based on the tuples. This plot would show each country in the y axis and the Log of Mean PM2.5 values in each country’s GRDI quarties.\n\n# List of GRDI values and their corresponding properties\n#column, value name, color, symbol\nplot_data =[\n    ('PM25_Q1', 'Q1', '#440154', '6'),  # Light Blue\n    ('PM25_Q2', 'Q2', '#31688E', '5'),  # Light Green\n    ('PM25_Q3', 'Q3', '#35B779', '7'),  # Yellow\n    ('PM25_Q4', 'Q4', '#FDE725', '8'),  # Orange\n    ('PM25_Q5', 'Q5', '#FF0000', '1')   # Red\n]\n\n# Create a figure\nfig = go.Figure()\n\n# Add traces to the figure.\nfor col, name, color, symbol in plot_data:\n    xlog  = np.log(stats_results[col])\n    fig.add_trace(go.Scatter(\n        x=stats_results[col], #xlog,\n        y=stats_results['Country_Name'],\n        mode='markers+text',  # Add 'text' to mode\n        text=[f'<b>{name}</b>' for _ in stats_results[col]],  # Repeat name for each point\n        name=name,\n        textfont=dict(color=color, size=12),\n        textposition='top center',  # Position the text above the symbol\n        marker_color=color,\n        marker_line_color=\"midnightblue\",\n        marker_symbol=symbol,\n        marker_size=14,\n        marker_line_width=2,\n        marker_opacity=0.6\n        ))\nfig.update_traces(textposition='top center')\n\n    # Customize layout\nfig.update_layout(\n    title='Mean PM2.5 in each GRDI Quartile by Country',\n    xaxis_title='PM2.5 Mean Values',\n    yaxis_title='Country Name',\n    yaxis=dict(tickmode='linear'),\n    legend_title='Statistics',\n    yaxis_type='category',\n    xaxis=dict(rangemode=\"tozero\"),\n    \n    #xaxis=dict(tickvals=[0, 20, 40, 60, 80, 100])\n    )\n\n# Show plot\nfig.show()\n\n\n                                                \n\n\nUse the plotly controls to take a closer look at the results.\nWith this shapely plot, We can examine differences between countries and PM2.5 values. The plot displays the coutnries on the Y axis and log values of the average PM2.5 value on the X axis. Each country displays PM2.5 values averaged within the quartile areas based on GRDI values of each country. A higher quartile (Q) implies a higher degree of deprivation, 1 being the lowest and 5 the highest.\nWhich countries did you identify to be over the WHO AQG values? Which countries have higher contentrations of air pollution in lower socioecomnomic strata? Which are the opposite?\nCongratulations! …. Now you should be able to:\n\nTest test…"
  },
  {
    "objectID": "m204-grdiv1-pm25.html#module-2-air-quality-home",
    "href": "m204-grdiv1-pm25.html#module-2-air-quality-home",
    "title": "Particulate Matter Across Socioeconomic Strata of Countries",
    "section": "Module 2: Air Quality Home",
    "text": "Module 2: Air Quality Home\nIn this lesson, we explored ….\nModule 2: Air Quality"
  },
  {
    "objectID": "m203-ejscreen.html",
    "href": "m203-ejscreen.html",
    "title": "EJSreen exploratory",
    "section": "",
    "text": "In this lesson, we will learn how to use EJScreen, the Environmental Justice Screening and Mapping Tool developed by the Environmental Protection Agency. This tool allows one to map different types of indices with the option of generating reports and side by side comparisons. We will be focusing on Detroit, Michigan and surrounding areas, with a focus on racism and health."
  },
  {
    "objectID": "m203-ejscreen.html#learning-objectives",
    "href": "m203-ejscreen.html#learning-objectives",
    "title": "EJSreen exploratory",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this lesson, you should be able to:\n\nCreate a polygon of an area of interest.\nGenerate community reports and explore charts of an area of interest or boundary.\nCreate bar graphs comparing separate regions from EJScreen generated graphs on Python.\nInterpret and compare different types of indexes.\nAccess and add a shapefile to the EJScreen mapper."
  },
  {
    "objectID": "m203-ejscreen.html#introduction",
    "href": "m203-ejscreen.html#introduction",
    "title": "EJSreen exploratory",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is EJScreen?\nEJScreen, the Environmental Justice Screening and Mapping Tool, was developed by the U.S. Environmental Protection Agency (EPA) to help identify and address environmental justice issues. The tool is designed to provide communities, NGOs, and policymakers with vital data on environmental and demographic factors that affect vulnerable populations.\n\n\nBackground on the development of EJScreen\nThe concept of environmental justice gained prominence in the 1980s and 1990s when it became clear that certain communities, especially low-income and minority populations, were disproportionately affected by environmental hazards. This concern led to the signing of Executive Order 12898 in 1994, which directed federal agencies, including the EPA, to address environmental justice in minority and low-income populations [https://www.epa.gov/laws-regulations/summary-executive-order-12898-federal-actions-address-environmental-justice] (Zahra et al., 2009) (USAID, 2024)\n\n\nImportance of EJScreen\nEJScreen is important in empowering communities. Access to EJScreen helps communities understand the specific environmental challenges they face, such as pollution levels or exposure to hazardous substances. By providing this data, the tool empowers residents to advocate for their health, safety, and well-being. According to the EPA, EJScreen allows communities to participate more effectively in public discussions, regulatory processes, and decision-making, backed by concrete data to support their concerns and needs. Environmental justice focuses on ensuring that no group of people, particularly minority and low-income communities, bears a disproportionate share of environmental burdens.[https://www.epa.gov/ejscreen/what-ejscreen] Environmental and social justice organizations often rely on data to build and support their campaigns. EJScreen provides this kind of data, which can highlight environmental injustices and help NGOs propose solutions and mobilize public support. Many NGOs also use these tools to strengthen their grant applications, demonstrating the specific needs of the communities they serve. This is especially important for organizations advocating for low-income and minority populations, who are often disproportionately impacted by environmental hazards. Policymakers require accurate data to create effective and equitable regulations. EJScreen helps identify areas of concern, prioritize actions, and design policies that better protect vulnerable populations. For example, the EPA uses this tool to inform regulatory actions, compliance monitoring, and enforcement activities.[https://www.epa.gov/environmentaljustice]. By making environmental data accessible to the public, EJScreen also promotes transparency and helps hold governments and industries accountable for their environmental impact. This public accessibility allows for trust and collaboration between communities, regulatory agencies, and policymakers, which is crucial for successfully implementing environmental policies [https://www.epa.gov/environmentaljustice/national-environmental-justice-advisory-council]. Environmental issues often require collaborative solutions involving multiple stakeholders, such as government agencies, NGOs, businesses, and community groups. EJScreen provides a common platform for discussing and addressing these challenges, which fosters more effective partnerships and solutions. EJScreen’s development reflects the EPA’s commitment to enhancing the accessibility of environmental data, promoting transparency, and addressing environmental justice concerns by equipping all stakeholders with the tools needed to assess and respond to environmental health disparities.\nKnowledge Check What is the importance of EJScreen? Helping environmental and social justice organizations support their campaigns Providing policymakers with data to create effective regulations Empowering residents to advocate for their health and safety Providing a common platform for collaborative solutions between multiple stakeholders All of the above"
  },
  {
    "objectID": "m203-ejscreen.html#load-the-data",
    "href": "m203-ejscreen.html#load-the-data",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "Load the Data",
    "text": "Load the Data"
  },
  {
    "objectID": "m203-ejscreen.html#svi-and-detroit-metro",
    "href": "m203-ejscreen.html#svi-and-detroit-metro",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "SVI and Detroit Metro",
    "text": "SVI and Detroit Metro\nSVI has a primary overall SVI score, but also provides sublayers. These include minority, socioeconomic, housing, and household data.\n\nimport xarray as xr\nimport rasterio\nimport rasterio.mask\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pygris\nimport numpy as np\n\n# Fetch Detroit metro area counties using pygris\nmetro_counties = pygris.counties(state=\"MI\", year=2022)\ndetroit_metro = metro_counties[metro_counties['NAME'].isin([\n    'Wayne', 'Oakland', 'Macomb', 'Livingston', 'St. Clair', 'Lapeer'\n])]\n\n# Dissolve the counties into a single polygon\ndetroit_metro = detroit_metro.dissolve(by='STATEFP')\n\n# Convert to GeoDataFrame\ndetroit_metro = gpd.GeoDataFrame(detroit_metro, geometry='geometry', crs='EPSG:4269')\n\n# Specify the TIF files\ntif_files = [\n    \"data/svi/svi_2020_tract_overall_wgs84.tif\",\n    \"data/svi/svi_2020_tract_minority_wgs84.tif\",\n    \"data/svi/svi_2020_tract_socioeconomic_wgs84.tif\",\n    \"data/svi/svi_2020_tract_housing_wgs84.tif\",\n    \"data/svi/svi_2020_tract_household_wgs84.tif\"\n]\n\n# Create an empty list to store the individual DataArrays\ndata_arrays = []\n\n# Read each TIF file, clip it to Detroit metro's extent, and append it to the list\nfor file in tif_files:\n    with rasterio.open(file) as src:\n        # Reproject Detroit metro boundary to match the raster CRS\n        metro_reprojected = detroit_metro.to_crs(src.crs)\n        \n        # Clip the raster to Detroit metro's geometry\n        out_image, out_transform = rasterio.mask.mask(src, metro_reprojected.geometry, crop=True)\n        out_meta = src.meta.copy()\n        \n        # Update the metadata\n        out_meta.update({\"driver\": \"GTiff\",\n                         \"height\": out_image.shape[1],\n                         \"width\": out_image.shape[2],\n                         \"transform\": out_transform})\n        \n        # Create coordinates\n        height = out_meta['height']\n        width = out_meta['width']\n        cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n        xs, ys = rasterio.transform.xy(out_transform, rows, cols)\n        \n        # Convert lists to numpy arrays\n        xs = np.array(xs)\n        ys = np.array(ys)\n        \n        # Reshape coordinates to match dimensions of the raster\n        xs = xs.reshape(height, width)\n        ys = ys.reshape(height, width)\n        \n        # Create a DataArray from the clipped data\n        da = xr.DataArray(out_image[0],  # Use the first band\n                          coords={'y': ('y', ys[:, 0]),\n                                  'x': ('x', xs[0, :])},\n                          dims=['y', 'x'])\n        da.attrs['crs'] = str(src.crs)  # Convert CRS to string\n        da.attrs['transform'] = out_transform\n        data_arrays.append(da)\n\n# Combine all DataArrays into a single DataSet\nds = xr.concat(data_arrays, dim='layer')\n\n# Rename the layers using the appropriate dimension\nds = ds.assign_coords(layer=('layer', ['Overall', 'Minority', 'Socioeconomic', 'Housing', 'Household']))\n\n# Define the colorbar limits\nvmin, vmax = 0, 1\n\n# Create a multipanel plot\nfig, axes = plt.subplots(3, 2, figsize=(15, 20))\naxes = axes.flatten()\n\n# Plot each layer\nfor i, layer in enumerate(ds.layer.values):\n    # Plot with custom color limits\n    im = ds.sel(layer=layer).plot(ax=axes[i], add_colorbar=False, vmin=vmin, vmax=vmax, cmap='viridis')\n    axes[i].set_title(layer)\n    \n    # Plot Detroit metro boundary\n    metro_reprojected.boundary.plot(ax=axes[i], color='red', linewidth=1)\n\n# Remove the extra subplot\nfig.delaxes(axes[5])\n\n# Add a single colorbar\ncbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\ncbar = fig.colorbar(im, cax=cbar_ax, label='SVI Score')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "m203-ejscreen.html#tri-api-column-definitions",
    "href": "m203-ejscreen.html#tri-api-column-definitions",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "TRI API Column Definitions",
    "text": "TRI API Column Definitions\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nFACILITY_NAME\nThe name of the facility reporting to TRI\n\n\nTRI_FACILITY_ID\nA unique identifier for the facility in the TRI database\n\n\nSTREET_ADDRESS\nThe street address of the facility\n\n\nCITY_NAME\nThe city where the facility is located\n\n\nCOUNTY_NAME\nThe county where the facility is located\n\n\nSTATE_ABBR\nThe two-letter abbreviation for the state where the facility is located\n\n\nZIP_CODE\nThe ZIP code of the facility’s location\n\n\nPREF_LATITUDE\nThe preferred latitude coordinate of the facility\n\n\nPREF_LONGITUDE\nThe preferred longitude coordinate of the facility\n\n\nPARENT_CO_NAME\nThe name of the parent company, if applicable\n\n\nINDUSTRY_SECTOR_CODE\nA code representing the industry sector of the facility\n\n\nINDUSTRY_SECTOR\nA description of the industry\n\n\n\nNote: The availability and exact names of these columns may vary depending on the specific TRI API endpoint and query parameters used. Always refer to the official EPA TRI documentation for the most up-to-date and comprehensive information.\n\nImportant Considerations\n\nNot all columns may be present in every API response.\nColumn names may have slight variations (e.g., with or without underscores).\nThe EPA occasionally updates their API and data structure.\nSome columns related to chemical releases and waste management may have additional variations or breakdowns.\nNumeric values (like release amounts) are typically reported in pounds, but always verify the units.\nFor coordinates (PREF_LATITUDE and PREF_LONGITUDE), be aware that these are the preferred coordinates, which may have been adjusted for accuracy or privacy reasons."
  },
  {
    "objectID": "m203-ejscreen.html#envirofacts-tri-facilities-w-api",
    "href": "m203-ejscreen.html#envirofacts-tri-facilities-w-api",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "ENVIROFACTS TRI Facilities w/ API",
    "text": "ENVIROFACTS TRI Facilities w/ API\n\nimport xarray as xr\nimport rasterio\nimport rasterio.mask\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pygris\nimport numpy as np\nfrom shapely.geometry import box\nimport pandas as pd\nimport requests\nimport contextily as ctx\n\n# Fetch Detroit metro area counties using pygris\nmetro_counties = pygris.counties(state=\"MI\", year=2022)\ndetroit_metro = metro_counties[metro_counties['NAME'].isin([\n    'Wayne', 'Oakland', 'Macomb', 'Livingston', 'St. Clair', 'Lapeer'\n])]\n\n# Dissolve the counties into a single polygon\ndetroit_metro = detroit_metro.dissolve(by='STATEFP')\n\n# Get the bounding box\nbbox = detroit_metro.total_bounds\n\n# Print the bounding box coordinates\nprint(\"Bounding Box:\")\nprint(f\"Minimum X (Longitude): {bbox[0]}\")\nprint(f\"Minimum Y (Latitude): {bbox[1]}\")\nprint(f\"Maximum X (Longitude): {bbox[2]}\")\nprint(f\"Maximum Y (Latitude): {bbox[3]}\")\n\n# Create the bounding box as a polygon\nbbox_polygon = gpd.GeoDataFrame(\n    geometry=[box(*bbox)],\n    crs=detroit_metro.crs\n)\n\n# Fetch TRI facility data from EPA API for each county\ncounties = ['Wayne', 'Oakland', 'Macomb', 'Livingston', 'St. Clair', 'Lapeer']\ntri_data = []\n\nfor county in counties:\n    api_url = f\"https://data.epa.gov/efservice/tri_facility/state_abbr/MI/county_name/{county}/JSON\"\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        county_data = response.json()\n        tri_data.extend(county_data)\n    else:\n        print(f\"Failed to fetch data for {county} County. Status code: {response.status_code}\")\n\n# Convert TRI data to a DataFrame\ntri_df = pd.DataFrame(tri_data)\n\nprint(f\"Number of facilities fetched: {len(tri_df)}\")\n\n# Create a copy of the dataframe to avoid SettingWithCopyWarning\ntri_df_clean = tri_df.copy()\n\n# Remove facilities with empty latitude or longitude values\ntri_df_clean = tri_df_clean.dropna(subset=['pref_latitude', 'pref_longitude'])\n\nprint(f\"Number of facilities after removing empty coordinates: {len(tri_df_clean)}\")\n\n# Convert latitude and longitude to numeric type\ntri_df_clean['pref_latitude'] = pd.to_numeric(tri_df_clean['pref_latitude'], errors='coerce')\ntri_df_clean['pref_longitude'] = pd.to_numeric(tri_df_clean['pref_longitude'], errors='coerce')\n\n# Function to correct longitude\ndef correct_longitude(lon):\n    if lon > 0:\n        return -lon\n    return lon\n\n# Apply longitude correction\ntri_df_clean['pref_longitude'] = tri_df_clean['pref_longitude'].apply(correct_longitude)\n\n# Calculate IQR for longitude\nQ1 = tri_df_clean['pref_longitude'].quantile(0.25)\nQ3 = tri_df_clean['pref_longitude'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Remove outliers\ntri_df_clean = tri_df_clean[(tri_df_clean['pref_longitude'] >= lower_bound) & \n                            (tri_df_clean['pref_longitude'] <= upper_bound)]\n\nprint(f\"Number of facilities after removing longitude outliers: {len(tri_df_clean)}\")\n\n# Create a GeoDataFrame from the cleaned TRI data\ndetroit_tri = gpd.GeoDataFrame(\n    tri_df_clean, \n    geometry=gpd.points_from_xy(tri_df_clean.pref_longitude, tri_df_clean.pref_latitude),\n    crs=\"EPSG:4326\"\n)\n\n# Reproject data to Web Mercator for contextily\ndetroit_metro = detroit_metro.to_crs(epsg=3857)\nbbox_polygon = bbox_polygon.to_crs(epsg=3857)\ndetroit_tri = detroit_tri.to_crs(epsg=3857)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Plot the metro area and bounding box\ndetroit_metro.plot(ax=ax, color='lightblue', edgecolor='black', alpha=0.5)\nbbox_polygon.boundary.plot(ax=ax, color='red', linewidth=2)\n\n# Plot TRI facilities\ndetroit_tri.plot(ax=ax, color='red', markersize=50, alpha=0.7)\n\n# Add the basemap\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the extent of the map to the bounding box\nax.set_xlim(bbox_polygon.total_bounds[0], bbox_polygon.total_bounds[2])\nax.set_ylim(bbox_polygon.total_bounds[1], bbox_polygon.total_bounds[3])\n\n# Remove axes\nax.set_axis_off()\n\nplt.title(\"Detroit Metro Area TRI Facilities\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final number of TRI facilities in the Detroit metro area: {len(detroit_tri)}\")"
  },
  {
    "objectID": "m203-ejscreen.html#regulated-facilities-w-icis-air",
    "href": "m203-ejscreen.html#regulated-facilities-w-icis-air",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "Regulated Facilities w/ ICIS-AIR",
    "text": "Regulated Facilities w/ ICIS-AIR\n\nimport pandas as pd\nimport requests\nimport time\n\n# Base URL for ECHO ICIS-AIR API\nbase_url = \"https://echodata.epa.gov/echo/air_rest_services\"\n\n# Parameters for the initial API call\nparams = {\n    \"output\": \"JSON\",\n    \"p_st\": \"MI\"\n}\n\ndef get_facilities():\n    response = requests.get(f\"{base_url}.get_facilities\", params=params)\n    if response.status_code == 200:\n        data = response.json()\n        if 'Results' in data:\n            qid = data['Results']['QueryID']\n            print(f\"Query ID: {qid}\")\n            print(f\"Total Facilities: {data['Results']['QueryRows']}\")\n            return qid\n    print(\"Failed to get facilities and QID\")\n    return None\n\ndef get_facility_data(qid):\n    all_facilities = []\n    page = 1\n    while True:\n        params = {\"qid\": qid, \"pageno\": page, \"output\": \"JSON\"}\n        response = requests.get(f\"{base_url}.get_qid\", params=params)\n        if response.status_code == 200:\n            data = response.json()\n            if 'Results' in data and 'Facilities' in data['Results']:\n                facilities = data['Results']['Facilities']\n                if not facilities:  # No more facilities to retrieve\n                    break\n                all_facilities.extend(facilities)\n                print(f\"Retrieved page {page}\")\n                page += 1\n            else:\n                break\n        else:\n            print(f\"Failed to retrieve page {page}\")\n            break\n    return all_facilities\n\n# Step 1: Get the Query ID\nqid = get_facilities()\n\nif qid:\n    # Step 2: Use get_qid to retrieve all facility data\n    print(\"Retrieving facility data...\")\n    facilities = get_facility_data(qid)\n    \n    # Convert to DataFrame\n    df_icis_air = pd.DataFrame(facilities)\n    \n    print(f\"\\nSuccessfully retrieved {len(df_icis_air)} ICIS-AIR facilities for Michigan\")\n    print(\"\\nColumns in the dataset:\")\n    print(df_icis_air.columns)\n    \n    # Display the first few rows\n    print(\"\\nFirst few rows of the data:\")\n    print(df_icis_air.head())\n    \n    # Save to CSV\nelse:\n    print(\"Failed to retrieve facility data\")\n\n    import pandas as pd\n\n# List of Detroit metro counties\nmetro_counties = ['Wayne', 'Oakland', 'Macomb', 'Livingston', 'St. Clair', 'Lapeer']\n\n# Subset the dataframe to include only the Detroit metro counties\ndf_detroit_metro = df_icis_air[df_icis_air['AIRCounty'].isin(metro_counties)]\n\n# Print information about the subset\nprint(f\"Total ICIS-AIR facilities in Michigan: {len(df_icis_air)}\")\nprint(f\"ICIS-AIR facilities in Detroit metro area: {len(df_detroit_metro)}\")\n\n# Display the count of facilities in each metro county\nprint(\"\\nFacilities per county:\")\nprint(df_detroit_metro['AIRCounty'].value_counts())\n\n# Display the first few rows of the subset\nprint(\"\\nFirst few rows of the Detroit metro ICIS-AIR facilities:\")\nprint(df_detroit_metro.head())\n\n# Additional information: unique values in AIRCounty\nprint(\"\\nUnique values in AIRCounty column:\")\nprint(df_icis_air['AIRCounty'].unique())\n\nCreate geopandas and plot.\n\n# Count records with missing coordinate values\nmissing_coords = df_detroit_metro[(df_detroit_metro['FacLat'].isnull()) | (df_detroit_metro['FacLong'].isnull())]\nprint(f\"Number of ICIS-AIR records with missing coordinates: {len(missing_coords)}\")\n\n# Remove records with missing coordinates\ndf_detroit_metro = df_detroit_metro.dropna(subset=['FacLat', 'FacLong'])\n\n# Create a GeoDataFrame for ICIS-AIR facilities\ngdf_icis_air = gpd.GeoDataFrame(\n    df_detroit_metro, \n    geometry=gpd.points_from_xy(df_detroit_metro.FacLong, df_detroit_metro.FacLat),\n    crs=\"EPSG:4326\"\n)\n\n# Reproject ICIS-AIR data to Web Mercator\ngdf_icis_air = gdf_icis_air.to_crs(epsg=3857)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Plot the metro area and bounding box (reusing objects from earlier)\ndetroit_metro.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=2)\nbbox_polygon.boundary.plot(ax=ax, color='red', linewidth=2)\n\n# Plot ICIS-AIR facilities\ngdf_icis_air.plot(ax=ax, color='cyan', markersize=50, alpha=0.7, label='ICIS-AIR Facilities')\n\n# Plot TRI facilities (reusing the detroit_tri object from earlier)\ndetroit_tri.plot(ax=ax, color='purple', markersize=50, alpha=0.7, label='TRI Facilities')\n\n# Add the basemap\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the extent of the map to the bounding box\nax.set_xlim(bbox_polygon.total_bounds[0], bbox_polygon.total_bounds[2])\nax.set_ylim(bbox_polygon.total_bounds[1], bbox_polygon.total_bounds[3])\n\n# Remove axes\nax.set_axis_off()\n\n# Add legend\nax.legend()\n\nplt.title(\"Detroit Metro Area TRI and ICIS-AIR Facilities\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Number of TRI facilities plotted: {len(detroit_tri)}\")\nprint(f\"Number of ICIS-AIR facilities plotted: {len(gdf_icis_air)}\")"
  },
  {
    "objectID": "m203-ejscreen.html#custom-tri-form-a-search",
    "href": "m203-ejscreen.html#custom-tri-form-a-search",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "Custom TRI Form A Search",
    "text": "Custom TRI Form A Search\n\n# URL of the CSV file\nurl = \"https://dmap-epa-enviro-prod-export.s3.amazonaws.com/338211710.CSV\"\n\n# Read the CSV file directly with pandas\ndf_tri_custom = pd.read_csv(url)\nprint(f\"Successfully read CSV. Number of records: {len(df_tri_custom)}\")\n\n# Display information about the dataset\nprint(\"\\nColumns in the dataset:\")\nprint(df_tri_custom.columns)\n\n# Assuming the latitude and longitude columns are named 'LATITUDE' and 'LONGITUDE'\n# Adjust these names if they're different in your CSV\nlat_col = 'LATITUDE'\nlon_col = 'LONGITUDE'\nrelease_col = 'AIR_TOTAL_RELEASE'  # Adjust this to the actual column name for air releases\n\n# Remove records with missing coordinates or air release data\ndf_tri_clean = df_tri_custom.dropna(subset=[lat_col, lon_col, release_col])\n\nprint(f\"\\nNumber of records after removing missing data: {len(df_tri_clean)}\")\n\n# Create a GeoDataFrame\ngdf_tri_custom = gpd.GeoDataFrame(\n    df_tri_clean, \n    geometry=gpd.points_from_xy(df_tri_clean[lon_col], df_tri_clean[lat_col]),\n    crs=\"EPSG:4326\"\n)\n\n# Reproject to Web Mercator\ngdf_tri_custom = gdf_tri_custom.to_crs(epsg=3857)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Plot the metro area and bounding box (reusing objects from earlier)\ndetroit_metro.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=2)\nbbox_polygon.boundary.plot(ax=ax, color='orangered', linewidth=2)\n\n# Plot TRI facilities with graduated symbols based on air releases\nscatter = ax.scatter(gdf_tri_custom.geometry.x, gdf_tri_custom.geometry.y, \n                     # s=gdf_tri_custom[release_col]/100,  # Adjust the scaling factor as needed\n                     c='orangered',  # Static fill color\n                     edgecolor='yellow',  # Outline color\n                     linewidth=1,  # Adjust the outline width as needed\n                     alpha=0.7)\n\n# Add the basemap\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the extent of the map to the bounding box\nax.set_xlim(bbox_polygon.total_bounds[0], bbox_polygon.total_bounds[2])\nax.set_ylim(bbox_polygon.total_bounds[1], bbox_polygon.total_bounds[3])\n\n# Remove axes\nax.set_axis_off()\n\n# Add a legend for symbol sizes\nlegend_sizes = [1000, 10000, 100000]  # Example sizes, adjust based on your data\nlegend_elements = [plt.scatter([], [], s=size/100, c='orangered', edgecolor='yellow', \n                               linewidth=1, alpha=1, label=f'{size:,}') \n                   for size in legend_sizes]\nax.legend(handles=legend_elements, title='Total Air Releases (lbs)', \n          loc='lower right', title_fontsize=12, fontsize=10)\n\nplt.title(\"Detroit Metro Area TRI Facilities - Total Air Releases (Custom Data)\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nNumber of TRI facilities plotted: {len(gdf_tri_custom)}\")\nprint(f\"Total air releases: {gdf_tri_custom[release_col].sum():,.2f} lbs\")\nprint(f\"Average air release per facility: {gdf_tri_custom[release_col].mean():,.2f} lbs\")\n\n\nRasterizing Pollution Sums\n\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport rasterio\nfrom rasterio import features\nfrom rasterio.transform import from_origin\nfrom matplotlib.colors import BoundaryNorm, ListedColormap\nimport xarray as xr\nimport rioxarray  # This imports rioxarray and adds the .rio accessor to xarray objects\n\n# Ensure gdf_tri_custom and detroit_metro are in the correct CRS (should be EPSG:3857 for Web Mercator)\ngdf_tri_custom = gdf_tri_custom.to_crs(epsg=3857)\ndetroit_metro = detroit_metro.to_crs(epsg=3857)\n\n# Get the bounds of the Detroit metro area\nminx, miny, maxx, maxy = detroit_metro.total_bounds\n\n# Define the resolution (100m)\nresolution = 5000\n\n# Calculate the number of cells\nnx = int((maxx - minx) / resolution)\nny = int((maxy - miny) / resolution)\n\n# Create the transform for the raster\ntransform = from_origin(minx, maxy, resolution, resolution)\n\n# Prepare geometries and values for rasterization\nshapes = ((geom, value) for geom, value in zip(gdf_tri_custom.geometry, gdf_tri_custom.AIR_TOTAL_RELEASE))\n\n# Rasterize the point data\nraster = features.rasterize(shapes=shapes, \n                            out_shape=(ny, nx), \n                            transform=transform, \n                            fill=0, \n                            all_touched=True, \n                            merge_alg=rasterio.enums.MergeAlg.add)\n\n# Convert the raster to an xarray DataArray\n# Note: We use ny and nx here to ensure the coordinates match the raster shape\nraster_da = xr.DataArray(raster, \n                         coords={'y': np.linspace(maxy, miny, ny),\n                                 'x': np.linspace(minx, maxx, nx)},\n                         dims=['y', 'x'])\nraster_da.rio.write_crs(detroit_metro.crs, inplace=True)\n\n# Clip the raster with the Detroit metro boundary\nclipped_raster = raster_da.rio.clip(detroit_metro.geometry.values, detroit_metro.crs, drop=False, all_touched=True)\n\n# Define the breaks for the discrete scale\nbreaks = [0, 1, 10, 100, 1000, 10000, 100000, 250000, 500000]\n\n# Create a custom colormap\ncolors = ['#FFFFFF', '#FFFFCC', '#FFEDA0', '#FED976', '#FEB24C', '#FD8D3C', '#FC4E2A', '#E31A1C', '#B10026']\ncmap = ListedColormap(colors)\n\n# Create a normalization based on the breaks\nnorm = BoundaryNorm(breaks, cmap.N)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Plot the clipped raster with the custom colormap and norm\nim = ax.imshow(clipped_raster, extent=[minx, maxx, miny, maxy], origin='upper', \n               cmap=cmap, norm=norm)\n\n# Add colorbar with discrete labels\ncbar = plt.colorbar(im, ax=ax, extend='max', \n                    label='Total Air Releases (pounds)', \n                    ticks=breaks)\ncbar.ax.set_yticklabels([f'{b:,}' for b in breaks])\n\n# Plot the TRI facility points\ngdf_tri_custom.plot(ax=ax, color='blue', markersize=20, alpha=0.7)\n\n# Plot the Detroit metro boundary\ndetroit_metro.boundary.plot(ax=ax, color='black', linewidth=2)\n\n# Set the extent to match the Detroit metro area\nax.set_xlim(minx, maxx)\nax.set_ylim(miny, maxy)\n\n# Add title and labels\nax.set_title('TRI Air Total Release (100m resolution sum) with Facility Locations', fontsize=16)\nax.set_xlabel('X Coordinate')\nax.set_ylabel('Y Coordinate')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Number of TRI facilities plotted: {len(gdf_tri_custom)}\")\nprint(f\"Total air releases: {gdf_tri_custom['AIR_TOTAL_RELEASE'].sum():,.2f}\")\nprint(f\"Maximum cell value in raster: {clipped_raster.max().values:,.2f}\")"
  },
  {
    "objectID": "m203-ejscreen.html#air-release-vulnerability-index",
    "href": "m203-ejscreen.html#air-release-vulnerability-index",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "Air Release Vulnerability Index",
    "text": "Air Release Vulnerability Index\n\nimport numpy as np\nimport xarray as xr\nimport rioxarray\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom rasterio.enums import Resampling\nfrom scipy import stats\n\n# Select the 'Overall' layer\nsvi_overall = ds.sel(layer='Overall')\n\n# Convert to rioxarray for geospatial operations\nsvi_overall = svi_overall.rio.write_crs(\"EPSG:4326\")\n\n# Reproject SVI to match the CRS of the air release raster\nsvi_reprojected = svi_overall.rio.reproject_match(clipped_raster)\n\n# Clip SVI raster to the Detroit metro boundary\nsvi_clipped = svi_reprojected.rio.clip(detroit_metro.geometry.values, detroit_metro.crs, drop=True, all_touched=True)\n\n# Disaggregate the air release data to match the resolution of the SVI data\nair_release_disaggregated = clipped_raster.rio.reproject_match(\n    svi_clipped,\n    resampling=Resampling.bilinear\n)\n\n# Calculate raster correlation between SVI overall and air release within Detroit metro boundary\nsvi_flat = svi_clipped.values.flatten()\nair_flat = air_release_disaggregated.values.flatten()\n\n# Remove NaN values\nmask = ~np.isnan(svi_flat) & ~np.isnan(air_flat)\nsvi_flat = svi_flat[mask]\nair_flat = air_flat[mask]\n\ncorrelation, p_value = stats.pearsonr(svi_flat, air_flat)\n\nprint(f\"Raster Correlation between SVI Overall and Air Release within Detroit metro: {correlation:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\n# New correlation analysis\n# Ensure gdf_tri_custom is in the same CRS as svi_clipped\ngdf_tri_custom = gdf_tri_custom.to_crs(svi_clipped.rio.crs)\n\n# Extract SVI values at TRI facility locations\nsvi_values = []\nfor point in gdf_tri_custom.geometry:\n    svi_value = svi_clipped.sel(x=point.x, y=point.y, method=\"nearest\").values\n    svi_values.append(svi_value)\n\n# Add SVI values to gdf_tri_custom\ngdf_tri_custom['SVI_OVERALL'] = svi_values\n\n# Perform correlation analysis\ncorrelation_points, p_value_points = stats.pearsonr(gdf_tri_custom['AIR_TOTAL_RELEASE'], gdf_tri_custom['SVI_OVERALL'])\n\nprint(f\"Point-based Correlation between SVI Overall and AIR_TOTAL_RELEASE: {correlation_points:.4f}\")\nprint(f\"P-value: {p_value_points:.4f}\")\n\n# Log1p transform the air release data and scale to 0-1\nair_release_log = np.log1p(air_release_disaggregated)\nair_release_scaled = (air_release_log - air_release_log.min()) / (air_release_log.max() - air_release_log.min())\n\n# Multiply scaled air release data with SVI data\nvulnerability_indicator = air_release_scaled * svi_clipped\n\n# Create the plots\nfig, axs = plt.subplots(3, 1, figsize=(15, 45))\n\n# Plot SVI Overall\nim1 = svi_clipped.plot(ax=axs[0], cmap='viridis', vmin=0, vmax=1, add_colorbar=False)\nplt.colorbar(im1, ax=axs[0], label='SVI Overall')\naxs[0].set_title('Social Vulnerability Index (Overall)', fontsize=16)\ndetroit_metro.boundary.plot(ax=axs[0], color='black', linewidth=2)\n\n# Plot Original Air Release (log-transformed for better visualization)\nim2 = np.log1p(air_release_disaggregated).plot(ax=axs[1], cmap='YlOrRd', add_colorbar=False)\nplt.colorbar(im2, ax=axs[1], label='Log(Air Release + 1)')\naxs[1].set_title('Air Release (Log-transformed)', fontsize=16)\ndetroit_metro.boundary.plot(ax=axs[1], color='black', linewidth=2)\n\n# Plot Air Release Vulnerability Indicator\nim3 = vulnerability_indicator.plot(ax=axs[2], cmap='YlOrRd', vmin=0, vmax=1, add_colorbar=False)\nplt.colorbar(im3, ax=axs[2], label='Air Release Vulnerability Indicator')\naxs[2].set_title('Air Release Vulnerability Indicator\\n(Scaled Air Release * SVI)', fontsize=16)\ndetroit_metro.boundary.plot(ax=axs[2], color='black', linewidth=2)\n\nfor ax in axs:\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    ax.set_xlim(svi_clipped.x.min(), svi_clipped.x.max())\n    ax.set_ylim(svi_clipped.y.min(), svi_clipped.y.max())\n\nplt.tight_layout()\nplt.show()\n\n# Print some statistics\nprint(f\"Minimum vulnerability indicator: {vulnerability_indicator.min().values:.4f}\")\nprint(f\"Maximum vulnerability indicator: {vulnerability_indicator.max().values:.4f}\")\nprint(f\"Mean vulnerability indicator: {vulnerability_indicator.mean().values:.4f}\")\n\n\n# Convert the vulnerability indicator to a pandas DataFrame\nvulnerability_df = vulnerability_indicator.to_dataframe(name='index').reset_index()\n\n# Sort by index value and get the top 10\ntop_10 = vulnerability_df.sort_values('index', ascending=False).head(10)\n\n# Create points from the coordinates\ntop_10['geometry'] = gpd.points_from_xy(top_10.x, top_10.y)\ntop_10_gdf = gpd.GeoDataFrame(top_10, geometry='geometry', crs=vulnerability_indicator.rio.crs)\n\n# Create the final map\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Plot the Detroit metro boundary\ndetroit_metro.boundary.plot(ax=ax, color='black', linewidth=2)\n\n# Plot the top 10 points\ntop_10_gdf.plot(ax=ax, color='blue', markersize=100, alpha=0.7)\n\n# Add labels to the points\nfor idx, row in top_10_gdf.iterrows():\n    ax.annotate(f\"#{idx+1}\", (row.geometry.x, row.geometry.y), \n                xytext=(3, 3), textcoords=\"offset points\", \n                color='white', fontweight='bold')\n\n# Add a basemap\nctx.add_basemap(ax, crs=vulnerability_indicator.rio.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the extent to match the Detroit metro area\nax.set_xlim(vulnerability_indicator.x.min(), vulnerability_indicator.x.max())\nax.set_ylim(vulnerability_indicator.y.min(), vulnerability_indicator.y.max())\n\nax.set_title('Top 10 Areas with Highest Air Release Vulnerability Index', fontsize=16)\nax.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n# Print the coordinates of the top 10 points\nprint(\"Coordinates of the top 10 points:\")\nfor idx, row in top_10_gdf.iterrows():\n    print(f\"#{idx+1}: ({row.geometry.x}, {row.geometry.y})\")"
  },
  {
    "objectID": "m203-ejscreen.html#places",
    "href": "m203-ejscreen.html#places",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "PLACES",
    "text": "PLACES\nThe CDC PLACES (Population Level Analysis and Community Estimates) dataset is a collaboration between the Centers for Disease Control and Prevention (CDC), the Robert Wood Johnson Foundation, and the CDC Foundation. It provides model-based population-level analysis and community estimates of health indicators for all counties, places (incorporated and census designated places), census tracts, and ZIP Code Tabulation Areas (ZCTAs) across the United States.\n\nKey Points\n\nGeographic Coverage: Entire United States, including all 50 states, the District of Columbia, and Puerto Rico.\nGeographic Granularity: Multiple levels including counties, cities/towns, census tracts, and ZIP codes.\nHealth Indicators: Wide range of chronic disease measures related to health outcomes, prevention, and health risk behaviors.\nData Sources:\n\nBehavioral Risk Factor Surveillance System (BRFSS)\nU.S. Census Bureau’s American Community Survey (ACS)\n\nMethodology: Uses small area estimation methods for small geographic areas.\nHealth Measures Include:\n\nChronic diseases: e.g., asthma, COPD, heart disease, diabetes\nHealth risk behaviors: e.g., smoking, physical inactivity, binge drinking\nPrevention practices: e.g., health insurance coverage, dental visits, cholesterol screening\n\nSocioeconomic Data: Includes some socioeconomic and demographic variables.\nAnnual Updates: Providing recent estimates for local areas.\n\nThis dataset is particularly valuable for: - Public health researchers - Policymakers - Community organizations\nIt provides a standardized way to compare health indicators across different geographic areas and can be used to inform targeted interventions and policy decisions, especially in addressing health disparities at a local level.\n\n\nProcessing\n\nimport geopandas as gpd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport requests\nimport contextily as ctx\n\n# Define the GeoJSON API endpoint\nurl = \"https://data.cdc.gov/resource/cwsq-ngmh.geojson\"\n\n# Define the Detroit metro area counties\ndetroit_counties = ['Wayne', 'Oakland', 'Macomb', 'Livingston', 'St. Clair', 'Lapeer']\n\n# Create the county filter string\ncounty_filter = \" OR \".join([f\"countyname = '{county}'\" for county in detroit_counties])\n\n# Define the query parameters\nparams = {\n    \"$where\": f\"stateabbr = 'MI' AND ({county_filter})\",\n    \"$limit\": 50000  # Adjust if necessary\n}\n\n# Make the API request\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = response.json()\n    print(f\"Successfully retrieved data\")\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n    print(response.text)\n\n# Convert to GeoDataFrame\ngdf = gpd.read_file(response.text)\n\n# Print available health measures\nprint(\"\\nAvailable health measures:\")\nprint(gdf['measure'].unique())\n\n# Print the first few rows to see the structure of the data\nprint(\"\\nFirst few rows of the GeoDataFrame:\")\nprint(gdf.head())\n\n# Print basic information about the GeoDataFrame\nprint(\"\\nGeoDataFrame Info:\")\nprint(gdf.info())\n\n# Create a sample map for one health measure (e.g., Current asthma)\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Filter for the specific measure and ensure data_value is numeric\ngdf_asthma = gdf[gdf['measure'] == 'Current asthma among adults'].copy()\ngdf_asthma['data_value'] = pd.to_numeric(gdf_asthma['data_value'], errors='coerce')\n\n# Plot the asthma data\ngdf_asthma.plot(column='data_value', \n                ax=ax, \n                legend=True, \n                legend_kwds={'label': 'Asthma Prevalence (%)', 'orientation': 'horizontal'},\n                cmap='YlOrRd',\n                missing_kwds={'color': 'lightgrey'})\n\n# Add county boundaries\n#gdf_asthma.dissolve(by='countyname').boundary.plot(ax=ax, color='black', linewidth=0.5)\n\n# Add basemap\nctx.add_basemap(ax, crs=gdf_asthma.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the extent to match the Detroit metro area\nax.set_xlim(gdf_asthma.total_bounds[0], gdf_asthma.total_bounds[2])\nax.set_ylim(gdf_asthma.total_bounds[1], gdf_asthma.total_bounds[3])\n\nplt.title('Asthma Prevalence in Detroit Metro Area', fontsize=16)\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print some statistics for the asthma data\nprint(\"\\nAsthma Statistics:\")\nprint(f\"Average asthma prevalence: {gdf_asthma['data_value'].mean():.2f}%\")\nprint(f\"Minimum asthma prevalence: {gdf_asthma['data_value'].min():.2f}%\")\nprint(f\"Maximum asthma prevalence: {gdf_asthma['data_value'].max():.2f}%\")\n\n# Print the number of census tracts per county\nprint(\"\\nNumber of census tracts per county:\")\nprint(gdf_asthma['countyname'].value_counts())\n\n\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport contextily as ctx\nfrom scipy.interpolate import griddata\nimport rasterio\nfrom rasterio.transform import from_origin\n\n# Assuming gdf_asthma is already created and contains the asthma data\n\n# Ensure gdf_asthma is in EPSG:4326\ngdf_asthma = gdf_asthma.to_crs(epsg=4326)\n\n# Extract coordinates and values\nX = gdf_asthma.geometry.x.values\nY = gdf_asthma.geometry.y.values\nZ = gdf_asthma['data_value'].values\n\n# Remove any NaN values\nmask = ~np.isnan(Z)\nX, Y, Z = X[mask], Y[mask], Z[mask]\n\n# Create a grid to interpolate over\ngrid_resolution = 0.025  # in degrees\nx_min, y_min, x_max, y_max = gdf_asthma.total_bounds\ngrid_x = np.arange(x_min, x_max, grid_resolution)\ngrid_y = np.arange(y_min, y_max, grid_resolution)\ngrid_xx, grid_yy = np.meshgrid(grid_x, grid_y)\n\n# Perform IDW interpolation\npoints = np.column_stack((X, Y))\ngrid_z = griddata(points, Z, (grid_xx, grid_yy), method='linear')\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Plot the interpolated data\nim = ax.imshow(grid_z, extent=[x_min, x_max, y_min, y_max], \n               origin='lower', cmap='YlOrRd', alpha=0.7)\n\n# Add colorbar\ncbar = plt.colorbar(im, ax=ax, label='Asthma Prevalence')\n\n# Add basemap\nctx.add_basemap(ax, crs=gdf_asthma.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Set the extent to match the Detroit metro area\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\n\nplt.title('IDW Interpolated Asthma Prevalence in Detroit Metro Area', fontsize=16)\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Save the interpolated raster\ntransform = from_origin(x_min, y_max, grid_resolution, grid_resolution)\nwith rasterio.open('idw_asthma.tif', 'w', \n                   driver='GTiff', \n                   height=grid_z.shape[0], \n                   width=grid_z.shape[1], \n                   count=1, \n                   dtype=grid_z.dtype, \n                   crs='EPSG:4326',  # Explicitly set the CRS\n                   transform=transform) as dst:\n    dst.write(grid_z, 1)\n\n# Print some statistics about the interpolated data\nprint(f\"Minimum interpolated value: {np.nanmin(grid_z):.2f}\")\nprint(f\"Maximum interpolated value: {np.nanmax(grid_z):.2f}\")\nprint(f\"Mean interpolated value: {np.nanmean(grid_z):.2f}\")\n\n\n\n\n\n\n\nData Review\n\n\n\nThe Toxics Release Inventory (TRI) and the Integrated Compliance Information System for Air (ICIS-AIR) are two important but distinct environmental reporting systems maintained by the U.S. Environmental Protection Agency (EPA). They have several key differences:\n\nRegulatory Basis\n\nTRI: Established under the Emergency Planning and Community Right-to-Know Act (EPCRA) of 1986\nICIS-AIR: Part of the Clean Air Act (CAA) compliance and enforcement program\n\nFocus\n\nTRI: Tracks the management of certain toxic chemicals that may pose a threat to human health and the environment\nICIS-AIR: Focuses specifically on air quality and emissions from facilities regulated under the Clean Air Act\n\nReported Information\n\nTRI: Facilities report on releases, waste management, and pollution prevention activities for specific toxic chemicals\nICIS-AIR: Tracks emissions data, compliance status, and enforcement actions related to air quality regulations\n\nFacility Coverage\n\nTRI: Covers facilities in specific industries that manufacture, process, or use TRI-listed chemicals above certain thresholds\nICIS-AIR: Includes a broader range of facilities that emit air pollutants, regardless of the specific chemicals involved\n\nReporting Thresholds\n\nTRI: Has specific chemical thresholds that trigger reporting requirements\nICIS-AIR: Generally doesn’t have chemical-specific thresholds; requirements are based on overall emissions and facility type\n\nPublic Accessibility\n\nTRI: Designed with a strong focus on public right-to-know, with data easily accessible to the public\nICIS-AIR: While public, it’s primarily designed for regulatory and enforcement purposes\n\nData Frequency\n\nTRI: Annual reporting is required for covered facilities\nICIS-AIR: May involve more frequent reporting, depending on permit requirements and compliance status\n\nScope of Pollutants\n\nTRI: Focuses on a specific list of toxic chemicals and chemical categories\nICIS-AIR: Covers a wider range of air pollutants, including criteria air pollutants and hazardous air pollutants\n\nUse in Environmental Management\n\nTRI: Often used for assessing long-term trends in toxic chemical releases and waste management practices\nICIS-AIR: More commonly used for day-to-day air quality management and enforcement activities\n\nGeographic Coverage\n\nTRI: Nationwide program with consistent reporting across states\nICIS-AIR: While national, implementation can vary more by state or local air quality management district\n\n\n\n\n\n\nDeployment\nCongratulations! …. Now you should be able to:\n\nTest test…"
  },
  {
    "objectID": "m203-ejscreen.html#lesson-3",
    "href": "m203-ejscreen.html#lesson-3",
    "title": "SVI, TRI, and Health Outcomes",
    "section": "Lesson 3",
    "text": "Lesson 3\nIn this lesson, we explored ….\nLesson 3"
  },
  {
    "objectID": "m203-ejscreen.html#exploring-ejscreen",
    "href": "m203-ejscreen.html#exploring-ejscreen",
    "title": "EJSreen exploratory",
    "section": "Exploring EJScreen",
    "text": "Exploring EJScreen\nNow we are going to explore the EJScreen website. Type in “Metro Detroit”, you should get a zoomed in region of the metropolitan region of Detroit. (IMAGE)\nBoundary: zip code -Square drawing\nNotice the EJScreen Community Report, Explore charts, etc. We can use that to view statistical reports once we overlay the data.\nLets view people of color. -People of color (IMAGE)\nWe can explore the map, and draw a boundary polygon that we are interested in viewing. (IMAGE)\nNotice the map contents: (IMAGE)\n-Report\n-describe everything - left vs right of first page\nright (IMAGE)\n(IMAGE)\n(IMAGE)\n-To show that you can print it (IMAGE) To show that you can save images (click on logo with 3 rows) (IMAGE) (IMAGE) (IMAGE)\nLow life expectancy: (IMAGE)\nAsthma (IMAGE)\n-Toxic releases to air -Report -Health disparities -Report -Add air pollution and toxic release inventory (IMAGE) (IMAGE)\nLets compare the air pollution with toxics release inventory. (IMAGE)\nSocioeconomic and air pollution (IMAGE) Notice the size of the circles, could there be a correlation?\nBoundaries (IMAGE)\nZip codes (IMAGE)\nIf you go to tools, and click on “side by side comparison” for better comparison between maps. (IMAGE)\nSchool districts data https://www.waynecounty.com/departments/technology/gis-data.aspx\nWe will now download a shapefile from Wayne County, the most populous county in Michigan [source]. The page has different datasets under the label “GIS Data”, including “Comission Districts”, “Municipal Boundaries”, etc. Please click on the one labeled “Zip Codes”. A zip file will immediately download. Once it is done downloading, unzip the contents into a folder.\nNext, go to tools, add shapefile, select shapefile\nCongratulations! …. Now you should be able to:\n\nTest test…"
  },
  {
    "objectID": "m203-ejscreen.html#continue-to-lesson-4",
    "href": "m203-ejscreen.html#continue-to-lesson-4",
    "title": "EJSreen exploratory",
    "section": "Continue to Lesson 4",
    "text": "Continue to Lesson 4\nLesson 3"
  },
  {
    "objectID": "m203-ejscreen.html#overview",
    "href": "m203-ejscreen.html#overview",
    "title": "EJSreen exploratory",
    "section": "",
    "text": "In this lesson, we will learn how to use EJScreen, the Environmental Justice Screening and Mapping Tool developed by the Environmental Protection Agency. This tool allows one to map different types of indices with the option of generating reports and side by side comparisons. We will be focusing on Detroit, Michigan and surrounding areas, with a focus on racism and health."
  },
  {
    "objectID": "m203-ejscreen.html#how-to-use-ejscreen",
    "href": "m203-ejscreen.html#how-to-use-ejscreen",
    "title": "EJSreen exploratory",
    "section": "How to use EJScreen?",
    "text": "How to use EJScreen?\nTo begin, click on the EJScreen link here: https://www.epa.gov/ejscreen. In that page, along with resources on how to use EJScreen, you will see a blue, bold text labeled “Launch the EJScreen Tool.” You can also go straight to the mapper using this link: https://ejscreen.epa.gov/mapper/.\nOnce you click the link, you’ll be directed to a Welcome screen. Here, you’ll find an introduction, resource links, and a video overview of the tool. We recommend exploring these resources before proceeding. After that, close the Welcome dialog to continue.\n\n\n\n\n\nYou can use the search bar to explore a region. For this lesson, we will be focusing on Detroit, Michigan.\n\nType ‘Metro Detroit’ in the search bar to focus in on the metropolitan region of Detroit. Alternatively, you can use latitude and longitude coordinates. You can also zoom in or out of a region by clicking on the “+” or “-” icons on the bottom right.\n\n\n\n\n\nWe can start exploring maps by noticing the Widget toolbar. Hovering your cursor over each icon will show the name of each tab. From right to left to right, the icons are ‘Maps’, ‘Places’, ‘Reports’, and ‘Tools.’ Clicking each icon will display a dropdown list of indicators to choose from. For now, we will focus on the ‘Maps’ icon.\nNow, click on the Socioeconomic Indicators icon. Clicking that will expose seven types of socioeconomic indicators, and two indexes that can be explored. Click on People of Color, and your mapper should look like this. We will be mapping People of Color compared to the US. One can also compare it to just the state. To better view the map, one can click on the ‘&lt;’ on the Map widget.\nNotice the ‘Map Contents’ on the upper right. In statistics, a percentile is a score that shows how a particular score compares to other scores in a dataset. For example, people of color in the 95-100 percentile in a block group means that 95-100% of people living in that area are non-white. You can learn more about how EJScreen uses percentiles here https://www.epa.gov/ejscreen/how-interpret-ejscreen-data.\nYou can hide the Map Contents by clicking on the sign. Noice the symbols in the Map Contents. To the right of that is the sign has the layer turned on; clicking on it turns it off. The allows you to view the metadata. On the opposite side, is an sign, which allows you to set the transparency of the map. A pop up with a sliding scale will appear when that is clicked. Clicking on removes the layer. Selecting provides a description of the index. Clicking minimizes the Map Contents pane, to expand it one can simply click on it again (this time the arrows will be pointing downwards). [info box] Metadata describes a dataset. Providing metadata aligns well with the FAIR (Findability, Accessibility, Interoperability, and Reusability) principle, helping users understand what the data means and how to use it.\nOne can generate reports and charts given an area of interest. Go to the ‘Reports’ tab. One option to select an area of interest is by drawing a polygon. Click on ‘Draw an Area’.\nYour cursor should turn into a crosshair. You can now click on the map and start drawing your polygon. Move your cursor over to where you want the next point of the polygon to be. To form the final polygon, just connect to the first point you made, and the polygon will turn into a transparent green.\nA pop-up will appear where you can name your study area, add a buffer, download reports, and explore charts. Name it ‘Detroit Study Area’ to proceed.\nClicking on EJScreen Community Report will open up a PDF file with detailed information about the study area. Let us explore this PDF. On the right side, you can find information about other characteristics of the community of interest, such as low income residents, the education level of residents, and the gender breakdown of the community. Right below Community Information is Breakdown By Race, which shows the percent of each race in this area. You will also find the percentage of age brackets, and limited English speaking breakdown.\nOn the left side of the document, there is be a map showing the study area, including the legend from the Map Contents tab. Centering the map or zooming in or out on the Mapper will result in a different view on the report. This view will also highlight the languages spoken at home.\nPage 2 of the report presents bar graphs comparing EJ indexes at state and national levels. Review these graphs for differences in environmental factors like drinking water non-compliance and wastewater discharge. Note: Although the graph does not include wastewater discharge, the table below shows values for each index from left to right.\nLet’s compare this report to one generated outside of Detroit. Try to pick an area that shows a lower percentile of people of color, or less red, like Sterling Heights or Westland. What differences do you see? Are the EJ indexes higher or lower?\nOne can also view other reports, and download excel data. Lets compare the bar graphs in a red population of Detroit and a region in Sterling Heights. Delete the original polygon and create new ones within those regions.\nClick on a polygon and select ‘Explore Charts.’ A pop up will open, showing tabs for Environmental Justice Indexes, Environmental Burden Indicators, Socioeconomic Indicators, and Supplemental Indexes. Placing the cursor over each bar will show the exact percentile for each index. You can also unselect and select indexes of your choosing. For now, let’s keep all of them selected. We will also only focus on the USA percentile. Environmental Justice Indexes, Environmental Burden Indicators, and Supplemental Indexes focus on the same environmental indexes. Selecting the Socioeconomic indicators will list different types of indexes focusing on socioeconomic factors.\nOne can also explore the Socioeconomic (ACS) Report, which includes data from the US Census Bureau American Community Survey (ACS). Select Get Data Table, and a tabular view of the data should pop up. This will include all the categories, selected variables, percentile in state and percentile in USA.\nClick on the save icon to export the data, and an excel file in .csv format will automatically download.\nExplore the charts for Sterling Heights.\nNotice the lower percentiles for the EJ Indexes. Is this data in line with what was seen in the generated report? Download the data for this as well.\nLet’s perform some basic data analysis!\nWe can now use this data to create graphs on our own and better visually compare the indexes between separate regions. Let us compare the data for Sterling Heights and the region we selected in the metro area of Detroit. Save the files that were downloaded in a working directory, and rename them so that it makes sense. One file can be named ejscreen_detroit and the other ejscreen_sterlingheights. Open one of the csv files and take note of how the data is formatted. Under the main header that describes the EPA region, there should be 8 columns, each one with its own header (Category, Selected Variables, etc.) The Category column has the category of each index and indicator. The Selected Variables are the indexes for each category. For the EJ Indexes category, you should see 13 indexes in the Selected Variable column. To work with these files, we will be using the pandas library, which allows us to handle and manipulate excel files as a dataframe.\nImport the pandas and matplotlib libraries. Documentation for pandas can be found here https://pandas.pydata.org/docs/, and for matplotlib, here https://matplotlib.org/stable/users/getting_started/\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nIn order to use the data that you just downloaded as a CSV file, you have to tell the computer to read the CSV. The pandas read_csv() function reads CSV files. You can specify if the data has a header by adding header = 1, otherwise it treats the column names as part of the data. By specifying the header, it is extracting the column names we noticed in the CSV file and treating them as the header. Reading these files will now turn the csv files into a dataframe.\n\ndf_detroit = pd.read_csv(r'/Users/camillapaige/Documents/code/TOPSTSCHOOL-air-quality/docs/ejscreen_detroit.csv', header = 1)\n\nPrinting out the shape of the dataframe lets one see the number of rows and columns the data has. It is important to know the dimensions of your data before performing any analysis on it, to ensure that you will be correctly manipulating the data. You can also print just the dataframe variable for Sterling Heights and/or Detroit. The output looks like the contents in the CSV file.\n\nprint(df_detroit.shape)\n\n#Read in Sterling Heights csv file.\ndf_sterlingheights = pd.read_csv(r'/Users/camillapaige/Documents/code/TOPSTSCHOOL-air-quality/docs/ejscreen_sterlingheights.csv', header = 1)\n\n#Print the dataframe to examine the contents.\nprint(df_sterlingheights)\n\n(50, 8)\n     #              Category  \\\n0    1              EJ Index   \n1    2              EJ Index   \n2    3              EJ Index   \n3    4              EJ Index   \n4    5              EJ Index   \n5    6              EJ Index   \n6    7              EJ Index   \n7    8              EJ Index   \n8    9              EJ Index   \n9   10              EJ Index   \n10  11              EJ Index   \n11  12              EJ Index   \n12  13              EJ Index   \n13  14  Environmental Burden   \n14  15  Environmental Burden   \n15  16  Environmental Burden   \n16  17  Environmental Burden   \n17  18  Environmental Burden   \n18  19  Environmental Burden   \n19  20  Environmental Burden   \n20  21  Environmental Burden   \n21  22  Environmental Burden   \n22  23  Environmental Burden   \n23  24  Environmental Burden   \n24  25  Environmental Burden   \n25  26  Environmental Burden   \n26  27         Socioeconomic   \n27  28         Socioeconomic   \n28  29         Socioeconomic   \n29  30         Socioeconomic   \n30  31         Socioeconomic   \n31  32         Socioeconomic   \n32  33         Socioeconomic   \n33  34         Socioeconomic   \n34  35         Socioeconomic   \n35  36         Socioeconomic   \n36  37         Socioeconomic   \n37  38    Supplemental Index   \n38  39    Supplemental Index   \n39  40    Supplemental Index   \n40  41    Supplemental Index   \n41  42    Supplemental Index   \n42  43    Supplemental Index   \n43  44    Supplemental Index   \n44  45    Supplemental Index   \n45  46    Supplemental Index   \n46  47    Supplemental Index   \n47  48    Supplemental Index   \n48  49    Supplemental Index   \n49  50    Supplemental Index   \n\n                                   Selected Variables    Value State Avg.  \\\n0                 EJ Index for Particulate Matter 2.5      NaN        NaN   \n1                                  EJ Index for Ozone      NaN        NaN   \n2                 EJ Index for Nitrogen Dioxide (NO2)      NaN        NaN   \n3              EJ Index for Diesel Particulate Matter      NaN        NaN   \n4                  EJ Index for Toxic Releases to Air      NaN        NaN   \n5                      EJ Index for Traffic Proximity      NaN        NaN   \n6                             EJ Index for Lead Paint      NaN        NaN   \n7                    EJ Index for Superfund Proximity      NaN        NaN   \n8                 EJ Index for RMP Facility Proximity      NaN        NaN   \n9              EJ Index for Hazardous Waste Proximity      NaN        NaN   \n10             EJ Index for Underground Storage Tanks      NaN        NaN   \n11                  EJ Index for Wastewater Discharge      NaN        NaN   \n12         EJ Index for Drinking Water Non-Compliance      NaN        NaN   \n13                     Particulate Matter 2.5 (ug/m3)      8.7       7.84   \n14                                        Ozone (ppb)     70.2       67.3   \n15                      Nitrogen Dioxide (NO2) (ppbv)       11        7.7   \n16                                  Diesel PM (ug/m3)    0.159      0.116   \n17  Toxic Releases to Air (toxicity-weighted conce...     1600       2500   \n18  Traffic Proximity and Volume (daily traffic co...  1300000     910000   \n19                   Lead Paint (% pre-1960s housing)     0.11       0.38   \n20       Superfund Proximity (site count/km distance)     0.24       0.28   \n21         RMP Proximity (facility count/km distance)     0.33       0.38   \n22  Hazardous Waste Proximity (facility count/km d...      3.1          2   \n23              Underground Storage Tanks (count/km2)      4.5        7.6   \n24  Wastewater Discharge (toxicity-weighted concen...       36        880   \n25             Drinking Water Non-Compliance (points)    0.016       0.39   \n26                              Demographic Index USA     0.84        NaN   \n27                            Demographic Index State     0.87       1.18   \n28                 Supplemental Demographic Index USA     1.53        NaN   \n29               Supplemental Demographic Index State     1.49        1.5   \n30                                    People of Color      19%        26%   \n31                                         Low Income      23%        31%   \n32                                  Unemployment Rate       5%         6%   \n33                Limited English Speaking Households       4%         2%   \n34    Population with Less Than High School Education       8%         9%   \n35                             Population under Age 5       5%         5%   \n36                             Population over Age 64      19%        18%   \n37       Supplemental Indexfor Particulate Matter 2.5      NaN        NaN   \n38                       Supplemental Index for Ozone      NaN        NaN   \n39      Supplemental Index for Nitrogen Dioxide (NO2)      NaN        NaN   \n40   Supplemental Index for Diesel Particulate Matter      NaN        NaN   \n41       Supplemental Index for Toxic Releases to Air      NaN        NaN   \n42           Supplemental Index for Traffic Proximity      NaN        NaN   \n43                  Supplemental Index for Lead Paint      NaN        NaN   \n44         Supplemental Index for Superfund Proximity      NaN        NaN   \n45      Supplemental Index for RMP Facility Proximity      NaN        NaN   \n46   Supplemental Index for Hazardous Waste Proximity      NaN        NaN   \n47   Supplemental Index for Underground Storage Tanks      NaN        NaN   \n48        Supplemental Index for Wastewater Discharge      NaN        NaN   \n49  Supplemental Index for Drinking Water Non-Comp...      NaN        NaN   \n\n    %ile in State USA Avg.  %ile in USA  \n0            66.0      NaN         55.0  \n1            71.0      NaN         65.0  \n2            65.0      NaN         59.0  \n3            64.0      NaN         47.0  \n4            61.0      NaN         56.0  \n5            63.0      NaN         51.0  \n6            28.0      NaN         32.0  \n7            73.0      NaN         66.0  \n8            59.0      NaN         48.0  \n9            63.0      NaN         55.0  \n10           52.0      NaN         55.0  \n11           60.0      NaN         43.0  \n12           83.0      NaN         73.0  \n13           69.0     8.45         66.0  \n14           86.0     61.8         86.0  \n15           75.0      7.8         79.0  \n16           77.0    0.191         55.0  \n17           64.0     4600         69.0  \n18           71.0  1700000         60.0  \n19           23.0      0.3         37.0  \n20           74.0     0.39         73.0  \n21           63.0     0.57         54.0  \n22           72.0      3.5         68.0  \n23           56.0      3.6         77.0  \n24           70.0   700000         47.0  \n25           83.0      2.2         73.0  \n26            NaN     1.34         34.0  \n27           46.0      NaN          NaN  \n28            NaN     1.64         49.0  \n29           55.0      NaN          NaN  \n30           57.0      40%         36.0  \n31           42.0      30%         43.0  \n32           58.0       6%         62.0  \n33           89.0       5%         73.0  \n34           60.0      11%         52.0  \n35           58.0       5%         55.0  \n36           59.0      18%         62.0  \n37           75.0      NaN         68.0  \n38           82.0      NaN         82.0  \n39           74.0      NaN         76.0  \n40           73.0      NaN         58.0  \n41           69.0      NaN         70.0  \n42           72.0      NaN         66.0  \n43           27.0      NaN         34.0  \n44           74.0      NaN         71.0  \n45           62.0      NaN         53.0  \n46           72.0      NaN         71.0  \n47           54.0      NaN         62.0  \n48           70.0      NaN         49.0  \n49           83.0      NaN         73.0  \n\n\nWe are going to merge the dataframes using the pandas merge() function. The merge function combines dataframes using a chosen parameter. For this case, we will be merging using the ‘#’, ‘Category’, and ‘Selected Variables’ column. The ‘on’ parameter chooses which column both dataframes will join on; the columns must be found in both dataframes. Since the ‘#’, ‘Category’, and ‘Selected Variables’ columns are the same in both dataframes, we will merge on those columns, meaning that the final dataframe will only have one of each of those columns. The rest of the columns will double, and the numbers of rows will remain the same. The ‘suffixes’ parameter allows you to add a suffix to the columns to help identify which column belongs to which dataset. Since the ‘Value’, ‘State Avg.’, %ile in State, USA Avg., and %ile in USA differ between the two datasets, those column headers will carry the suffixes.\n\n#Suffix ‘_dt’ for Detroit; ‘_sh’ for Sterling Heights.\ndf_merge = pd.merge(df_detroit, df_sterlingheights, on = ('#', 'Category', 'Selected Variables'), suffixes = ('_dt', '_sh'))\nprint(df_merge)\n\n     #              Category  \\\n0    1              EJ Index   \n1    2              EJ Index   \n2    3              EJ Index   \n3    4              EJ Index   \n4    5              EJ Index   \n5    6              EJ Index   \n6    7              EJ Index   \n7    8              EJ Index   \n8    9              EJ Index   \n9   10              EJ Index   \n10  11              EJ Index   \n11  12              EJ Index   \n12  13              EJ Index   \n13  14  Environmental Burden   \n14  15  Environmental Burden   \n15  16  Environmental Burden   \n16  17  Environmental Burden   \n17  18  Environmental Burden   \n18  19  Environmental Burden   \n19  20  Environmental Burden   \n20  21  Environmental Burden   \n21  22  Environmental Burden   \n22  23  Environmental Burden   \n23  24  Environmental Burden   \n24  25  Environmental Burden   \n25  26  Environmental Burden   \n26  27         Socioeconomic   \n27  28         Socioeconomic   \n28  29         Socioeconomic   \n29  30         Socioeconomic   \n30  31         Socioeconomic   \n31  32         Socioeconomic   \n32  33         Socioeconomic   \n33  34         Socioeconomic   \n34  35         Socioeconomic   \n35  36         Socioeconomic   \n36  37         Socioeconomic   \n37  38    Supplemental Index   \n38  39    Supplemental Index   \n39  40    Supplemental Index   \n40  41    Supplemental Index   \n41  42    Supplemental Index   \n42  43    Supplemental Index   \n43  44    Supplemental Index   \n44  45    Supplemental Index   \n45  46    Supplemental Index   \n46  47    Supplemental Index   \n47  48    Supplemental Index   \n48  49    Supplemental Index   \n49  50    Supplemental Index   \n\n                                   Selected Variables Value_dt State Avg._dt  \\\n0                 EJ Index for Particulate Matter 2.5      NaN           NaN   \n1                                  EJ Index for Ozone      NaN           NaN   \n2                 EJ Index for Nitrogen Dioxide (NO2)      NaN           NaN   \n3              EJ Index for Diesel Particulate Matter      NaN           NaN   \n4                  EJ Index for Toxic Releases to Air      NaN           NaN   \n5                      EJ Index for Traffic Proximity      NaN           NaN   \n6                             EJ Index for Lead Paint      NaN           NaN   \n7                    EJ Index for Superfund Proximity      NaN           NaN   \n8                 EJ Index for RMP Facility Proximity      NaN           NaN   \n9              EJ Index for Hazardous Waste Proximity      NaN           NaN   \n10             EJ Index for Underground Storage Tanks      NaN           NaN   \n11                  EJ Index for Wastewater Discharge      NaN           NaN   \n12         EJ Index for Drinking Water Non-Compliance      NaN           NaN   \n13                     Particulate Matter 2.5 (ug/m3)     9.67          7.84   \n14                                        Ozone (ppb)     69.3          67.3   \n15                      Nitrogen Dioxide (NO2) (ppbv)       12           7.7   \n16                                  Diesel PM (ug/m3)    0.181         0.116   \n17  Toxic Releases to Air (toxicity-weighted conce...     3400          2500   \n18  Traffic Proximity and Volume (daily traffic co...  2800000        910000   \n19                   Lead Paint (% pre-1960s housing)     0.84          0.38   \n20       Superfund Proximity (site count/km distance)        0          0.28   \n21         RMP Proximity (facility count/km distance)     0.59          0.38   \n22  Hazardous Waste Proximity (facility count/km d...      3.2             2   \n23              Underground Storage Tanks (count/km2)       16           7.6   \n24  Wastewater Discharge (toxicity-weighted concen...       15           880   \n25             Drinking Water Non-Compliance (points)   0.0062          0.39   \n26                              Demographic Index USA     2.74           NaN   \n27                            Demographic Index State     2.91          1.18   \n28                 Supplemental Demographic Index USA      2.4           NaN   \n29               Supplemental Demographic Index State     2.38           1.5   \n30                                    People of Color      89%           26%   \n31                                         Low Income      57%           31%   \n32                                  Unemployment Rate      16%            6%   \n33                Limited English Speaking Households       2%            2%   \n34    Population with Less Than High School Education      16%            9%   \n35                             Population under Age 5       7%            5%   \n36                             Population over Age 64      15%           18%   \n37       Supplemental Indexfor Particulate Matter 2.5      NaN           NaN   \n38                       Supplemental Index for Ozone      NaN           NaN   \n39      Supplemental Index for Nitrogen Dioxide (NO2)      NaN           NaN   \n40   Supplemental Index for Diesel Particulate Matter      NaN           NaN   \n41       Supplemental Index for Toxic Releases to Air      NaN           NaN   \n42           Supplemental Index for Traffic Proximity      NaN           NaN   \n43                  Supplemental Index for Lead Paint      NaN           NaN   \n44         Supplemental Index for Superfund Proximity      NaN           NaN   \n45      Supplemental Index for RMP Facility Proximity      NaN           NaN   \n46   Supplemental Index for Hazardous Waste Proximity      NaN           NaN   \n47   Supplemental Index for Underground Storage Tanks      NaN           NaN   \n48        Supplemental Index for Wastewater Discharge      NaN           NaN   \n49  Supplemental Index for Drinking Water Non-Comp...      NaN           NaN   \n\n    %ile in State_dt USA Avg._dt  %ile in USA_dt Value_sh State Avg._sh  \\\n0               96.0         NaN            96.0      NaN           NaN   \n1               94.0         NaN            96.0      NaN           NaN   \n2               94.0         NaN            95.0      NaN           NaN   \n3               95.0         NaN            86.0      NaN           NaN   \n4               94.0         NaN            95.0      NaN           NaN   \n5               95.0         NaN            94.0      NaN           NaN   \n6               95.0         NaN            98.0      NaN           NaN   \n7                0.0         NaN             0.0      NaN           NaN   \n8               92.0         NaN            88.0      NaN           NaN   \n9               92.0         NaN            90.0      NaN           NaN   \n10              92.0         NaN            96.0      NaN           NaN   \n11              89.0         NaN            72.0      NaN           NaN   \n12              83.0         NaN            73.0      NaN           NaN   \n13              96.0        8.45            85.0      8.7          7.84   \n14              70.0        61.8            84.0     70.2          67.3   \n15              84.0         7.8            85.0       11           7.7   \n16              88.0       0.191            55.0    0.159         0.116   \n17              84.0        4600            81.0     1600          2500   \n18              94.0     1700000            80.0  1300000        910000   \n19              90.0         0.3            94.0     0.11          0.38   \n20               0.0        0.39             0.0     0.24          0.28   \n21              75.0        0.57            67.0     0.33          0.38   \n22              73.0         3.5            69.0      3.1             2   \n23              83.0         3.6            94.0      4.5           7.6   \n24              58.0      700000            39.0       36           880   \n25              83.0         2.2            73.0    0.016          0.39   \n26               NaN        1.34            91.0     0.84           NaN   \n27              93.0         NaN             NaN     0.87          1.18   \n28               NaN        1.64            86.0     1.53           NaN   \n29              89.0         NaN             NaN     1.49           1.5   \n30              92.0         40%            88.0      19%           26%   \n31              86.0         30%            86.0      23%           31%   \n32              90.0          6%            92.0       5%            6%   \n33              82.0          5%            65.0       4%            2%   \n34              84.0         11%            74.0       8%            9%   \n35              74.0          5%            71.0       5%            5%   \n36              42.0         18%            46.0      19%           18%   \n37              95.0         NaN            95.0      NaN           NaN   \n38              90.0         NaN            95.0      NaN           NaN   \n39              91.0         NaN            93.0      NaN           NaN   \n40              93.0         NaN            83.0      NaN           NaN   \n41              92.0         NaN            93.0      NaN           NaN   \n42              94.0         NaN            93.0      NaN           NaN   \n43              93.0         NaN            96.0      NaN           NaN   \n44               0.0         NaN             0.0      NaN           NaN   \n45              89.0         NaN            84.0      NaN           NaN   \n46              88.0         NaN            88.0      NaN           NaN   \n47              90.0         NaN            94.0      NaN           NaN   \n48              80.0         NaN            59.0      NaN           NaN   \n49              83.0         NaN            73.0      NaN           NaN   \n\n    %ile in State_sh USA Avg._sh  %ile in USA_sh  \n0               66.0         NaN            55.0  \n1               71.0         NaN            65.0  \n2               65.0         NaN            59.0  \n3               64.0         NaN            47.0  \n4               61.0         NaN            56.0  \n5               63.0         NaN            51.0  \n6               28.0         NaN            32.0  \n7               73.0         NaN            66.0  \n8               59.0         NaN            48.0  \n9               63.0         NaN            55.0  \n10              52.0         NaN            55.0  \n11              60.0         NaN            43.0  \n12              83.0         NaN            73.0  \n13              69.0        8.45            66.0  \n14              86.0        61.8            86.0  \n15              75.0         7.8            79.0  \n16              77.0       0.191            55.0  \n17              64.0        4600            69.0  \n18              71.0     1700000            60.0  \n19              23.0         0.3            37.0  \n20              74.0        0.39            73.0  \n21              63.0        0.57            54.0  \n22              72.0         3.5            68.0  \n23              56.0         3.6            77.0  \n24              70.0      700000            47.0  \n25              83.0         2.2            73.0  \n26               NaN        1.34            34.0  \n27              46.0         NaN             NaN  \n28               NaN        1.64            49.0  \n29              55.0         NaN             NaN  \n30              57.0         40%            36.0  \n31              42.0         30%            43.0  \n32              58.0          6%            62.0  \n33              89.0          5%            73.0  \n34              60.0         11%            52.0  \n35              58.0          5%            55.0  \n36              59.0         18%            62.0  \n37              75.0         NaN            68.0  \n38              82.0         NaN            82.0  \n39              74.0         NaN            76.0  \n40              73.0         NaN            58.0  \n41              69.0         NaN            70.0  \n42              72.0         NaN            66.0  \n43              27.0         NaN            34.0  \n44              74.0         NaN            71.0  \n45              62.0         NaN            53.0  \n46              72.0         NaN            71.0  \n47              54.0         NaN            62.0  \n48              70.0         NaN            49.0  \n49              83.0         NaN            73.0  \n\n\nExtract the ‘EJ Index’ rows from the ‘Category’ columns, the corresponding ‘%ile in State’ values and the ‘Selected Variables’ for the Detroit and Sterling Heights.\n\n#Extracting only the rows that have ‘EJ Index’ as the category from the merged dataframe, and turning it into a new dataframe called ‘ej_index_df’ that only has that category. The rest of the variables will be extracted from this dataframe.\nej_index_df = df_merge[(df_merge['Category'] == 'EJ Index')]\n\n#Extracting the '%ile in State' for Detroit and Sterling Heights.\nej_index_detroit = ej_index_df['%ile in State_dt']\nej_index_sterlingheights = ej_index_df['%ile in State_sh']\n\n#Extracting the ‘Selected Variables.’\nselected_variables = ej_index_df['Selected Variables']\n\nNotice how each of the indexes in the selected variables dataframe starts with ‘EJ Index for’ before the name of the index. When we plot this, the x axis will have the label of ‘EJ Index’, so having it before each index seems redundant. We can remove the first three words by using the .split() function. We will create a function that removes the first three words of each of the indexes in the selected_variables dataframe, and then returns the remaining words. Each selected variable is a string, which in this case, is a sequence of words. The .split() function, without specifying any parameters, splits the string into individual words. The .join() function then combines the separate words back into a string.\n\n\n\n\n\n\nTip\n\n\n\nA function is a unit of code that performs a specific task. Its main advantage is that it can be reused, making code more efficient. def() is the keyword used to define the function, and is placed before the name of the function. What goes in the parentheses is the optional parameters, which are the input values. Sometimes, a function ends with ‘return’, which outputs a specific value from the function.\n\n\n\ndef remove_first_three_words(text):\n    #Splitting the string in between spaces.\n    words = text.split()\n    #Returning a new string that excludes the first three words.\n    return ' '.join(words[3:])\n\n#Applying the function to the selected_variables dataframe.\nselected_variables = selected_variables.apply(remove_first_three_words)\n\nA bar graph can now be created using the information that was extracted. The ‘index’ variable is storing a sequence of indices from 0 to the length of selected_variables minus 1. The len() function calculates the size of the selected_variables dataframe, which is 13 since there are 13 indexes. The range() function then creates a sequence of numbers starting from 0 to minus 1 of the length of selected variables.\n\n#Configuring size of bar graph.\nplt.figure(figsize=(12, 8))\nbar_width = 0.35\n\n#Creating sequence of numbers from 0 to 12 for plotting the 13 indexes as labels.\nindex = range(len(selected_variables))\n\n&lt;Figure size 1152x768 with 0 Axes&gt;\n\n\nplt.bar(index, ej_index_detroit, bar_width, label=‘Detroit’) plots a series of bars at the x-positions specified by index and sets the height according to the ej_index_detroit values, with each bar having a width defined by the bar_width variable. The second line, plt.bar([i + bar_width for i in index], ej_index_sterlingheights, bar_width, label=‘Sterling Heights’), plots another series of bars that are shifted to the right by bar_width per each index value to position them next to the Detroit bars. Using only ‘index’ without the adjustment overlaps the bars instead. A similar thing is done to plot the x ticks in the plt.xticks([i + bar_width/2 for i in index], selected_variables, rotation=45, ha=‘right’) line, where selected_variables is used as the labels at a 45 degree angle, and ensures that the ticks and labels are positioned between the Detroit and Sterling Heights bars.\n\nplt.bar(index, ej_index_detroit, bar_width, label='Detroit')\n\nplt.bar([i + bar_width for i in index], ej_index_sterlingheights, bar_width, label='Sterling Heights')\n\n#Labeling the x-axis and y-axis.\nplt.xlabel('EJ Index')\nplt.ylabel('%ile in State')\nplt.title('Comparison of EJ Index (%ile in State) between Detroit and Sterling Heights')\nplt.xticks([i + bar_width/2 for i in index], selected_variables, rotation=45, ha='right')\n\n#Adding a legend\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nWhat conclusions can you draw from looking at the resulting bar graph?\nNext, we can start mapping other indexes. Let’s increase our study area. Go back to the mapper and zoom out so that you can see the metro area of Detroit, and the surrounding area, up to around Sterling Heights. Delete the polygons that were created by selecting them and choosing “Delete this site.”\nWe are going to do some side by side map comparisons of other indexes. Go to tools and select ‘Side by Side Map Comparisons.’ That will take you to a different web page which should show a left and right hand side of the map. At the top, on the right hand side, click on “Map Data”. A tab similar to the mapper should appear. Select Threshold map for the theme of the map, choose EJ Indexes as type of Index, and Compare to US for location. Keep the lower and upper bounds the same. Select ‘Particulate Matter 2.5’. Then click on ‘Update Map’. For the right hand side, select ‘More’ for the theme of the map, choose Health Disparities as the service, and Asthma as the Layer.\n-The result should look similar to this:\nExpand the Legend to further understand the data. Do you see a correlation between the data? Play around with the comparisons. Try choosing People of Color while keeping asthma.\nYou can also create reports based on boundaries. Under Tools, select Boundaries. The Map Contents will show you several types of boundaries you can add to your maps, like zip codes, counties, etc. You can generate reports based on these boundaries, and boundaries you want to import, as well.\nWe can add a shapefile to the Mapper. Head on to the Wayne County datapage [https://www.waynecounty.com/departments/technology/gis-data.aspx]. The page has different datasets under the label “GIS Data”, including “Commission Districts”, “Municipal Boundaries”, etc. Please click on the one labeled “School Districts”. A zip file will immediately download. Once it is done downloading, unzip the contents into a folder.\nNext, go to Tools, select Add Shapefile, and then select Add File.\nNow, you can generate reports for each school district. What conclusions do you think you can come to by comparing school districts that overlay high percentiles of certain indexes or indicators, in Detroit and other counties?\nFeel free to explore the EJScreen more. There are many other things you can do with this helpful tool. If you ever want to come back to your work, you can save it as a session. In Tools, select Save Session, and add a name to it. Select save, and then under Actions, select ‘Save to File.’ Your session will be downloaded as a .json, which can then be added back in a later time by selecting ‘Load from file’ and choosing the .json file from your folder. For more information, please check out the EJScreen User Guide. https://ejscreen.epa.gov/mapper/help/ejscreen_help.pdf\n##Conclusion\nCongratulations! …. Now you should be able to:\n\nMap different types of indices and understand percentiles on the Map Contents.\nInterpret the results of the Community Report and understand demographic and indices information for a specific area.\nGenerate charts and download the charts as an excel file to use for simple data analysis.\nCreate a side by side comparison of health disparities and socioeconomic indexes.\nUpload a shapefile to the EJScreen Mapper."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#global-level",
    "href": "m201-student-led-monitoring-nyc.html#global-level",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Global level",
    "text": "Global level\nWhile the challenges of air pollution and its associated health impacts are evident in various states across the U.S., they are part of a broader global crisis that affects millions. Air pollution is a significant global threat , affecting both human health and environmental integrity. Responsible for an estimated 4.2 million premature deaths annually (World Health Organization 2021), air pollution doesn’t respect borders or zip codes; its impact ripples across continents and infiltrates our everyday lives. Many people go about their day, often unaware that their cough may be linked to harmful air particles, believing the air they breathe is clean unless an advisory alert is issued.\nThis lack of awareness extends to the reality that natural disasters occurring far from home or excessive anthropogenic sources can also pollute the air we breathe, illustrating how air pollution transcends local boundaries and affects populations worldwide. In many low- and middle-income countries (LMICs), air pollution poses a particularly acute threat to vulnerable populations. These communities, often located near industrial sites or high-traffic areas, face disproportionate exposure to toxic pollutants that can lead to chronic respiratory diseases, cardiovascular problems, and other serious health conditions.\nVarious natural and human-made sources contribute to air pollution, but anthropogenic emissions have become the primary driver, resulting in complex mixtures of pollutants that can harm human health, ecosystems, and the environment—even with brief exposures. The WHO estimates that children, the elderly, and those with pre-existing health conditions are at the greatest risk, highlighting the urgent need for targeted interventions (World Health Organization 2021). By focusing on the unique needs of these populations, we can develop more effective policies and programs to mitigate the health impacts of air pollution.\nThe relationship between air pollution and climate change further complicates the public health landscape. Many of the same activities that contribute to greenhouse gas emissions—such as burning fossil fuels and deforestation—also release harmful pollutants into the atmosphere. As climate change intensifies, phenomena like wildfires and heatwaves can worsen air quality, leading to increased respiratory ailments and hospitalizations. The interplay between air quality and climate change necessitates a dual approach to policy-making, where strategies to reduce emissions are integrated with efforts to improve public health outcomes. Addressing these interconnected challenges will require collaboration across various sectors, including health, environment, and transportation.\nTo effectively combat air pollution on a global scale, a multi-faceted strategy is essential. This strategy should include implementing stricter emissions standards and promoting clean energy solutions. Enhancing public awareness about the health risks associated with air pollution is also crucial. Furthermore, international cooperation is critical, as air quality issues do not adhere to national borders. Pollutants can travel long distances, impacting air quality in regions far from their source.\nData analysis plays a crucial role in understanding the complexities of air pollution and its socio-economic implications. Utilizing resources like NASA’s socioeconomic and environmental data available via NASA’s Earthdata portal (NASA Earthdata) allows researchers to compare relationships between levels of socioeconomic deprivation and air quality data on particulate matter (PM) across various international administrative areas.\nBy calculating and visualizing zonal statistics, statistical measures that summarize data values within specified geographic areas, for different countries, researchers can assess PM2.5 concentration levels in relation to socioeconomic factors. This analysis involves extracting relevant zonal statistics, such as mean and median PM2.5 levels, and creating visualizations like choropleth maps, which use color gradients to represent data values across geographic regions, helps highlight air quality disparities. Such studies underscore the importance of examining air quality in the context of socio-economic metrics, ultimately enhancing our understanding of environmental justice issues and informing targeted policy interventions.\nUltimately, recognizing that the health of our populations is intricately linked to the quality of our air is vital for developing sustainable solutions that protect public health and the environment. Air pollution is a multifaceted issue that operates on local, regional, national, and global scales, making it essential to monitor its effects across these levels. Local data empowers communities to tackle specific pollution sources and advocate for cleaner air, while regional assessments help address cross-boundary issues through collaborative policies. National monitoring informs regulatory frameworks and evaluates the effectiveness of air quality standards, while global cooperation is crucial for addressing transboundary pollution and aligning efforts with international climate agreements.\nIn conclusion, by prioritizing air quality monitoring across these scales, we gain a comprehensive understanding of its impact on health and the environment. This coordinated approach not only enhances public health outcomes but also promotes a sustainable future, ensuring that clean air is a shared right for all communities, regardless of their location. Moreover, the intersection of air pollution and climate change highlights the urgency of conducting rigorous research to inform effective policy responses. Open science research, in particular, plays a critical role in this effort by promoting transparency, collaboration, and accessibility of data. By sharing findings widely, researchers can accelerate the development of innovative solutions and strategies that address both pollution and climate change. Engaging diverse stakeholders in this research process fosters a more inclusive dialogue, ensuring that the voices of affected communities are heard. Ultimately, prioritizing open science in the study of air quality and climate will lead to more effective interventions and a healthier planet for future generations."
  },
  {
    "objectID": "m201-student-led-monitoring-nyc.html#conclusion",
    "href": "m201-student-led-monitoring-nyc.html#conclusion",
    "title": "Enabling Student-led Air Quality and Extreme Temperature Monitoring in New York",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You have now been introduced to the fundamentals of air pollution, its sources, and its impacts on health, the environment, and climate. This lesson explored how air pollution operates on multiple scales—local, regional, and global—and emphasized the importance of understanding its disproportionate effects on vulnerable populations. By learning about particulate matter, historical injustices like redlining, and the role of data in assessing air quality, you have taken the first step toward understanding the complexities of this critical issue."
  },
  {
    "objectID": "index.html#module-2-air-quality-datasets-and-use-cases-cover",
    "href": "index.html#module-2-air-quality-datasets-and-use-cases-cover",
    "title": "Welcome SCHOOL Module 2: Air Quality",
    "section": "Module 2: Air Quality datasets and use cases cover:",
    "text": "Module 2: Air Quality datasets and use cases cover:\n\nAcquiring, Pre-Processing, and Visualizing Student-Monitored Data for New York City schools: This lesson will emphasize the relationship between hazardous chronic air quality and socioeconomic characteristics of different New York City school environments.\n\nLesson 1: Acquiring, Pre-Processing, and Visualizing Student-Monitored Data for New York City schools\n\nExploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit: Participants will investigate how the Social Vulnerability Index (SVI) highlights areas at greater risk for adverse health outcomes due to environmental hazards. This lesson will examine the connections between socioeconomic status, health disparities, and environmental injustice.\n\nLesson 2: Exploring Air Quality, Social Vulnerability, and Health Outcomes in Metro Detroit\n\nEnvironmental Justice Screening and Mapping Tool (EJScreen): Explore the environmental and demographic characteristics of a geographic area using the Environmental Protection Agency’s environmental justice screening and mapping tool.\n\nLesson 3: Exploring EJScreen: Environmental Justice Screening and Mapping Tool\n\nParticulate Matter Across Socioeconomic Strata of Countries: Analyzing the Global Gridded Relative Deprivation Index Version 1 (GRDIv1) and annual PM2.5 grids to explore global relationships between socioeconomic vulnerability and particulate matter concentrations over time. This lesson will provide insights into how air quality varies across different regions and its implications for public health, focusing on both local and global contexts.\n\nLesson 4: Particulate Matter Across Socioeconomic Strata of Countries\n\n\n\nStart Lesson 1\n\nThis course was made possible thanks to the work of our NASA Transform to Open Science (TOPS) team, our SCHOOL Open Science team, open science Subject Matter Experts (SMEs), and the SCHOOL Development team!"
  }
]